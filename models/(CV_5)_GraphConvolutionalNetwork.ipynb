{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rdkit\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "#!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\itmo\\\\BONUS TRACK\\\\final_project\\\\data\\\\NN_ML.csv\", delimiter=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Определяю типы атомов в \".xyz\"-файлах\n",
    "# Расстояния между ними будут использованы пакетом RDKit для присвоения типов связей\n",
    "# P.S. \".xyz\"-файлы не содержат \"connectivity\", поэтому визуализаторы по типу \"ChemCraft\" и \"Avogadro\" используют такой подход для присвоения связей\n",
    "\n",
    "def read_xyz_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[2:] \n",
    "        elements = []\n",
    "        for line in lines:\n",
    "            parts = line.split() \n",
    "            element = parts[0]  \n",
    "            elements.append(element)  \n",
    "    return elements \n",
    "\n",
    "def identify_atom_types(directory_path):\n",
    "    atom_types = set()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.xyz'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            elements = read_xyz_file(file_path)\n",
    "            atom_types.update(elements)\n",
    "    return atom_types\n",
    "\n",
    "directory_path = 'C:\\\\itmo\\\\BONUS TRACK\\\\final_project\\\\xyz'\n",
    "atom_types = identify_atom_types(directory_path)\n",
    "print(\"Atom types found in .xyz files:\")\n",
    "for atom in sorted(atom_types):\n",
    "    print(atom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Функция read_xyz_file() читает \".xyz\"--файлы (типы атомов + координаты)\n",
    "# Функция check_valence() проверяет валентность атомов\n",
    "# Функция bond_exists() проверяет наличие связи\n",
    "# Функция xyz_to_rdkit_mol() конвертирует \".xyz\"-данные в RDKit-молекулы (типы связей присвоены в соответствии с расстоянием между атомами)\n",
    "# Цикл выдаёт \"SMILES\" RDKit-молекул (при необходимости, можно сравнить связи и структуры молекул с оригинальными \".xyz\"-файлами)\n",
    "\n",
    "def read_xyz_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[2:] \n",
    "        coordinates = []\n",
    "        elements = []\n",
    "        for line in lines:\n",
    "            parts = line.split() \n",
    "            element = parts[0]  \n",
    "            x, y, z = float(parts[1]), float(parts[2]), float(parts[3]) \n",
    "            elements.append(element) \n",
    "            coordinates.append([x, y, z]) \n",
    "    return elements, coordinates\n",
    "\n",
    "def check_valence(mol, atom_idx):\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return True\n",
    "    except Chem.AtomValenceException:\n",
    "        return False\n",
    "\n",
    "def bond_exists(mol, atom_idx1, atom_idx2):\n",
    "    bond = mol.GetBondBetweenAtoms(atom_idx1, atom_idx2)\n",
    "    return bond is not None\n",
    "\n",
    "def xyz_to_rdkit_mol(elements, coordinates):\n",
    "    single_bonds = {\n",
    "        ('C', 'H'): 1.09,\n",
    "        ('C', 'C'): 1.54,\n",
    "        ('C', 'N'): 1.47,\n",
    "        ('C', 'O'): 1.43,\n",
    "        ('H', 'H'): 0.74,\n",
    "        ('N', 'H'): 1.01,\n",
    "        ('O', 'H'): 0.96,\n",
    "        ('N', 'N'): 1.45,\n",
    "        ('N', 'O'): 1.40,\n",
    "        ('O', 'O'): 1.48,\n",
    "    }\n",
    "\n",
    "    double_bonds = {\n",
    "        ('C', 'C'): 1.30,\n",
    "        ('C', 'N'): 1.30,\n",
    "        ('C', 'O'): 1.23,\n",
    "        ('N', 'O'): 1.22,\n",
    "    }\n",
    "\n",
    "    triple_bonds = {\n",
    "        ('C', 'C'): 1.20,\n",
    "        ('C', 'N'): 1.16,\n",
    "    }\n",
    "\n",
    "    mol = Chem.RWMol()\n",
    "    conf = Chem.Conformer(len(elements))\n",
    "\n",
    "    for i, element in enumerate(elements):\n",
    "        atom = Chem.Atom(element)\n",
    "        mol.AddAtom(atom)\n",
    "        conf.SetAtomPosition(i, Chem.rdGeometry.Point3D(*coordinates[i]))\n",
    "\n",
    "    mol.AddConformer(conf)\n",
    "\n",
    "    num_atoms = len(elements)\n",
    "    for i in range(num_atoms):\n",
    "        for j in range(i + 1, num_atoms):\n",
    "            distance = np.linalg.norm(np.array(coordinates[i]) - np.array(coordinates[j]))\n",
    "            pair = (elements[i], elements[j])\n",
    "            if pair not in single_bonds:\n",
    "                pair = (elements[j], elements[i])\n",
    "\n",
    "            if bond_exists(mol, i, j):\n",
    "                continue\n",
    "\n",
    "            if pair in triple_bonds and distance < triple_bonds[pair] * 1.04:\n",
    "                if not bond_exists(mol, i, j):\n",
    "                    mol.AddBond(i, j, Chem.BondType.TRIPLE)\n",
    "                    if not check_valence(mol, i) or not check_valence(mol, j):\n",
    "                        mol.RemoveBond(i, j)\n",
    "                        continue\n",
    "\n",
    "            if pair in double_bonds and distance < double_bonds[pair] * 1.04:\n",
    "                if not bond_exists(mol, i, j):\n",
    "                    mol.AddBond(i, j, Chem.BondType.DOUBLE)\n",
    "                    if not check_valence(mol, i) or not check_valence(mol, j):\n",
    "                        mol.RemoveBond(i, j)\n",
    "                        continue\n",
    "\n",
    "            if pair in single_bonds and distance < single_bonds[pair] * 1.04:\n",
    "                if not bond_exists(mol, i, j): \n",
    "                    mol.AddBond(i, j, Chem.BondType.SINGLE)\n",
    "                    if not check_valence(mol, i) or not check_valence(mol, j):\n",
    "                        mol.RemoveBond(i, j)\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except Chem.AtomValenceException as e:\n",
    "        print(f\"Valence error for molecule: {e}\")\n",
    "        return None\n",
    "    except Chem.AtomSanitizeException as e:\n",
    "        print(f\"Sanitization error for molecule: {e}\")\n",
    "        return None\n",
    "\n",
    "    return mol\n",
    "\n",
    "\"\"\"directory_path = 'C:\\\\itmo\\\\BONUS TRACK\\\\final_project\\\\xyz'\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.xyz'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        elements, coordinates = read_xyz_file(file_path)\n",
    "        mol = xyz_to_rdkit_mol(elements, coordinates)\n",
    "        if mol:\n",
    "            print(f\"{filename}: {Chem.MolToSmiles(mol)}\")\n",
    "        else:\n",
    "            print(f\"Failed to create RDKit molecule for {filename}.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Функция прводит вычисления RDKit-признаков для каждой молекулы\n",
    "\n",
    "def compute_rdkit_features(mol):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    morgan_fp = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    return np.array(morgan_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Функция для графового представления БД\n",
    "# HF-3с столбцы & RdKit-столбец - признаки нодов\n",
    "# Порог в 3.5 А установлен для эджей (они нужны для выявления геометрических связей между нодами [атомами], а не химических)\n",
    "# Добавил DFT энергии (таргеты) в качестве меток\n",
    "\n",
    "def create_graph_data(df):\n",
    "    graph_data_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        coordinates = np.array(row['coordinates'])\n",
    "        num_atoms = coordinates.shape[0]\n",
    "\n",
    "        hf_features = row[['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "                           'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_homo_ev', 'hf_lumo_ev', 'mass_au']].astype(float).values\n",
    "\n",
    "        rdkit_features = row['rdkit_features']\n",
    "        if rdkit_features is not None:\n",
    "            hf_features = np.concatenate([hf_features, rdkit_features])\n",
    "\n",
    "        node_features = np.tile(hf_features, (num_atoms, 1))\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        edge_index = []\n",
    "        for i in range(num_atoms):\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                distance = np.linalg.norm(coordinates[i] - coordinates[j])\n",
    "                if distance < 3.5:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_index.append([j, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "        graph_data.y = torch.tensor(row[['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                                         'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_homo_ev', 'dft_lumo_ev']].astype(float).values, dtype=torch.float)\n",
    "        graph_data_list.append(graph_data)\n",
    "    \n",
    "    return graph_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Добавил столбцы с координатами и атомами\n",
    "\n",
    "xyz_folder = \"C:/itmo/BONUS TRACK/final_project/xyz\"\n",
    "\n",
    "df['coordinates'] = [[] for _ in range(len(df))]\n",
    "df['elements'] = [[] for _ in range(len(df))]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    file_name = f\"{row['name']}.xyz\"\n",
    "    file_path = os.path.join(xyz_folder, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        elements, coordinates = read_xyz_file(file_path)\n",
    "        df.at[idx, 'coordinates'] = coordinates\n",
    "        df.at[idx, 'elements'] = elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Добавил столбец с RDKit-признаками\n",
    "\n",
    "df['rdkit_features'] = df.apply(lambda row: compute_rdkit_features(xyz_to_rdkit_mol(row['elements'], row['coordinates'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Список графов\n",
    "\n",
    "graph_data_list = create_graph_data(df)\n",
    "graph_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# У первого графа 34 нод, а количество крпизнаков у ноды = 2056 (2048 фингерпринтов + 7 HF-3c столбцов + mass_au)\n",
    "# Каждый эдж представлен двумя нодами, а общее количество эджей в графе = 310 (при пороге в 3.5 А)\n",
    "# 7 - количество таргетов\n",
    "\n",
    "graph_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "# Графы были сгенерированы для каждого наблюдения \n",
    "# Проверяю количество графов \n",
    "\n",
    "num_graphs = len(graph_data_list)\n",
    "print(f\"Number of graphs: {num_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Визуализирую распределение количества нодов и эджей для каждого графа\n",
    "\n",
    "num_nodes = []\n",
    "num_edges = []\n",
    "\n",
    "for graph_data in graph_data_list:\n",
    "    num_nodes.append(graph_data.num_nodes)\n",
    "    num_edges.append(graph_data.num_edges // 2) \n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(num_nodes, bins=20, edgecolor='black')\n",
    "plt.xlabel('Number of Nodes', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Number of Nodes', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(num_edges, bins=20, edgecolor='black')\n",
    "plt.xlabel('Number of Edges', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Number of Edges', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Визуализирую первые 5 графов\n",
    "\n",
    "num_graphs_to_visualize = 5\n",
    "\n",
    "for i in range(num_graphs_to_visualize):\n",
    "    graph_data = graph_data_list[i]\n",
    "    num_nodes = graph_data.num_nodes\n",
    "    edge_index = graph_data.edge_index.numpy().T\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    G.add_edges_from(edge_index)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G) \n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=500, edge_color='gray', linewidths=1, font_size=12)\n",
    "    plt.title(f'Graph {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Матрица корреляции признаков нод (HF-3c признаков + mass_au) и таргетов (DFT)\n",
    "\n",
    "hf_features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "               'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_homo_ev', 'hf_lumo_ev']\n",
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_homo_ev', 'dft_lumo_ev']\n",
    "\n",
    "correlation_matrix = df[hf_features + dft_features].corr().loc[hf_features, dft_features]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix between HF Features and DFT Targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Cредняя абсолютная корреляция каждого HF-3c признака с любым таргетом DFT\n",
    "# Максимальная абсолютная корреляция каждого HF-3c признака с любым таргетом DFT\n",
    "\n",
    "average_correlations = correlation_matrix.abs().mean(axis=1)\n",
    "ranked_features_avg = average_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by average absolute correlation:\\n\",ranked_features_avg)\n",
    "\n",
    "max_correlations = correlation_matrix.abs().max(axis=1)\n",
    "ranked_features_max = max_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by maximum absolute correlation:\\n\",ranked_features_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Функция для гистограмм распределения значений\n",
    "\n",
    "def plot_histograms(features, title):\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.hist(df[feature], bins=30, edgecolor='k', alpha=0.7)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# HF-3c (гистограммы распределения)\n",
    "\n",
    "plot_histograms(hf_features, 'Distribution Histograms for HF Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# DFT (гистограммы распределения)\n",
    "\n",
    "plot_histograms(dft_features, 'Distribution Histograms for DFT Targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# Функция для диаграмм \"Ящик с усами\"\n",
    "\n",
    "def plot_boxplots(features, title):\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.boxplot(df[feature].dropna(), vert=True, patch_artist=True)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_ylabel('Value')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# HF-3c (\"Ящик с усами\")\n",
    "\n",
    "plot_boxplots(hf_features, 'Box-and-Whisker Diagrams for HF Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# DFT (\"Ящик с усами\")\n",
    "\n",
    "plot_boxplots(dft_features, 'Box-and-Whisker Diagrams for DFT Targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разведочный анализ данных (EDA)\"\"\"\n",
    "# P-значения корреляции HF-3c признаков с каждым DFT таргетом\n",
    "\n",
    "p_values = pd.DataFrame(index=hf_features, columns=dft_features)\n",
    "\n",
    "for hf in hf_features:\n",
    "    for dft in dft_features:\n",
    "        _, p_value = pearsonr(df[hf], df[dft])\n",
    "        p_values.loc[hf, dft] = p_value\n",
    "\n",
    "print(\"\\nP-values for correlations between HF features and DFT targets:\\n\", p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Делю на тренировочную, тестовую и валидационную выборки \n",
    "# 80 / 10 / 10\n",
    "\n",
    "train_val_graphs, test_graphs = train_test_split(graph_data_list, test_size=0.1, random_state=42)\n",
    "train_graphs, val_graphs = train_test_split(train_val_graphs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
    "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
    "print(f\"Number of test graphs: {len(test_graphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Нормализация \n",
    "# Проверяю наличие пустых графов\n",
    "\n",
    "empty_graphs = [i for i, graph in enumerate(graph_data_list) if graph.x.shape[0] == 0]\n",
    "print(f\"Empty graphs indices: {empty_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Нормализация \n",
    "# Извлекаю все признаки тренировочных графов для обучения StandardScaler()\n",
    "\n",
    "node_features = np.vstack([graph.x.numpy() for graph in train_graphs])\n",
    "target_values = np.vstack([graph.y.numpy() for graph in train_graphs])\n",
    "\n",
    "feature_scaler = StandardScaler().fit(node_features)\n",
    "target_scaler = StandardScaler().fit(target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Нормализация \n",
    "# Функция преобразует признаки в тренировочном, валидационном и тестовом сетах \n",
    "\n",
    "def transform_graphs(graphs, feature_scaler, target_scaler):\n",
    "    for graph in graphs:\n",
    "        graph.x = torch.tensor(feature_scaler.transform(graph.x.numpy()), dtype=torch.float)\n",
    "        graph.y = torch.tensor(target_scaler.transform(graph.y.numpy().reshape(1, -1)), dtype=torch.float).view(-1)\n",
    "\n",
    "transform_graphs(train_graphs, feature_scaler, target_scaler)\n",
    "transform_graphs(val_graphs, feature_scaler, target_scaler)\n",
    "transform_graphs(test_graphs, feature_scaler, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Архитерктура\n",
    "# Параметры \"Early Stopping\"\n",
    "\n",
    "num_epochs = 400\n",
    "patience = 10\n",
    "min_delta = 0.0001\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Архитерктура\n",
    "# Поиск оптимальных гиперпараметров с помощью пятикратной \"KFold\" кросс-валидации\n",
    "# Создаю класс ГСН, который принимает гиперпараметры (такой же класс будет создан еще раз, но уже для обучения и оценки)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_dim': [16, 32, 64],\n",
    "    'num_layers': [4, 8, 12],\n",
    "    'learning_rate': [0.00001, 0.0001, 0.001],\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop', 'Adagrad'],\n",
    "    'activation_function': ['relu', 'sigmoid', 'tanh', 'softmax']\n",
    "}\n",
    "\n",
    "results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, activation_function):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = getattr(F, self.activation_function)(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "for train_index, val_index in kf.split(train_val_graphs):\n",
    "    train_graphs = [train_val_graphs[i] for i in train_index]\n",
    "    val_graphs = [train_val_graphs[i] for i in val_index]\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        hidden_dim = params['hidden_dim']\n",
    "        num_layers = params['num_layers']\n",
    "        learning_rate = params['learning_rate']\n",
    "        optimizer_name = params['optimizer']\n",
    "        activation_function = params['activation_function']\n",
    "        \n",
    "        print(f\"Training with Hyperparameters: Hidden Dim={hidden_dim}, Num Layers={num_layers}, Learning Rate={learning_rate}, Optimizer={optimizer_name}, Activation Function={activation_function}\")\n",
    "        \n",
    "        input_dim = node_features.shape[1]\n",
    "        output_dim = 7\n",
    "        \n",
    "        model = GCN(input_dim, hidden_dim, output_dim, num_layers, activation_function)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        optimizer = None\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for data in train_graphs:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        for data in val_graphs:\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                val_loss = mean_squared_error(data.y.numpy(), output.numpy())\n",
    "                val_losses.append(val_loss)\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        print(f\"Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Hidden Dim': hidden_dim,\n",
    "            'Num Layers': num_layers,\n",
    "            'Learning Rate': learning_rate,\n",
    "            'Optimizer': optimizer_name,\n",
    "            'Activation Function': activation_function,\n",
    "            'Validation Loss': avg_val_loss\n",
    "        })\n",
    "\n",
    "best_params = min(results, key=lambda x: x['Validation Loss'])\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Наилучшая комбинация гиперпараметров\n",
    "\n",
    "best_hidden_dim = best_params['Hidden Dim']\n",
    "best_num_layers = best_params['Num Layers']\n",
    "best_learning_rate = best_params['Learning Rate']\n",
    "best_optimizer_name = best_params['Optimizer']\n",
    "best_activation_function = best_params['Activation Function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Сохраняю тип наилучшего оптимайзера для последующей ссылки на него\n",
    "\n",
    "best_optimizer = None\n",
    "if best_optimizer_name == 'Adam':\n",
    "    best_optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "elif best_optimizer_name == 'SGD':\n",
    "    best_optimizer = torch.optim.SGD(model.parameters(), lr=best_learning_rate)\n",
    "elif best_optimizer_name == 'RMSprop':\n",
    "    best_optimizer = torch.optim.RMSprop(model.parameters(), lr=best_learning_rate)\n",
    "elif best_optimizer_name == 'Adagrad':\n",
    "    best_optimizer = torch.optim.Adagrad(model.parameters(), lr=best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Создал модель ГСН, которая принимает лучшие значения гиперпараметров после кросс-валидации\n",
    "    \n",
    "best_model = GCN(input_dim, best_hidden_dim, output_dim, best_num_layers, best_activation_function, best_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Обучение и валидация модели ГСН\n",
    "# Применил \"Early Stopping\" для исключения возможности переобучения\n",
    "# Веса лучшей эпохи сохранил \n",
    "# Построил Train & Validation кривые\n",
    "\n",
    "best_model_weights = None\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_val_graphs:\n",
    "        best_model.train()\n",
    "        best_optimizer.zero_grad()\n",
    "        output = best_model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_val_graphs)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    val_epoch_losses = []\n",
    "    for data in val_graphs:\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = best_model(data)\n",
    "            val_loss = mean_squared_error(data.y.numpy(), output.numpy())\n",
    "            val_epoch_losses.append(val_loss)\n",
    "    avg_val_loss = np.mean(val_epoch_losses)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_weights = best_model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve == patience:\n",
    "        print(f'Early stopping after {epoch + 1} epochs.')\n",
    "        break\n",
    "\n",
    "if best_model_weights is not None:\n",
    "    best_model.load_state_dict(best_model_weights)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Оцениваю модель на тестовой выборке\n",
    "# Применяю inverse_tranform()\n",
    "# Создаю массивы с предсказанными и фактическими значениями для последующей визуализации\n",
    "\n",
    "test_losses = []\n",
    "test_predictions = []\n",
    "original_targets = []\n",
    "\n",
    "for data in test_graphs:\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = best_model(data)\n",
    "        test_loss = criterion(output, data.y).item()\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        model_output_unscaled = target_scaler.inverse_transform(output.numpy().reshape(1, -1)).flatten()\n",
    "        test_predictions.append(model_output_unscaled)\n",
    "        \n",
    "        original_target_unscaled = target_scaler.inverse_transform(data.y.numpy().reshape(1, -1)).flatten()\n",
    "        original_targets.append(original_target_unscaled)\n",
    "\n",
    "average_test_loss = np.mean(test_losses)\n",
    "print(f\"Average Test Loss of Scaled Data: {average_test_loss}\")\n",
    "\n",
    "mae = mean_absolute_error(np.concatenate(original_targets), np.concatenate(test_predictions))\n",
    "print(f\"Average Mean Absolute Error (MAE) of Inversed Data: {mae}\")\n",
    "\n",
    "original_targets = np.array(original_targets)\n",
    "test_predictions = np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Графовая сверточная нейросеть\"\"\"\n",
    "# Визуализация результатов\n",
    "\n",
    "maes = []\n",
    "num_targets = original_targets.shape[1]\n",
    "target_names = [\n",
    "    'Gibbs Free Energy (DFT)', 'Electronic Energy (DFT)', 'Entropy (DFT)', \n",
    "    'Enthalpy (DFT)', 'Dipole Moment (DFT)', 'HOMO (DFT)', 'LUMO (DFT)'\n",
    "]\n",
    "\n",
    "for i in range(num_targets):\n",
    "    mae = mean_absolute_error(original_targets[:, i], test_predictions[:, i])\n",
    "    maes.append(mae)\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = (num_targets + 1) // num_rows\n",
    "num_plots = num_rows * num_cols \n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, 12))\n",
    "\n",
    "if num_plots > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes] \n",
    "\n",
    "for i in range(num_targets):\n",
    "    if i < num_plots: \n",
    "        ax = axes[i]\n",
    "        ax.scatter(original_targets[:, i], test_predictions[:, i], alpha=0.5)\n",
    "        ax.plot([original_targets[:, i].min(), original_targets[:, i].max()], \n",
    "                [original_targets[:, i].min(), original_targets[:, i].max()], 'r--')\n",
    "        ax.set_xlabel('Actual')\n",
    "        ax.set_ylabel('Predicted')\n",
    "        ax.set_title(target_names[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax.text(0.05, 0.95, f\"MAE: {maes[i]:.4f}\", transform=ax.transAxes, fontsize=12,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "for j in range(num_targets, num_plots):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
