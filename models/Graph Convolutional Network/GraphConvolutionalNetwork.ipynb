{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The data represents the energy values of supramolecular systems, which were calculated using two different quantum chemical approximations. The \"HF\" (Hartree-Fock) set was calculated using a fast and inaccurate approximation, while \"DFT\" (Density Functional Theory) was calculated using a resource-intensive but accurate approximation.\n",
    "\n",
    "Feature  | Feature Type | Description\n",
    "-------------------|--------------------|--------------------\n",
    "dft_gibbs_free_energy_ev       |Target| Gibbs free energy of the supramolecular system, calculated using the DFT approximation \n",
    "dft_electronic_energy_ev       |Target| Electronic energy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_entropy_ev       |Target| Entropy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_enthalpy_ev       |Target| Enthalpy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_dipole_moment_d       |Target| Dipole moment of the supramolecular system, calculated using the DFT approximation\n",
    "dft_gap_ev      |Target| Energy gap between HOMO and LUMO, calculated using the DFT approximation\n",
    "hf_gibbs_free_energy_ev       |Training| Gibbs free energy of the supramolecular system, calculated using the HF approximation \n",
    "hf_electronic_energy_ev       |Training| Electronic energy of the supramolecular system, calculated using the HF approximation\n",
    "hf_entropy_ev       |Training| Entropy of the supramolecular system, calculated using the HF approximation\n",
    "hf_enthalpy_ev       |Training| Enthalpy of the supramolecular system, calculated using the HF approximation\n",
    "hf_dipole_moment_d       |Training| Dipole moment of the supramolecular system, calculated using the HF approximation\n",
    "hf_gap_ev      |Training| Energy gap between HOMO and LUMO, calculated using the HF approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Check if the GPU is available on the device\n",
    "#   If not available, the process will run on CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Load data from a CSV file\n",
    "df = pd.read_csv(\"./NN_ML.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_molecule(mol):\n",
    "    \"\"\"\n",
    "    Function to call the Chem.SanitizeMol(mol) method to check and fix structural errors in a molecule.\n",
    "    \n",
    "    Parameters:\n",
    "    - mol: RDKit molecule object.\n",
    "    \n",
    "    Returns:\n",
    "    - mol: RDKit molecule object if the check is successful.\n",
    "    - None: If an exception occurs during the check.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to sanitize molecule: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_sdf_files(directory):\n",
    "    \"\"\"\n",
    "    Function to read SDF (Structure Data File) files in a directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory: Path to the directory with SDF files.\n",
    "    \n",
    "    Returns:\n",
    "    - molecules: List of tuples (file_name, sanitized_mol) for successfully read molecules.\n",
    "    \"\"\"\n",
    "    molecules = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.sdf'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            try:\n",
    "                suppl = Chem.SDMolSupplier(file_path)\n",
    "                for mol in suppl:\n",
    "                    if mol is not None:\n",
    "                        sanitized_mol = sanitize_molecule(mol)\n",
    "                        if sanitized_mol is not None:\n",
    "                            molecules.append((file_name, sanitized_mol))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {file_name}: {str(e)}\")\n",
    "                failed_files.append(file_name)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"Failed to read {len(failed_files)} files.\")\n",
    "    \n",
    "    return molecules\n",
    "\n",
    "def compute_rdkit_features(mol):\n",
    "    \"\"\"\n",
    "    Function to compute molecule features (fingerprints) using the RDKit library.\n",
    "    \n",
    "    Parameters:\n",
    "    - mol: RDKit molecule object.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Array of molecule features.\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return None\n",
    "    morgan_fp = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=256)\n",
    "    return np.array(morgan_fp)\n",
    "\n",
    "#-# Read structures from SDF files\n",
    "sdf_directory = \"./sdf\"\n",
    "molecules = read_sdf_files(sdf_directory)\n",
    "\n",
    "#-# Extract properties and map them to DataFrame\n",
    "mol_features = {}\n",
    "mol_objects = {}\n",
    "for file_name, mol in molecules:\n",
    "    mol_name = os.path.splitext(file_name)[0] \n",
    "    features = compute_rdkit_features(mol)\n",
    "    mol_features[mol_name] = features\n",
    "    mol_objects[mol_name] = mol\n",
    "\n",
    "df['rdkit_features'] = df['name'].map(mol_features)\n",
    "df['rdkit_molecules'] = df['name'].map(mol_objects)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Visualize 5 random systems from the dataframe\n",
    "#   Systems that do not start with \"dimer_\" may contain more than 2 molecules\n",
    "\n",
    "def visualize_random_molecules_from_df(df, num_molecules=5):\n",
    "    sampled_df = df.sample(n=num_molecules)\n",
    "    mols = sampled_df['rdkit_molecules'].tolist()\n",
    "    labels = sampled_df['name'].tolist()\n",
    "    img = Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(200, 200), legends=labels)\n",
    "    return img\n",
    "\n",
    "img = visualize_random_molecules_from_df(df, num_molecules=5)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Graph representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(df):\n",
    "    \"\"\"\n",
    "    Function to create a graph representation of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame containing data about molecules and their chemical properties.\n",
    "    \n",
    "    Returns:\n",
    "    - graph_data_list: List of Data objects representing the graph data.\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        rdkit_features = row['rdkit_features']\n",
    "        rdkit_mol = row['rdkit_molecules']\n",
    "        if rdkit_features is None or rdkit_mol is None:\n",
    "            continue  \n",
    "\n",
    "        rdkit_mol = Chem.AddHs(rdkit_mol)\n",
    "        num_atoms = rdkit_mol.GetNumAtoms()\n",
    "        \n",
    "        hf_features = row[['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "                           'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev']].astype(np.float64).values\n",
    "        \n",
    "        node_features_list = []\n",
    "        \n",
    "        for atom_idx in range(num_atoms):\n",
    "            atom = rdkit_mol.GetAtomWithIdx(atom_idx)\n",
    "            \n",
    "            atom_features = [\n",
    "                atom.GetAtomicNum(),            #-# Atomic number\n",
    "                atom.GetExplicitValence(),      #-# Valence       \n",
    "                atom.GetFormalCharge(),         #-# Formal charge       \n",
    "                atom.GetIsAromatic(),           #-# Aromaticity       \n",
    "                atom.GetMass()                  #-# Mass\n",
    "            ]\n",
    "            \n",
    "            combined_features = np.concatenate([hf_features, rdkit_features, atom_features])\n",
    "            node_features_list.append(combined_features)\n",
    "        \n",
    "        node_features = np.array(node_features_list, dtype=np.float64)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float64).to(device)\n",
    "        \n",
    "        edge_index = []\n",
    "        for bond in rdkit_mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)\n",
    "        \n",
    "        graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "        graph_data.y = torch.tensor(row[['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                                         'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']].astype(np.float64).values, dtype=torch.float64).to(device)\n",
    "        graph_data_list.append(graph_data)\n",
    "    \n",
    "    return graph_data_list\n",
    "\n",
    "graph_data_list = create_graph_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Graphs have been generated for each observation \n",
    "#   Check the number of graphs\n",
    "\n",
    "num_graphs = len(graph_data_list)\n",
    "print(f\"Number of graphs: {num_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Visualize first 5 graphs \n",
    "\n",
    "num_graphs_to_visualize = 5\n",
    "\n",
    "for i in range(num_graphs_to_visualize):\n",
    "    graph_data = graph_data_list[i]\n",
    "    num_nodes = graph_data.num_nodes\n",
    "    edge_index = graph_data.edge_index.numpy().T\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    G.add_edges_from(edge_index)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G) \n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=500, edge_color='gray', linewidths=1, font_size=12)\n",
    "    plt.title(f'Graph {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% confidence intervals for target (DFT) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for feature in dft_features:\n",
    "    mean = df[feature].mean()\n",
    "    std_dev = df[feature].std()\n",
    "    n = len(df)\n",
    "    std_error = std_dev / np.sqrt(n)\n",
    "    z_score = stats.norm.ppf(0.975) \n",
    "    margin_of_error = z_score * std_error\n",
    "    ci_lower = mean - margin_of_error\n",
    "    ci_upper = mean + margin_of_error\n",
    "    ci_width = ci_upper - ci_lower\n",
    "    \n",
    "    metrics[feature] = {\n",
    "        '95% CI Lower': ci_lower,\n",
    "        '95% CI Upper': ci_upper,\n",
    "        '95% CI Width': ci_width\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sactter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "               'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev']\n",
    "\n",
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "features_of_interest = hf_features + dft_features\n",
    "\n",
    "for feature in features_of_interest:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df.index, df[feature], alpha=0.6)\n",
    "    plt.title(f'{feature}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(f'{feature} Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix between training (HF) and target (DFT) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "               'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev']\n",
    "\n",
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "correlation_matrix = df[hf_features + dft_features].corr().loc[hf_features, dft_features]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix between HF Features and DFT Targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Absolute Correlation of each training (HF) feature with any (DFT) target\n",
    "### Maximum Absolute Correlation of each training (HF) feature with any (DFT) target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_correlations = correlation_matrix.abs().mean(axis=1)\n",
    "ranked_features_avg = average_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by average absolute correlation:\\n\",ranked_features_avg)\n",
    "\n",
    "max_correlations = correlation_matrix.abs().max(axis=1)\n",
    "ranked_features_max = max_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by maximum absolute correlation:\\n\",ranked_features_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(features, title):\n",
    "    \"\"\"\n",
    "    Function to plot histograms.\n",
    "\n",
    "    Parameters:\n",
    "    - features (list): List of feature names for which histograms are to be plotted.\n",
    "    - title (str): Title for the entire plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    This function generates subplot histograms for each specified feature, arranging them in a grid.\n",
    "    It dynamically adjusts the layout based on the number of features for optimal display.\n",
    "    \"\"\"\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.hist(df[feature], bins=30, edgecolor='k', alpha=0.7)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms for training (HF) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(hf_features, 'Distribution Histograms for HF Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms for target (DFT) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(dft_features, 'Distribution Histograms for DFT Targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(features, title):\n",
    "    \"\"\"\n",
    "    Function to create box-and-whiskers diagrams.\n",
    "\n",
    "    Parameters:\n",
    "    - features (list): List of feature names for which to create boxplots.\n",
    "    - title (str): Title for the entire plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    This function generates subplots of boxplots for each specified feature, arranging them in a grid.\n",
    "    It dynamically adjusts the layout based on the number of features for optimal display.\n",
    "    \"\"\"\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.boxplot(df[feature].dropna(), vert=True, patch_artist=True)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_ylabel('Value')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams for training (HF) set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(hf_features, 'Box-and-Whisker Diagrams for HF Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams for target (DFT) set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(dft_features, 'Box-and-Whisker Diagrams for DFT Targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values for training (HF) and target (DFT) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.DataFrame(index=hf_features, columns=dft_features)\n",
    "\n",
    "for hf in hf_features:\n",
    "    for dft in dft_features:\n",
    "        _, p_value = pearsonr(df[hf], df[dft])\n",
    "        p_values.loc[hf, dft] = p_value\n",
    "\n",
    "print(\"\\nP-values for correlations between HF features and DFT targets:\\n\", p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('./NN_ML.csv')\n",
    "dataframe = dataframe.drop(columns=['mass_au', 'name'])\n",
    "dataframe = dataframe.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "def detect_outliers_iqr(data):\n",
    "    \"\"\"\n",
    "    Function to detect outliers in data using the Interquartile Range (IQR).\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.Series or pandas.DataFrame): One-dimensional or two-dimensional dataset for outlier analysis.\n",
    "\n",
    "    Returns:\n",
    "    - outliers (pandas.Series or pandas.DataFrame): Boolean array of the same shape as the input data,\n",
    "      indicating the presence (True) or absence (False) of outliers in the data.\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
    "    return outliers\n",
    "\n",
    "outliers = dataframe.apply(detect_outliers_iqr)\n",
    "print(\"\\nOutliers detected in each feature:\\n\", outliers.sum())\n",
    "\n",
    "sns.pairplot(dataframe[hf_features + dft_features])\n",
    "plt.suptitle('Pairplot of HF and DFT Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Create a list of original indices for further mapping\n",
    "original_indices = np.arange(len(graph_data_list))\n",
    "\n",
    "train_val_graphs, test_graphs, train_val_indices, test_indices = train_test_split(\n",
    "    graph_data_list, original_indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_graphs, val_graphs, train_indices, val_indices = train_test_split(\n",
    "    train_val_graphs, train_val_indices, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
    "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
    "print(f\"Number of test graphs: {len(test_graphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Check if data contains empty graphs\n",
    "\n",
    "empty_graphs = [i for i, graph in enumerate(graph_data_list) if graph.x.shape[0] == 0]\n",
    "print(f\"Empty graphs indices: {empty_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Extract node features (graph.x.numpy()) from the training set train_graphs, representing the node feature matrices for each graph\n",
    "#   np.vstack is used to vertically concatenate these feature matrices into one large node_features matrix \n",
    "node_features = np.vstack([graph.x.cpu().numpy() for graph in train_graphs])\n",
    "\n",
    "#-# Extract target values (graph.y.numpy()) from the training set train_graphs, representing the target value matrices for each graph\n",
    "#   np.vstack is used to vertically concatenate these target value matrices into one large target_values matrix \n",
    "target_values = np.vstack([graph.y.cpu().numpy() for graph in train_graphs])\n",
    "\n",
    "#-# Use StandardScaler() to normalize the node features\n",
    "feature_scaler = StandardScaler().fit(node_features)\n",
    "\n",
    "#-# Use MinMaxScaler() to normalize the target values to the 0 to 1 range \n",
    "target_scaler = MinMaxScaler().fit(target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dir = \"./data\"\n",
    "\n",
    "feature_scaler_path = os.path.join(scaler_dir, \"feature_scaler.pkl\")\n",
    "target_scaler_path = os.path.join(scaler_dir, \"target_scaler.pkl\")\n",
    "\n",
    "joblib.dump(feature_scaler, feature_scaler_path)\n",
    "joblib.dump(target_scaler, target_scaler_path)\n",
    "\n",
    "print(f\"Scalers saved to {scaler_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_graphs(graphs, feature_scaler, target_scaler):\n",
    "    \"\"\"\n",
    "    Function to transform the features (graph.x) and target variables (graph.y) in the graphs used in the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - graphs: List of Data objects.\n",
    "    - feature_scaler: Scaler for the features.\n",
    "    - target_scaler: Scaler for the target values.\n",
    "\n",
    "    Returns:\n",
    "    - None: Transforms features and targets in place.\n",
    "    \"\"\"\n",
    "    for graph in graphs:\n",
    "        graph.x = torch.tensor(feature_scaler.transform(graph.x.cpu().numpy()), dtype=torch.float64).to(device)\n",
    "        graph.y = torch.tensor(target_scaler.transform(graph.y.cpu().numpy().reshape(1, -1)), dtype=torch.float64).view(-1).to(device)\n",
    "\n",
    "transform_graphs(train_graphs, feature_scaler, target_scaler)\n",
    "transform_graphs(val_graphs, feature_scaler, target_scaler)\n",
    "transform_graphs(test_graphs, feature_scaler, target_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Architecture of \"Graph Convolutional Network\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Define GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network (GCN) using PyTorch.\n",
    "\n",
    "    This class defines a GCN with multiple GCNConv layers followed by a fully connected layer.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dim (int): Number of input features per node.\n",
    "    - hidden_dim (int): Number of features in hidden layers.\n",
    "    - output_dim (int): Number of output features.\n",
    "    - num_layers (int): Number of GCNConv layers.\n",
    "    - activation_function (str): Activation function name (e.g., 'relu').\n",
    "\n",
    "    Methods:\n",
    "    - forward(data): Executes a forward pass through the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, activation_function, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()                                #-# List to hold GCNConv layers\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))           #-# Initial GCNConv layer\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))      #-# Additional GCNConv layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)                 #-# Fully connected layer for output\n",
    "        self.activation_function = activation_function              #-# Activation function for layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)                   #-# Add Dropout layer for regularization\n",
    "        self.float()                                                \n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through the GCN model.\n",
    "\n",
    "        Parameters:\n",
    "        - data (Data): A PyTorch Geometric Data object containing:\n",
    "          - x (Tensor): Node features.\n",
    "          - edge_index (Tensor): Graph connectivity in COO format.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output feature tensor after passing through all layers.\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        #-# Apply GCNConv layers and activation function\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = getattr(F, self.activation_function)(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        #-# Aggregate node features\n",
    "        x = torch.mean(x, dim=0)\n",
    "\n",
    "        #-# Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Set the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#-# Set the dimensions of the input and output layers \n",
    "input_dim = node_features.shape[1]\n",
    "output_dim = 6\n",
    "        \n",
    "num_epochs = 1000         #-# Sets the total number of times the model will process the entire dataset to update weights and reduce error\n",
    "patience = 20             #-# Sets the number of epochs to tolerate without improvement in validation metric (early stopping)\n",
    "min_delta = 0.0001        #-# Minimum change required in validation loss to be considered as an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective function for Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna that defines the task for hyperparameter search.\n",
    "\n",
    "    Parameters:\n",
    "    - trial (optuna.trial.Trial): Optuna trial object used to suggest hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean MSE value across all folds of cross-validation.\n",
    "    \"\"\"\n",
    "    #-# Set the hyperparameter grid\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 5, 200, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 20)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop', 'NAdam', 'RAdam', 'AdamW'])\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'sigmoid', 'tanh', 'leaky_relu'])\n",
    "\n",
    "    #-# Create a KFold object for 3-fold cross-validation with data shuffling\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "\n",
    "    #-# Loop through the data splits\n",
    "    for train_index, val_index in kf.split(train_graphs):\n",
    "        train_graphs_cv = [train_graphs[i] for i in train_index]\n",
    "        val_graphs_cv = [train_graphs[i] for i in val_index]\n",
    "        \n",
    "        #-# Create a model with the current hyperparameter values\n",
    "        model = GCN(input_dim, hidden_dim, output_dim, num_layers, activation_function, dropout_rate).double()\n",
    "        model.to(device)\n",
    "\n",
    "        #-# Choose optimizer\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'NAdam':\n",
    "            optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'RAdam':\n",
    "            optimizer = torch.optim.RAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "        #-# Counters for EarlyStopping\n",
    "        #   best_val_loss:     Initial value for the variable that will store the best (lowest) validation loss \n",
    "        #   epochs_no_improve: Counter for epochs without improvement. Once this counter reaches the patience value, training is stopped\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        #-# Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for data in train_graphs_cv:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #-# Evaluate on the validation set\n",
    "            model.eval()\n",
    "            val_epoch_losses = []\n",
    "            for data in val_graphs_cv:\n",
    "                data = data.to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(data)\n",
    "                    val_loss = mean_squared_error(data.y.cpu().numpy(), output.cpu().numpy())\n",
    "                    val_epoch_losses.append(val_loss)\n",
    "\n",
    "            avg_val_loss = np.mean(val_epoch_losses)\n",
    "\n",
    "            #-# Apply EarlyTopping technique to prevent overfitting\n",
    "            if avg_val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "        \n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Print the best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "print(f'Best trial value: {best_trial.value}')\n",
    "print(f'Best hyperparameters: {best_trial.params}')\n",
    "\n",
    "with open('./optimization_results.txt', 'w') as f:\n",
    "    f.write(f'Best trial value: {best_trial.value}\\n')\n",
    "    f.write(f'Best hyperparameters: {best_trial.params}\\n')\n",
    "\n",
    "    f.write('\\nAll trial results:\\n')\n",
    "    for trial in study.trials:\n",
    "        f.write(f'Trial {trial.number}: Value={trial.value}, Params={trial.params}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Print the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Get the best hyperparameters\n",
    "best_hidden_dim = best_params['hidden_dim']\n",
    "best_num_layers = best_params['num_layers']\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "best_dropout_rate = best_params['dropout_rate']\n",
    "best_optimizer_name = best_params['optimizer']\n",
    "best_activation_function = best_params['activation_function']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Create the model with best hyperparameters \n",
    "#   Model is initialized with double() for double precision\n",
    "best_model = GCN(input_dim, best_hidden_dim, output_dim, best_num_layers, best_activation_function, best_dropout_rate).double()\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Get the best optimizer\n",
    "best_optimizer = None\n",
    "if best_optimizer_name == 'Adam':\n",
    "    best_optimizer = torch.optim.Adam(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "elif best_optimizer_name == 'SGD':\n",
    "    best_optimizer = torch.optim.SGD(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "elif best_optimizer_name == 'RMSprop':\n",
    "    best_optimizer = torch.optim.RMSprop(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "elif best_optimizer_name == 'NAdam':\n",
    "    best_optimizer = torch.optim.NAdam(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "elif best_optimizer_name == 'RAdam':\n",
    "    best_optimizer = torch.optim.RAdam(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "elif best_optimizer_name == 'AdamW':\n",
    "    best_optimizer = torch.optim.AdamW(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_optimizer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Variables to track the best model\n",
    "#   best_model_weights is initialized as None to store the best model weights\n",
    "#   best_val_loss is initialized as infinity (float('inf')) to track the best validation loss\n",
    "#   epochs_no_improve is initialized as 0 to track the number of epochs without improvements\n",
    "best_model_weights = None\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Create empty lists to append results \n",
    "train_losses = []\n",
    "val_losses = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# total_loss computes the total loss on all training graphs\n",
    "#-# In each epoch, the mean squared error on the validation set (val_graphs) is also computed\n",
    "#-# train_losses and val_losses store the average loss for each epoch for later plotting of training curves\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_graphs:\n",
    "        data = data.to(device)\n",
    "        best_model.train()\n",
    "        best_optimizer.zero_grad()\n",
    "        output = best_model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_graphs)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    val_epoch_losses = []\n",
    "    for data in val_graphs:\n",
    "        data = data.to(device)\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = best_model(data)\n",
    "            val_loss = mean_squared_error(data.y.cpu().numpy(), output.cpu().numpy())\n",
    "            val_epoch_losses.append(val_loss)\n",
    "    \n",
    "    #-# If validation loss (avg_val_loss) improves (less than best_val_loss - min_delta), update best_val_loss \n",
    "    #   epochs_no_improve is reset to 0, and best model weights (best_model_weights) are saved\n",
    "    avg_val_loss = np.mean(val_epoch_losses)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_weights = best_model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve == patience:\n",
    "        print(f'Early stopping after {epoch + 1} epochs.')\n",
    "        break\n",
    "\n",
    "#-# If best model weights were saved (best_model_weights is not None), load them back into the best_model\n",
    "if best_model_weights is not None:\n",
    "    best_model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Train & Val curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Plot Train & Val curves\n",
    "#   Save them as training_validation_loss.png\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "\n",
    "plot_path = \"./training_validation_loss.png\"  \n",
    "plt.savefig(plot_path)\n",
    "\n",
    "print(f\"Saved Train vs. Val plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./best_gcn_model.pt\"\n",
    "\n",
    "#-# model_info creates a dictionary containing information about the model parameters to be saved \n",
    "model_info = {\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'output_dim': output_dim,\n",
    "    'num_layers': best_num_layers,\n",
    "    'activation_function': best_activation_function,\n",
    "    'dropout_rate': best_dropout_rate\n",
    "}\n",
    "\n",
    "#-# torch.save saves the dictionary with model information:\n",
    "#   - model information;\n",
    "#   - model state dictionary;\n",
    "#   - optimizer state dictionary \n",
    "torch.save({\n",
    "    'model_info': model_info,\n",
    "    'state_dict': best_model.state_dict(),\n",
    "    'optimizer_state_dict': best_optimizer.state_dict(),\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Saved model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Predict values on the test data\n",
    "test_losses = []\n",
    "test_predictions = []\n",
    "original_targets = []\n",
    "predicted_indices = []\n",
    "\n",
    "for idx, data in zip(test_indices, test_graphs):\n",
    "    \n",
    "    #-# Set the model to evaluation mode to disable dropout and batch normalization\n",
    "    best_model.eval()\n",
    "\n",
    "    #-# Disable gradient computation to speed up calculations and save memory\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #-# Obtain model predictions for the current graph data\n",
    "        output = best_model(data)\n",
    "\n",
    "        #-# Compute the loss function on the current test data\n",
    "        test_loss = criterion(output, data.y).item()\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #-# Inverse transform model predictions from normalized state\n",
    "        model_output_unscaled = target_scaler.inverse_transform(output.cpu().numpy().reshape(1, -1)).flatten()\n",
    "        \n",
    "        #-# Append transformed predictions to test_predictions list\n",
    "        test_predictions.append(model_output_unscaled)\n",
    "        \n",
    "        #-# Inverse transform the original target values from normalized state\n",
    "        original_target_unscaled = target_scaler.inverse_transform(data.y.cpu().numpy().reshape(1, -1)).flatten()\n",
    "        original_targets.append(original_target_unscaled)\n",
    "        \n",
    "        #-# Append the index of the current graph to the predicted_indices list\n",
    "        predicted_indices.append(idx)\n",
    "\n",
    "#-# Compute the average loss function on the test dataset\n",
    "average_test_loss = np.mean(test_losses)\n",
    "print(f\"Average Test Loss of Scaled Data: {average_test_loss}\")\n",
    "\n",
    "#-# Compute the mean absolute error between concatenated arrays of original target values and predictions\n",
    "mae = mean_absolute_error(np.concatenate(original_targets), np.concatenate(test_predictions))\n",
    "print(f\"Average Mean Absolute Error (MAE) of Inversed Data: {mae}\")\n",
    "\n",
    "#-# Print indices of predicted graphs\n",
    "print(f\"Indices of Predicted Graphs: {predicted_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_targets = np.array(original_targets)\n",
    "test_predictions = np.array(test_predictions)\n",
    "\n",
    "#-# Set title\n",
    "num_targets = original_targets.shape[1]\n",
    "target_names = [\n",
    "    'Gibbs Energy', 'Electronic Energy', 'Entropy', \n",
    "    'Enthalpy', 'Dipole Moment', 'Band Gap'\n",
    "]\n",
    "\n",
    "#-# Create a .csv file with three columns: index, actual value, and predicted value\n",
    "data = {\n",
    "    'Index': predicted_indices,\n",
    "    'Actual_Gibbs_Energy': original_targets[:, 0],\n",
    "    'Predicted_Gibbs_Energy': test_predictions[:, 0],\n",
    "    'Actual_Electronic_Energy': original_targets[:, 1],\n",
    "    'Predicted_Electronic_Energy': test_predictions[:, 1],\n",
    "    'Actual_Entropy': original_targets[:, 2],\n",
    "    'Predicted_Entropy': test_predictions[:, 2],\n",
    "    'Actual_Enthalpy': original_targets[:, 3],\n",
    "    'Predicted_Enthalpy': test_predictions[:, 3],\n",
    "    'Actual_Dipole_Moment': original_targets[:, 4],\n",
    "    'Predicted_Dipole_Moment': test_predictions[:, 4],\n",
    "    'Actual_Band_Gap': original_targets[:, 5],\n",
    "    'Predicted_Band_Gap': test_predictions[:, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#-# Save the file\n",
    "csv_path = \"./test_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved DataFrame to {csv_path}\")\n",
    "\n",
    "#-# Calculate metrics\n",
    "maes = []\n",
    "mapes = []\n",
    "mse_values = []\n",
    "rmse_values = []\n",
    "r2_values = []\n",
    "\n",
    "for i in range(num_targets):\n",
    "    mae = mean_absolute_error(original_targets[:, i], test_predictions[:, i])\n",
    "    maes.append(mae)\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(original_targets[:, i], test_predictions[:, i])\n",
    "    mapes.append(mape)\n",
    "    \n",
    "    mse = mean_squared_error(original_targets[:, i], test_predictions[:, i])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_values.append(rmse)\n",
    "    \n",
    "    r2 = r2_score(original_targets[:, i], test_predictions[:, i])\n",
    "    r2_values.append(r2)\n",
    "\n",
    "#-# Create subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "#-# Create scatter plot with conditional coloring (red if observation number < 48)\n",
    "for i in range(num_targets):\n",
    "    ax = axes[i]\n",
    "    for idx in range(len(df)):\n",
    "        color = 'red' if df['Index'].iloc[idx] < 48 else 'blue'\n",
    "        ax.scatter(df[f'Actual_{target_names[i].replace(\" \", \"_\")}'].iloc[idx],\n",
    "                   df[f'Predicted_{target_names[i].replace(\" \", \"_\")}'].iloc[idx],\n",
    "                   alpha=0.5, color=color)\n",
    "    \n",
    "    #-# Create diagonal line\n",
    "    ax.plot([original_targets[:, i].min(), original_targets[:, i].max()],\n",
    "            [original_targets[:, i].min(), original_targets[:, i].max()],\n",
    "            color='black', linestyle='--')\n",
    "    \n",
    "    #-# Set titles for axes \n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(target_names[i])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    #-# Add metrics to the plots\n",
    "    ax.text(0.05, 0.95, f\"MSE: {mse_values[i]:.4f}\\nRMSE: {rmse_values[i]:.4f}\\nR-squared: {r2_values[i]:.4f}\\nMAE: {maes[i]:.4f}\\nMAPE: {mapes[i]:.2f}%\", \n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "for j in range(num_targets, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = \"./test_results.png\"\n",
    "plt.savefig(plot_path)\n",
    "print(f\"Saved plots to {plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
