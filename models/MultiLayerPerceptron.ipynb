{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\norma\\\\OneDrive\\\\Desktop\\\\NOVICHOK\\\\novichok_356_cross_validation\\\\data\\\\csv\\\\dataset_no_imaginary.csv\",delimiter=\",\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создал функцию, которая проходит по директории и читает наши '.xyz'-файлы\n",
    "# Название каждого файла в моей директории отражено в датасете в столбце 'name' без расширения .xyz \n",
    "# Сконвертировал координаты в numpy-массив и закинул в новый столбец 'coordinates' в датасете\n",
    "\n",
    "coordinates_list = []\n",
    "\n",
    "def read_xyz_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[2:]  \n",
    "        coordinates = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 4:\n",
    "                coordinates.append([float(parts[1]), float(parts[2]), float(parts[3])])\n",
    "        return np.array(coordinates)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    file_name = str(row['name']) + \".xyz\"\n",
    "    file_path = os.path.join(\"C:\\\\Users\\\\norma\\\\OneDrive\\\\Desktop\\\\NOVICHOK\\\\novichok_356_cross_validation\\\\xyz\", file_name)\n",
    "    coordinates = read_xyz_file(file_path)\n",
    "    coordinates_list.append(coordinates)\n",
    "data['coordinates'] = coordinates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>hf_gibbs_free_energy_ev</th>\n",
       "      <th>hf_electronic_energy_ev</th>\n",
       "      <th>hf_entropy_ev</th>\n",
       "      <th>hf_enthalpy_ev</th>\n",
       "      <th>hf_dipole_moment</th>\n",
       "      <th>hf_homo_ev</th>\n",
       "      <th>hf_lumo_ev</th>\n",
       "      <th>dft_gibbs_free_energy_ev</th>\n",
       "      <th>dft_electronic_energy_ev</th>\n",
       "      <th>dft_entropy_ev</th>\n",
       "      <th>dft_enthalpy_ev</th>\n",
       "      <th>dft_dipole_moment</th>\n",
       "      <th>dft_homo_ev</th>\n",
       "      <th>dft_lumo_ev</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002-M-BA1-b</td>\n",
       "      <td>-27275.076351</td>\n",
       "      <td>-27281.888683</td>\n",
       "      <td>1.795316</td>\n",
       "      <td>-27273.281035</td>\n",
       "      <td>0.87236</td>\n",
       "      <td>-8.8130</td>\n",
       "      <td>4.0489</td>\n",
       "      <td>-27617.108248</td>\n",
       "      <td>-27623.000160</td>\n",
       "      <td>1.877779</td>\n",
       "      <td>-27615.230469</td>\n",
       "      <td>1.47466</td>\n",
       "      <td>-6.2925</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>[[-3.6610385002265, 6.93533048376608, -0.58345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002-M-BA1-pipi</td>\n",
       "      <td>-27275.128802</td>\n",
       "      <td>-27281.952769</td>\n",
       "      <td>1.772166</td>\n",
       "      <td>-27273.356635</td>\n",
       "      <td>1.19181</td>\n",
       "      <td>-9.2580</td>\n",
       "      <td>4.4306</td>\n",
       "      <td>-27617.107555</td>\n",
       "      <td>-27623.063060</td>\n",
       "      <td>1.822257</td>\n",
       "      <td>-27615.285298</td>\n",
       "      <td>1.69901</td>\n",
       "      <td>-6.7665</td>\n",
       "      <td>-1.3131</td>\n",
       "      <td>[[-3.24946263013423, 6.99473259895769, -1.4223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005-MEL-CA</td>\n",
       "      <td>-25599.449771</td>\n",
       "      <td>-25604.305425</td>\n",
       "      <td>1.564143</td>\n",
       "      <td>-25597.885628</td>\n",
       "      <td>0.93514</td>\n",
       "      <td>-8.9909</td>\n",
       "      <td>4.6443</td>\n",
       "      <td>-25917.872472</td>\n",
       "      <td>-25922.091294</td>\n",
       "      <td>1.641525</td>\n",
       "      <td>-25916.230947</td>\n",
       "      <td>0.85984</td>\n",
       "      <td>-6.4902</td>\n",
       "      <td>-0.8343</td>\n",
       "      <td>[[-5.415595, -7.691943, 0.824725], [-4.323509,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006-MEL-UA</td>\n",
       "      <td>-29133.284441</td>\n",
       "      <td>-29138.979188</td>\n",
       "      <td>1.668533</td>\n",
       "      <td>-29131.615908</td>\n",
       "      <td>2.68196</td>\n",
       "      <td>-8.2371</td>\n",
       "      <td>3.9070</td>\n",
       "      <td>-29496.320185</td>\n",
       "      <td>-29501.224258</td>\n",
       "      <td>1.791889</td>\n",
       "      <td>-29494.528296</td>\n",
       "      <td>3.15832</td>\n",
       "      <td>-6.1902</td>\n",
       "      <td>-1.1975</td>\n",
       "      <td>[[-5.251705, -7.658659, 1.031666], [-4.20755, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007-MEL-CR</td>\n",
       "      <td>-22635.215921</td>\n",
       "      <td>-22641.289493</td>\n",
       "      <td>1.651416</td>\n",
       "      <td>-22633.564505</td>\n",
       "      <td>6.85797</td>\n",
       "      <td>-8.6600</td>\n",
       "      <td>5.1451</td>\n",
       "      <td>-22922.502813</td>\n",
       "      <td>-22927.782911</td>\n",
       "      <td>1.687934</td>\n",
       "      <td>-22920.814880</td>\n",
       "      <td>6.66567</td>\n",
       "      <td>-6.3708</td>\n",
       "      <td>-0.2151</td>\n",
       "      <td>[[-5.271343, -7.699293, 0.886577], [-4.227247,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>dimer_006121</td>\n",
       "      <td>-19613.756532</td>\n",
       "      <td>-19622.869155</td>\n",
       "      <td>1.793542</td>\n",
       "      <td>-19611.962991</td>\n",
       "      <td>6.65332</td>\n",
       "      <td>-9.2198</td>\n",
       "      <td>6.4409</td>\n",
       "      <td>-19866.663508</td>\n",
       "      <td>-19874.318183</td>\n",
       "      <td>1.856166</td>\n",
       "      <td>-19864.807342</td>\n",
       "      <td>7.42245</td>\n",
       "      <td>-6.4218</td>\n",
       "      <td>0.1743</td>\n",
       "      <td>[[1.30646491596164, -0.57524990278906, -1.3873...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>dimer_006125</td>\n",
       "      <td>-20685.601738</td>\n",
       "      <td>-20693.928747</td>\n",
       "      <td>1.763538</td>\n",
       "      <td>-20683.838200</td>\n",
       "      <td>6.26955</td>\n",
       "      <td>-9.4013</td>\n",
       "      <td>4.9169</td>\n",
       "      <td>-20947.601629</td>\n",
       "      <td>-20954.521584</td>\n",
       "      <td>1.884566</td>\n",
       "      <td>-20945.717064</td>\n",
       "      <td>3.73848</td>\n",
       "      <td>-6.6824</td>\n",
       "      <td>-1.1960</td>\n",
       "      <td>[[0.72411350301662, -0.84318903552034, 1.47813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>dimer_006130</td>\n",
       "      <td>-20747.583954</td>\n",
       "      <td>-20757.490597</td>\n",
       "      <td>1.733202</td>\n",
       "      <td>-20745.850752</td>\n",
       "      <td>7.75520</td>\n",
       "      <td>-9.8085</td>\n",
       "      <td>11.3969</td>\n",
       "      <td>-21011.664617</td>\n",
       "      <td>-21019.968290</td>\n",
       "      <td>1.833384</td>\n",
       "      <td>-21009.831233</td>\n",
       "      <td>6.57082</td>\n",
       "      <td>-6.5023</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>[[-0.1568812460469, 1.12137289084067, -0.50992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>dimer_006131</td>\n",
       "      <td>-22685.503101</td>\n",
       "      <td>-22693.815819</td>\n",
       "      <td>1.715946</td>\n",
       "      <td>-22683.787155</td>\n",
       "      <td>7.83449</td>\n",
       "      <td>-9.9891</td>\n",
       "      <td>11.2032</td>\n",
       "      <td>-22967.819716</td>\n",
       "      <td>-22974.854229</td>\n",
       "      <td>1.795052</td>\n",
       "      <td>-22966.024664</td>\n",
       "      <td>6.23344</td>\n",
       "      <td>-6.7157</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>[[-0.06237487272243, 0.8137375865662, -0.11994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>dimer_006132</td>\n",
       "      <td>-18809.304420</td>\n",
       "      <td>-18820.714966</td>\n",
       "      <td>1.794782</td>\n",
       "      <td>-18807.509638</td>\n",
       "      <td>3.19981</td>\n",
       "      <td>-10.3796</td>\n",
       "      <td>12.8019</td>\n",
       "      <td>-19055.389959</td>\n",
       "      <td>-19064.917052</td>\n",
       "      <td>1.887702</td>\n",
       "      <td>-19053.502257</td>\n",
       "      <td>2.70457</td>\n",
       "      <td>-6.9760</td>\n",
       "      <td>0.4067</td>\n",
       "      <td>[[-0.20955229645577, 1.17165843085321, 0.38015...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  hf_gibbs_free_energy_ev  hf_electronic_energy_ev  \\\n",
       "0       002-M-BA1-b            -27275.076351            -27281.888683   \n",
       "1    002-M-BA1-pipi            -27275.128802            -27281.952769   \n",
       "2        005-MEL-CA            -25599.449771            -25604.305425   \n",
       "3        006-MEL-UA            -29133.284441            -29138.979188   \n",
       "4        007-MEL-CR            -22635.215921            -22641.289493   \n",
       "..              ...                      ...                      ...   \n",
       "153    dimer_006121            -19613.756532            -19622.869155   \n",
       "154    dimer_006125            -20685.601738            -20693.928747   \n",
       "155    dimer_006130            -20747.583954            -20757.490597   \n",
       "156    dimer_006131            -22685.503101            -22693.815819   \n",
       "157    dimer_006132            -18809.304420            -18820.714966   \n",
       "\n",
       "     hf_entropy_ev  hf_enthalpy_ev  hf_dipole_moment  hf_homo_ev  hf_lumo_ev  \\\n",
       "0         1.795316   -27273.281035           0.87236     -8.8130      4.0489   \n",
       "1         1.772166   -27273.356635           1.19181     -9.2580      4.4306   \n",
       "2         1.564143   -25597.885628           0.93514     -8.9909      4.6443   \n",
       "3         1.668533   -29131.615908           2.68196     -8.2371      3.9070   \n",
       "4         1.651416   -22633.564505           6.85797     -8.6600      5.1451   \n",
       "..             ...             ...               ...         ...         ...   \n",
       "153       1.793542   -19611.962991           6.65332     -9.2198      6.4409   \n",
       "154       1.763538   -20683.838200           6.26955     -9.4013      4.9169   \n",
       "155       1.733202   -20745.850752           7.75520     -9.8085     11.3969   \n",
       "156       1.715946   -22683.787155           7.83449     -9.9891     11.2032   \n",
       "157       1.794782   -18807.509638           3.19981    -10.3796     12.8019   \n",
       "\n",
       "     dft_gibbs_free_energy_ev  dft_electronic_energy_ev  dft_entropy_ev  \\\n",
       "0               -27617.108248             -27623.000160        1.877779   \n",
       "1               -27617.107555             -27623.063060        1.822257   \n",
       "2               -25917.872472             -25922.091294        1.641525   \n",
       "3               -29496.320185             -29501.224258        1.791889   \n",
       "4               -22922.502813             -22927.782911        1.687934   \n",
       "..                        ...                       ...             ...   \n",
       "153             -19866.663508             -19874.318183        1.856166   \n",
       "154             -20947.601629             -20954.521584        1.884566   \n",
       "155             -21011.664617             -21019.968290        1.833384   \n",
       "156             -22967.819716             -22974.854229        1.795052   \n",
       "157             -19055.389959             -19064.917052        1.887702   \n",
       "\n",
       "     dft_enthalpy_ev  dft_dipole_moment  dft_homo_ev  dft_lumo_ev  \\\n",
       "0      -27615.230469            1.47466      -6.2925      -1.4983   \n",
       "1      -27615.285298            1.69901      -6.7665      -1.3131   \n",
       "2      -25916.230947            0.85984      -6.4902      -0.8343   \n",
       "3      -29494.528296            3.15832      -6.1902      -1.1975   \n",
       "4      -22920.814880            6.66567      -6.3708      -0.2151   \n",
       "..               ...                ...          ...          ...   \n",
       "153    -19864.807342            7.42245      -6.4218       0.1743   \n",
       "154    -20945.717064            3.73848      -6.6824      -1.1960   \n",
       "155    -21009.831233            6.57082      -6.5023       0.1869   \n",
       "156    -22966.024664            6.23344      -6.7157       0.1428   \n",
       "157    -19053.502257            2.70457      -6.9760       0.4067   \n",
       "\n",
       "                                           coordinates  \n",
       "0    [[-3.6610385002265, 6.93533048376608, -0.58345...  \n",
       "1    [[-3.24946263013423, 6.99473259895769, -1.4223...  \n",
       "2    [[-5.415595, -7.691943, 0.824725], [-4.323509,...  \n",
       "3    [[-5.251705, -7.658659, 1.031666], [-4.20755, ...  \n",
       "4    [[-5.271343, -7.699293, 0.886577], [-4.227247,...  \n",
       "..                                                 ...  \n",
       "153  [[1.30646491596164, -0.57524990278906, -1.3873...  \n",
       "154  [[0.72411350301662, -0.84318903552034, 1.47813...  \n",
       "155  [[-0.1568812460469, 1.12137289084067, -0.50992...  \n",
       "156  [[-0.06237487272243, 0.8137375865662, -0.11994...  \n",
       "157  [[-0.20955229645577, 1.17165843085321, 0.38015...  \n",
       "\n",
       "[158 rows x 16 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для обучения использовал все дескрипторы, посчитанные на HF-3c уровне + столбец 'coordinates'  \n",
    "# Первые 56 строк в документе - реальный датасет\n",
    "\n",
    "features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "            'hf_enthalpy_ev', 'hf_dipole_moment', 'hf_homo_ev', 'hf_lumo_ev', 'coordinates']\n",
    "target = 'dft_homo_ev'\n",
    "\n",
    "train_data = data.iloc[18:]  \n",
    "test_data = data.iloc[:18]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скопировал сеты, т.к. предупреждение 'SettingWithCopyWarning' постоянно выскакивало при сглаживанни\n",
    "\n",
    "X_train = train_data[features].copy()\n",
    "y_train = train_data[target].copy()\n",
    "X_test = test_data[features].copy()\n",
    "y_test = test_data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сглаживание numpy-массива с координатами\n",
    "\n",
    "X_train['coordinates'] = X_train['coordinates'].apply(np.ravel)\n",
    "X_test['coordinates'] = X_test['coordinates'].apply(np.ravel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определил максимальную длину сглаженных массивов для каждого сета\n",
    "# Эта информация пригодится для дополнения сглаженных массивов, чтобы они были одинаковой длины \n",
    "# Это нужно для совместимости массива с моделями из сайкит-лерн\n",
    "\n",
    "max_length_train = X_train['coordinates'].apply(len).max()\n",
    "max_length_test = X_test['coordinates'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['coordinates'] = X_train['coordinates'].apply(lambda x: np.pad(x, (0, max_length_train - len(x)), mode='constant'))\n",
    "X_test['coordinates'] = X_test['coordinates'].apply(lambda x: np.pad(x, (0, max_length_test - len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конкатенирую полученные сглаженные массивы с остальными столбцами \n",
    "\n",
    "X_train_numeric = pd.concat([X_train.drop(columns='coordinates'),\n",
    "                             pd.DataFrame(np.vstack(X_train['coordinates']), \n",
    "                                          columns=[f'coord_{i}' for i in range(max_length_train)], \n",
    "                                          index=X_train.index)],\n",
    "                            axis=1)\n",
    "\n",
    "X_test_numeric = pd.concat([X_test.drop(columns='coordinates'),\n",
    "                            pd.DataFrame(np.vstack(X_test['coordinates']), \n",
    "                                         columns=[f'coord_{i}' for i in range(max_length_test)], \n",
    "                                         index=X_test.index)],\n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy-массив у тестовой выборки (реальный датасет) короче, чем у тренировочной\n",
    "# Определил разницу в длинах массивов, а затем привел к общей длине\n",
    " \n",
    "max_columns = max(X_train_numeric.shape[1], X_test_numeric.shape[1])\n",
    "X_train_numeric = X_train_numeric.reindex(columns=X_train_numeric.columns.union(X_test_numeric.columns), fill_value=0)\n",
    "X_test_numeric = X_test_numeric.reindex(columns=X_train_numeric.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация тренировочного датасета\n",
    "# Процедуру применил ко всем дескрипторам, кроме координат\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "coord_columns = [col for col in X_train_numeric.columns if col.startswith('coord_')]\n",
    "non_coord_columns = [col for col in X_train_numeric.columns if col not in coord_columns]\n",
    "\n",
    "X_train_non_coord_scaled = scaler.fit_transform(X_train_numeric[non_coord_columns])\n",
    "X_train_numeric_scaled = pd.concat([pd.DataFrame(X_train_non_coord_scaled, columns=non_coord_columns, index=X_train_numeric.index),\n",
    "                                    X_train_numeric[coord_columns]], axis=1)\n",
    "\n",
    "X_test_non_coord_scaled = scaler.transform(X_test_numeric[non_coord_columns])\n",
    "X_test_numeric_scaled = pd.concat([pd.DataFrame(X_test_non_coord_scaled, columns=non_coord_columns, index=X_test_numeric.index),\n",
    "                                   X_test_numeric[coord_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация таргета\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаю разные значения оптимайзеров, скоростей, слоев, нейронов и функций активации для поиска наилучших комбинаций \n",
    "\n",
    "optimizers = ['adam', 'sgd', 'rmsprop', 'adagrad']\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh', \"softmax\"]\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "layer_configs = [(1, 16), (2, 32), (3, 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.4714\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "MAE: 0.6790\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.6455\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.6177\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.6645\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.7036\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.6508\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.7097\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.5941\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.6825\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.6650\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.6858\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "MAE: 0.7975\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.7879\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.7926\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.7964\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.8255\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "MAE: 0.6341\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "MAE: 0.6562\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.6729\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "MAE: 0.7824\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.7972\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "MAE: 0.8173\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "MAE: 0.8017\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.7988\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.7955\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.6757\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "MAE: 0.8360\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "MAE: 0.7825\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "MAE: 0.7833\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.7803\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "MAE: 0.7826\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.5412\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "MAE: 0.6272\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "MAE: 0.8440\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.5656\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.7106\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step\n",
      "MAE: 0.6521\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.6873\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.6852\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.5696\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.7095\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.6331\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "MAE: 0.5742\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7910\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.7885\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7934\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7923\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6889\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.8959\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6698\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6716\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.7993\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7970\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.8117\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "MAE: 0.7989\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.7531\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.9207\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "MAE: 0.7465\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "MAE: 0.7251\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.8219\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.8277\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.8229\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.8185\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.4754\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "MAE: 0.4969\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.5265\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "MAE: 0.5370\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.5442\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "MAE: 0.5582\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.4904\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.5214\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.4641\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "MAE: 0.5115\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "MAE: 0.5642\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.5694\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.7293\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.7429\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.7270\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.7342\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "MAE: 0.6162\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.6003\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.6579\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.5005\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.6661\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.6651\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.6686\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.6645\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.5020\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.6218\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.6131\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7360\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.6777\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.6777\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "MAE: 0.6786\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.6730\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.4727\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.3794\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.4746\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "MAE: 0.5345\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.5595\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.5171\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.5588\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.5281\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7188\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.5439\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.6448\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "MAE: 0.4843\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.7229\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.7290\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.7173\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.7204\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.8018\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8995\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.7635\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.5720\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "MAE: 0.7721\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.6662\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6721\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.6620\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.7194\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.6218\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.6458\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.7400\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.8073\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.8241\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.8214\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.7946\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.7851\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "MAE: 0.6730\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.7582\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.6527\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.7860\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAE: 0.7730\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.7816\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.7655\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "MAE: 0.6253\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.6713\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.6923\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.8123\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.9652\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.9567\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.9360\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.9498\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.9178\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.9639\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.7906\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "MAE: 0.8945\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "MAE: 0.8573\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "MAE: 0.8646\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8672\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.8599\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.6507\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 1.0202\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.9236\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step\n",
      "MAE: 0.7682\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8910\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "MAE: 0.8889\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "MAE: 0.8887\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.8882\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.7613\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.7316\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.8314\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.7506\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "MAE: 0.8258\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "MAE: 0.7432\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "MAE: 0.7500\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "MAE: 0.7883\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.5802\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step\n",
      "MAE: 0.7316\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.8506\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.7274\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.9553\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.9365\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.9362\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.9630\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.7672\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 1.0489\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 1.0524\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.9144\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "MAE: 0.8566\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8807\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.8617\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.8792\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "MAE: 0.8177\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.8178\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.8275\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.9143\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 1.0312\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 1.0493\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "MAE: 1.0390\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 1.0353\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.7254\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.6114\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "MAE: 0.5879\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.4985\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.5695\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "MAE: 0.6407\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.6649\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6276\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.4950\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.5525\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.5153\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.5427\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "MAE: 0.7705\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.7642\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "MAE: 0.7712\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "MAE: 0.7589\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.5418\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.6392\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "MAE: 0.6325\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.6680\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.7476\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "MAE: 0.7540\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "MAE: 0.7678\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.7479\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.6201\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "MAE: 0.7073\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.5909\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.5376\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.7386\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.7397\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.7388\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.7397\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.6609\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "MAE: 0.5511\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "MAE: 0.5830\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.5872\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "MAE: 0.5825\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.6231\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "MAE: 0.5815\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.6905\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.5632\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.6112\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.4603\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step\n",
      "MAE: 0.5464\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "MAE: 0.7619\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "MAE: 0.7602\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.7620\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.7673\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.8038\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6524\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.8251\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "MAE: 0.6662\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "MAE: 0.7447\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.7588\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.8172\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.7398\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "MAE: 0.6968\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.7351\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.7355\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8055\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.8014\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.8158\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.8077\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "MAE: 0.8063\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.8781\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.5536\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.6391\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "MAE: 0.5703\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.5462\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.5851\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.5599\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.5904\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.5591\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "MAE: 0.6662\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.6313\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.6309\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.6131\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.5889\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "MAE: 0.5973\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adam, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.5703\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.7559\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.5916\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "MAE: 0.6859\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.6880\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "MAE: 0.5538\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.5527\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "MAE: 0.5520\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "MAE: 0.5470\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.6576\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.6526\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.4973\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.7247\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "MAE: 0.5490\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "MAE: 0.5490\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "MAE: 0.5490\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: sgd, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "MAE: 0.5490\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "MAE: 0.4993\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "MAE: 0.5680\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "MAE: 0.7395\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.6597\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.5726\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.6048\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.5514\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.6329\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MAE: 0.7204\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "MAE: 0.5025\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.7429\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.6499\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.5939\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "MAE: 0.5987\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MAE: 0.5973\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: rmsprop, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "MAE: 0.5735\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.5087\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.7249\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "MAE: 0.9195\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: relu, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.9797\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "MAE: 0.5454\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "MAE: 0.6316\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "MAE: 0.5451\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: sigmoid, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "MAE: 0.5496\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "MAE: 0.5873\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "MAE: 0.6672\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "MAE: 0.6259\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: tanh, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "MAE: 0.6485\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.6693\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MAE: 0.6729\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "MAE: 0.6716\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 1, Num Neurons: 16\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 2, Num Neurons: 32\n",
      "Optimizer: adagrad, Activation Function: softmax, Learning Rate: 0.1, Num Layers: 3, Num Neurons: 64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "MAE: 0.6786\n"
     ]
    }
   ],
   "source": [
    "# Цикл перебирает различные комбинации оптимизаторов и функций активации, обучает модель для каждой комбинации, и оценивает ее точность с помощью MAE\n",
    "# Для каждой из 192 возможных комбинаций проводится 5-кратная кросс-валидация\n",
    "# Лист \"results\" содержит результаты кросс-валидации\n",
    "\n",
    "results = []\n",
    "\n",
    "############### 5-кратная кросс-валидация ############### \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_numeric_scaled):\n",
    "    X_train_cv, X_test_cv = X_train_numeric_scaled.iloc[train_index], X_train_numeric_scaled.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_train_scaled[train_index], y_train_scaled[test_index]\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        for activation_function in activation_functions:\n",
    "            for learning_rate in learning_rates:\n",
    "                for num_layers, num_neurons in layer_configs:\n",
    "                    print(f\"Optimizer: {optimizer}, Activation Function: {activation_function}, Learning Rate: {learning_rate}, Num Layers: {num_layers}, Num Neurons: {num_neurons}\")\n",
    "                \n",
    "                    ############### Компилирую и обучаю MLP модель ############### \n",
    "                    model = tf.keras.Sequential()\n",
    "                    for _ in range(num_layers):\n",
    "                        model.add(tf.keras.layers.Dense(num_neurons, activation=activation_function))\n",
    "                    model.add(tf.keras.layers.Dense(1))  \n",
    "\n",
    "                    model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n",
    "                    history = model.fit(X_train_cv, y_train_cv, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "                ############### Оцениваю модель ############### \n",
    "                y_pred_scaled = model.predict(X_test_cv)\n",
    "                \n",
    "                y_pred_inverse = scaler_y.inverse_transform(y_pred_scaled)\n",
    "                y_test_inverse = scaler_y.inverse_transform(y_test_cv)\n",
    "\n",
    "                mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "                print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "                results.append({\n",
    "                    'Optimizer': optimizer,\n",
    "                    'Activation Function': activation_function,\n",
    "                    'Learning Rate' : learning_rate,\n",
    "                    'Num Layers' : num_layers,\n",
    "                    'Num Neurons' : num_neurons, \n",
    "                    'MAE': mae\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                             Top 3 combinations for 'HOMO' descriptor:                              \u001b[0m\n",
      "\n",
      "1. Optimizer: rmsprop, Activation Function: relu, Learning Rate: 0.001, Num Layers: 3, Num Neurons: 64, MAE: 0.3794\n",
      "2. Optimizer: rmsprop, Activation Function: tanh, Learning Rate: 0.01, Num Layers: 3, Num Neurons: 64, MAE: 0.4603\n",
      "3. Optimizer: adam, Activation Function: tanh, Learning Rate: 0.0001, Num Layers: 3, Num Neurons: 64, MAE: 0.4641\n"
     ]
    }
   ],
   "source": [
    "# ТОП-3 комбинаций из 192 возможных с лучшими MAE после 5-кратной кросс-валидации\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x['MAE'])\n",
    "print(\"\\033[1m\" + \"Top 3 combinations for 'HOMO' descriptor:\".center(100) + \"\\033[0m\")\n",
    "print()\n",
    "for i, result in enumerate(results_sorted[:3], 1):\n",
    "    print(f\"{i}. Optimizer: {result['Optimizer']}, Activation Function: {result['Activation Function']}, Learning Rate: {result['Learning Rate']}, Num Layers: {result['Num Layers']}, Num Neurons: {result['Num Neurons']}, MAE: {result['MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Повторно обучаю модель, но теперь на полном тренировочном датасете\n",
    "# Беру оптимальные значения гиперпараметров, найденные с помощью кросс-валидации\n",
    "\n",
    "best_combination = results_sorted[0]\n",
    "best_optimizer = best_combination['Optimizer']\n",
    "best_activation_function = best_combination['Activation Function']\n",
    "best_learning_rate = best_combination['Learning Rate']\n",
    "best_num_layers = best_combination['Num Layers']\n",
    "best_num_neurons = best_combination['Num Neurons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 1.0985 - mae: 1.1185 - val_loss: 0.5371 - val_mae: 0.5371\n",
      "Epoch 2/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7918 - mae: 0.7879 - val_loss: 0.7699 - val_mae: 0.7699\n",
      "Epoch 3/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5304 - mae: 0.5258 - val_loss: 0.7127 - val_mae: 0.7127\n",
      "Epoch 4/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4824 - mae: 0.4794 - val_loss: 0.5355 - val_mae: 0.5355\n",
      "Epoch 5/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4549 - mae: 0.4543 - val_loss: 0.6837 - val_mae: 0.6837\n",
      "Epoch 6/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4349 - mae: 0.4368 - val_loss: 0.8596 - val_mae: 0.8596\n",
      "Epoch 7/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4356 - mae: 0.4417 - val_loss: 0.4919 - val_mae: 0.4919\n",
      "Epoch 8/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3330 - mae: 0.3268 - val_loss: 0.6165 - val_mae: 0.6165\n",
      "Epoch 9/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3374 - mae: 0.3316 - val_loss: 0.5855 - val_mae: 0.5855\n",
      "Epoch 10/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2810 - mae: 0.2761 - val_loss: 0.4686 - val_mae: 0.4686\n",
      "Epoch 11/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2650 - mae: 0.2583 - val_loss: 0.4606 - val_mae: 0.4606\n",
      "Epoch 12/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3220 - mae: 0.3233 - val_loss: 0.4669 - val_mae: 0.4669\n",
      "Epoch 13/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3010 - mae: 0.3040 - val_loss: 0.5156 - val_mae: 0.5156\n",
      "Epoch 14/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2285 - mae: 0.2256 - val_loss: 0.4910 - val_mae: 0.4910\n",
      "Epoch 15/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3250 - mae: 0.3253 - val_loss: 0.4404 - val_mae: 0.4404\n",
      "Epoch 16/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2780 - mae: 0.2850 - val_loss: 0.4794 - val_mae: 0.4794\n",
      "Epoch 17/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2282 - mae: 0.2231 - val_loss: 0.5346 - val_mae: 0.5346\n",
      "Epoch 18/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2502 - mae: 0.2484 - val_loss: 0.6165 - val_mae: 0.6165\n",
      "Epoch 19/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2528 - mae: 0.2507 - val_loss: 0.5553 - val_mae: 0.5553\n",
      "Epoch 20/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2338 - mae: 0.2324 - val_loss: 0.4489 - val_mae: 0.4489\n",
      "Epoch 21/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2132 - mae: 0.2113 - val_loss: 0.6000 - val_mae: 0.6000\n",
      "Epoch 22/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2048 - mae: 0.2037 - val_loss: 0.5397 - val_mae: 0.5397\n",
      "Epoch 23/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1970 - mae: 0.1982 - val_loss: 0.4636 - val_mae: 0.4636\n",
      "Epoch 24/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1903 - mae: 0.1911 - val_loss: 0.4696 - val_mae: 0.4696\n",
      "Epoch 25/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1695 - mae: 0.1696 - val_loss: 0.4453 - val_mae: 0.4453\n",
      "Epoch 26/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2138 - mae: 0.2144 - val_loss: 0.4383 - val_mae: 0.4383\n",
      "Epoch 27/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1830 - mae: 0.1828 - val_loss: 0.4424 - val_mae: 0.4424\n",
      "Epoch 28/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1854 - mae: 0.1876 - val_loss: 0.4917 - val_mae: 0.4917\n",
      "Epoch 29/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1761 - mae: 0.1759 - val_loss: 0.5266 - val_mae: 0.5266\n",
      "Epoch 30/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1605 - mae: 0.1591 - val_loss: 0.4775 - val_mae: 0.4775\n",
      "Epoch 31/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1915 - mae: 0.1914 - val_loss: 0.4187 - val_mae: 0.4187\n",
      "Epoch 32/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2122 - mae: 0.2131 - val_loss: 0.4446 - val_mae: 0.4446\n",
      "Epoch 33/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1542 - mae: 0.1552 - val_loss: 0.4659 - val_mae: 0.4659\n",
      "Epoch 34/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1884 - mae: 0.1870 - val_loss: 0.3991 - val_mae: 0.3991\n",
      "Epoch 35/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2019 - mae: 0.2028 - val_loss: 0.5346 - val_mae: 0.5346\n",
      "Epoch 36/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1602 - mae: 0.1614 - val_loss: 0.4826 - val_mae: 0.4826\n",
      "Epoch 37/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1528 - mae: 0.1550 - val_loss: 0.4671 - val_mae: 0.4671\n",
      "Epoch 38/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2013 - mae: 0.1996 - val_loss: 0.4761 - val_mae: 0.4761\n",
      "Epoch 39/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1615 - mae: 0.1592 - val_loss: 0.4389 - val_mae: 0.4389\n",
      "Epoch 40/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1377 - mae: 0.1359 - val_loss: 0.4560 - val_mae: 0.4560\n",
      "Epoch 41/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1707 - mae: 0.1702 - val_loss: 0.4873 - val_mae: 0.4873\n",
      "Epoch 42/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1593 - mae: 0.1567 - val_loss: 0.4773 - val_mae: 0.4773\n",
      "Epoch 43/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1487 - mae: 0.1497 - val_loss: 0.4346 - val_mae: 0.4346\n",
      "Epoch 44/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1575 - mae: 0.1560 - val_loss: 0.5146 - val_mae: 0.5146\n",
      "Epoch 45/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1546 - mae: 0.1547 - val_loss: 0.4585 - val_mae: 0.4585\n",
      "Epoch 46/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1255 - mae: 0.1256 - val_loss: 0.4781 - val_mae: 0.4781\n",
      "Epoch 47/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1656 - mae: 0.1695 - val_loss: 0.4440 - val_mae: 0.4440\n",
      "Epoch 48/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1352 - mae: 0.1364 - val_loss: 0.4417 - val_mae: 0.4417\n",
      "Epoch 49/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1408 - mae: 0.1416 - val_loss: 0.4831 - val_mae: 0.4831\n",
      "Epoch 50/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1988 - mae: 0.1988 - val_loss: 0.4668 - val_mae: 0.4668\n",
      "Epoch 51/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1208 - mae: 0.1185 - val_loss: 0.4299 - val_mae: 0.4299\n",
      "Epoch 52/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1337 - mae: 0.1341 - val_loss: 0.5532 - val_mae: 0.5532\n",
      "Epoch 53/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1860 - mae: 0.1879 - val_loss: 0.4715 - val_mae: 0.4715\n",
      "Epoch 54/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1009 - mae: 0.0995 - val_loss: 0.4235 - val_mae: 0.4235\n",
      "Epoch 55/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1308 - mae: 0.1340 - val_loss: 0.4042 - val_mae: 0.4042\n",
      "Epoch 56/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1251 - mae: 0.1277 - val_loss: 0.4657 - val_mae: 0.4657\n",
      "Epoch 57/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1269 - mae: 0.1295 - val_loss: 0.4314 - val_mae: 0.4314\n",
      "Epoch 58/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1691 - mae: 0.1706 - val_loss: 0.4284 - val_mae: 0.4284\n",
      "Epoch 59/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1229 - mae: 0.1234 - val_loss: 0.4162 - val_mae: 0.4162\n",
      "Epoch 60/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1373 - mae: 0.1358 - val_loss: 0.4420 - val_mae: 0.4420\n",
      "Epoch 61/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1439 - mae: 0.1459 - val_loss: 0.4444 - val_mae: 0.4444\n",
      "Epoch 62/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1411 - mae: 0.1432 - val_loss: 0.4201 - val_mae: 0.4201\n",
      "Epoch 63/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1531 - mae: 0.1523 - val_loss: 0.4312 - val_mae: 0.4312\n",
      "Epoch 64/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1247 - mae: 0.1224 - val_loss: 0.4329 - val_mae: 0.4329\n",
      "Epoch 65/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1210 - mae: 0.1223 - val_loss: 0.5285 - val_mae: 0.5285\n",
      "Epoch 66/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1607 - mae: 0.1608 - val_loss: 0.4173 - val_mae: 0.4173\n",
      "Epoch 67/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1316 - mae: 0.1292 - val_loss: 0.4288 - val_mae: 0.4288\n",
      "Epoch 68/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1074 - mae: 0.1068 - val_loss: 0.4187 - val_mae: 0.4187\n",
      "Epoch 69/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1564 - mae: 0.1574 - val_loss: 0.4233 - val_mae: 0.4233\n",
      "Epoch 70/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1259 - mae: 0.1245 - val_loss: 0.4078 - val_mae: 0.4078\n",
      "Epoch 71/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1085 - mae: 0.1047 - val_loss: 0.4041 - val_mae: 0.4041\n",
      "Epoch 72/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1173 - mae: 0.1181 - val_loss: 0.4170 - val_mae: 0.4170\n",
      "Epoch 73/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1366 - mae: 0.1364 - val_loss: 0.4595 - val_mae: 0.4595\n",
      "Epoch 74/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1266 - mae: 0.1289 - val_loss: 0.4183 - val_mae: 0.4183\n",
      "Epoch 75/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1089 - mae: 0.1087 - val_loss: 0.4166 - val_mae: 0.4166\n",
      "Epoch 76/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1212 - mae: 0.1198 - val_loss: 0.4690 - val_mae: 0.4690\n",
      "Epoch 77/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1525 - mae: 0.1532 - val_loss: 0.4068 - val_mae: 0.4068\n",
      "Epoch 78/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1147 - mae: 0.1141 - val_loss: 0.4282 - val_mae: 0.4282\n",
      "Epoch 79/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0871 - mae: 0.0870 - val_loss: 0.4668 - val_mae: 0.4668\n",
      "Epoch 80/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1792 - mae: 0.1824 - val_loss: 0.4302 - val_mae: 0.4302\n",
      "Epoch 81/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1251 - mae: 0.1260 - val_loss: 0.4214 - val_mae: 0.4214\n",
      "Epoch 82/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1029 - mae: 0.1021 - val_loss: 0.4493 - val_mae: 0.4493\n",
      "Epoch 83/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1129 - mae: 0.1139 - val_loss: 0.4292 - val_mae: 0.4292\n",
      "Epoch 84/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1030 - mae: 0.1034 - val_loss: 0.4679 - val_mae: 0.4679\n",
      "Epoch 85/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1201 - mae: 0.1194 - val_loss: 0.4063 - val_mae: 0.4063\n",
      "Epoch 86/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1042 - mae: 0.1046 - val_loss: 0.4380 - val_mae: 0.4380\n",
      "Epoch 87/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1020 - mae: 0.1011 - val_loss: 0.4303 - val_mae: 0.4303\n",
      "Epoch 88/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1128 - mae: 0.1109 - val_loss: 0.4155 - val_mae: 0.4155\n",
      "Epoch 89/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0987 - mae: 0.0986 - val_loss: 0.4462 - val_mae: 0.4462\n",
      "Epoch 90/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1274 - mae: 0.1278 - val_loss: 0.4765 - val_mae: 0.4765\n",
      "Epoch 91/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1193 - mae: 0.1191 - val_loss: 0.4143 - val_mae: 0.4143\n",
      "Epoch 92/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0864 - mae: 0.0852 - val_loss: 0.4888 - val_mae: 0.4888\n",
      "Epoch 93/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1456 - mae: 0.1492 - val_loss: 0.4927 - val_mae: 0.4927\n",
      "Epoch 94/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1082 - mae: 0.1086 - val_loss: 0.4555 - val_mae: 0.4555\n",
      "Epoch 95/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1029 - mae: 0.1034 - val_loss: 0.4284 - val_mae: 0.4284\n",
      "Epoch 96/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1010 - mae: 0.0997 - val_loss: 0.4348 - val_mae: 0.4348\n",
      "Epoch 97/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1268 - mae: 0.1279 - val_loss: 0.4372 - val_mae: 0.4372\n",
      "Epoch 98/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0898 - mae: 0.0892 - val_loss: 0.4787 - val_mae: 0.4787\n",
      "Epoch 99/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1104 - mae: 0.1112 - val_loss: 0.4504 - val_mae: 0.4504\n",
      "Epoch 100/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1023 - mae: 0.1036 - val_loss: 0.4684 - val_mae: 0.4684\n",
      "Epoch 101/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1061 - mae: 0.1046 - val_loss: 0.4262 - val_mae: 0.4262\n",
      "Epoch 102/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1107 - mae: 0.1101 - val_loss: 0.4328 - val_mae: 0.4328\n",
      "Epoch 103/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1079 - mae: 0.1082 - val_loss: 0.4142 - val_mae: 0.4142\n",
      "Epoch 104/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1067 - mae: 0.1058 - val_loss: 0.4418 - val_mae: 0.4418\n",
      "Epoch 105/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0890 - mae: 0.0883 - val_loss: 0.4300 - val_mae: 0.4300\n",
      "Epoch 106/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1075 - mae: 0.1070 - val_loss: 0.4504 - val_mae: 0.4504\n",
      "Epoch 107/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1014 - mae: 0.1026 - val_loss: 0.4237 - val_mae: 0.4237\n",
      "Epoch 108/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1209 - mae: 0.1226 - val_loss: 0.4317 - val_mae: 0.4317\n",
      "Epoch 109/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0952 - mae: 0.0958 - val_loss: 0.4544 - val_mae: 0.4544\n",
      "Epoch 110/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1170 - mae: 0.1203 - val_loss: 0.4400 - val_mae: 0.4400\n",
      "Epoch 111/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1312 - mae: 0.1326 - val_loss: 0.4175 - val_mae: 0.4175\n",
      "Epoch 112/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0957 - mae: 0.0951 - val_loss: 0.4086 - val_mae: 0.4086\n",
      "Epoch 113/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0827 - mae: 0.0819 - val_loss: 0.4399 - val_mae: 0.4399\n",
      "Epoch 114/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1211 - mae: 0.1200 - val_loss: 0.4018 - val_mae: 0.4018\n",
      "Epoch 115/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1081 - mae: 0.1088 - val_loss: 0.4214 - val_mae: 0.4214\n",
      "Epoch 116/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0857 - mae: 0.0852 - val_loss: 0.4068 - val_mae: 0.4068\n",
      "Epoch 117/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0895 - mae: 0.0891 - val_loss: 0.4273 - val_mae: 0.4273\n",
      "Epoch 118/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1030 - mae: 0.1022 - val_loss: 0.3948 - val_mae: 0.3948\n",
      "Epoch 119/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1115 - mae: 0.1100 - val_loss: 0.3997 - val_mae: 0.3997\n",
      "Epoch 120/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1145 - mae: 0.1140 - val_loss: 0.4289 - val_mae: 0.4289\n",
      "Epoch 121/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0789 - mae: 0.0784 - val_loss: 0.4466 - val_mae: 0.4466\n",
      "Epoch 122/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0890 - mae: 0.0907 - val_loss: 0.4371 - val_mae: 0.4371\n",
      "Epoch 123/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1045 - mae: 0.1046 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 124/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0991 - mae: 0.0987 - val_loss: 0.4318 - val_mae: 0.4318\n",
      "Epoch 125/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0867 - mae: 0.0876 - val_loss: 0.4174 - val_mae: 0.4174\n",
      "Epoch 126/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0884 - mae: 0.0857 - val_loss: 0.4293 - val_mae: 0.4293\n",
      "Epoch 127/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0973 - mae: 0.0975 - val_loss: 0.4263 - val_mae: 0.4263\n",
      "Epoch 128/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1210 - mae: 0.1238 - val_loss: 0.3975 - val_mae: 0.3975\n",
      "Epoch 129/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0685 - mae: 0.0688 - val_loss: 0.3907 - val_mae: 0.3907\n",
      "Epoch 130/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1142 - mae: 0.1142 - val_loss: 0.4101 - val_mae: 0.4101\n",
      "Epoch 131/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0942 - mae: 0.0944 - val_loss: 0.4363 - val_mae: 0.4363\n",
      "Epoch 132/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1062 - mae: 0.1047 - val_loss: 0.4510 - val_mae: 0.4510\n",
      "Epoch 133/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1018 - mae: 0.1028 - val_loss: 0.4246 - val_mae: 0.4246\n",
      "Epoch 134/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1060 - mae: 0.1072 - val_loss: 0.4183 - val_mae: 0.4183\n",
      "Epoch 135/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0793 - mae: 0.0794 - val_loss: 0.4998 - val_mae: 0.4998\n",
      "Epoch 136/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1265 - mae: 0.1271 - val_loss: 0.4352 - val_mae: 0.4352\n",
      "Epoch 137/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0908 - mae: 0.0930 - val_loss: 0.4237 - val_mae: 0.4237\n",
      "Epoch 138/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0704 - mae: 0.0693 - val_loss: 0.4196 - val_mae: 0.4196\n",
      "Epoch 139/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0985 - mae: 0.0977 - val_loss: 0.4033 - val_mae: 0.4033\n",
      "Epoch 140/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1042 - mae: 0.1050 - val_loss: 0.4122 - val_mae: 0.4122\n",
      "Epoch 141/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0822 - mae: 0.0823 - val_loss: 0.4300 - val_mae: 0.4300\n",
      "Epoch 142/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0759 - mae: 0.0769 - val_loss: 0.4303 - val_mae: 0.4303\n",
      "Epoch 143/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1062 - mae: 0.1054 - val_loss: 0.4172 - val_mae: 0.4172\n",
      "Epoch 144/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1126 - mae: 0.1130 - val_loss: 0.4300 - val_mae: 0.4300\n",
      "Epoch 145/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0704 - mae: 0.0712 - val_loss: 0.4187 - val_mae: 0.4187\n",
      "Epoch 146/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0872 - mae: 0.0888 - val_loss: 0.4302 - val_mae: 0.4302\n",
      "Epoch 147/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1112 - mae: 0.1128 - val_loss: 0.4066 - val_mae: 0.4066\n",
      "Epoch 148/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1004 - mae: 0.1010 - val_loss: 0.4153 - val_mae: 0.4153\n",
      "Epoch 149/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0770 - mae: 0.0767 - val_loss: 0.4450 - val_mae: 0.4450\n",
      "Epoch 150/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1067 - mae: 0.1087 - val_loss: 0.4315 - val_mae: 0.4315\n",
      "Epoch 151/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0866 - mae: 0.0870 - val_loss: 0.3877 - val_mae: 0.3877\n",
      "Epoch 152/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0943 - mae: 0.0931 - val_loss: 0.4316 - val_mae: 0.4316\n",
      "Epoch 153/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0805 - mae: 0.0793 - val_loss: 0.3847 - val_mae: 0.3847\n",
      "Epoch 154/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0864 - mae: 0.0868 - val_loss: 0.3871 - val_mae: 0.3871\n",
      "Epoch 155/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0877 - mae: 0.0859 - val_loss: 0.4156 - val_mae: 0.4156\n",
      "Epoch 156/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0988 - mae: 0.1007 - val_loss: 0.4436 - val_mae: 0.4436\n",
      "Epoch 157/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0797 - mae: 0.0788 - val_loss: 0.4122 - val_mae: 0.4122\n",
      "Epoch 158/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0908 - mae: 0.0918 - val_loss: 0.4162 - val_mae: 0.4162\n",
      "Epoch 159/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0695 - mae: 0.0695 - val_loss: 0.4497 - val_mae: 0.4497\n",
      "Epoch 160/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0919 - mae: 0.0927 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 161/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0659 - mae: 0.0668 - val_loss: 0.4161 - val_mae: 0.4161\n",
      "Epoch 162/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0954 - mae: 0.0966 - val_loss: 0.4156 - val_mae: 0.4156\n",
      "Epoch 163/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0921 - mae: 0.0939 - val_loss: 0.4386 - val_mae: 0.4386\n",
      "Epoch 164/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0811 - mae: 0.0792 - val_loss: 0.4216 - val_mae: 0.4216\n",
      "Epoch 165/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0946 - mae: 0.0944 - val_loss: 0.3991 - val_mae: 0.3991\n",
      "Epoch 166/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0888 - mae: 0.0889 - val_loss: 0.4231 - val_mae: 0.4231\n",
      "Epoch 167/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0921 - mae: 0.0935 - val_loss: 0.4491 - val_mae: 0.4491\n",
      "Epoch 168/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0660 - mae: 0.0659 - val_loss: 0.3921 - val_mae: 0.3921\n",
      "Epoch 169/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0990 - mae: 0.0999 - val_loss: 0.3993 - val_mae: 0.3993\n",
      "Epoch 170/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0879 - mae: 0.0890 - val_loss: 0.4117 - val_mae: 0.4117\n",
      "Epoch 171/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0721 - mae: 0.0727 - val_loss: 0.3983 - val_mae: 0.3983\n",
      "Epoch 172/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0827 - mae: 0.0828 - val_loss: 0.3986 - val_mae: 0.3986\n",
      "Epoch 173/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0814 - mae: 0.0824 - val_loss: 0.4185 - val_mae: 0.4185\n",
      "Epoch 174/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0643 - mae: 0.0653 - val_loss: 0.4263 - val_mae: 0.4263\n",
      "Epoch 175/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0842 - mae: 0.0852 - val_loss: 0.4395 - val_mae: 0.4395\n",
      "Epoch 176/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0859 - mae: 0.0859 - val_loss: 0.4050 - val_mae: 0.4050\n",
      "Epoch 177/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0803 - mae: 0.0809 - val_loss: 0.3967 - val_mae: 0.3967\n",
      "Epoch 178/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0781 - mae: 0.0776 - val_loss: 0.4219 - val_mae: 0.4219\n",
      "Epoch 179/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0959 - mae: 0.0970 - val_loss: 0.3965 - val_mae: 0.3965\n",
      "Epoch 180/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0738 - mae: 0.0728 - val_loss: 0.4434 - val_mae: 0.4434\n",
      "Epoch 181/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1177 - mae: 0.1199 - val_loss: 0.4066 - val_mae: 0.4066\n",
      "Epoch 182/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0668 - mae: 0.0674 - val_loss: 0.3975 - val_mae: 0.3975\n",
      "Epoch 183/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0917 - mae: 0.0927 - val_loss: 0.3931 - val_mae: 0.3931\n",
      "Epoch 184/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0752 - mae: 0.0756 - val_loss: 0.4103 - val_mae: 0.4103\n",
      "Epoch 185/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1018 - mae: 0.1031 - val_loss: 0.4090 - val_mae: 0.4090\n",
      "Epoch 186/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0761 - mae: 0.0769 - val_loss: 0.3930 - val_mae: 0.3930\n",
      "Epoch 187/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0751 - mae: 0.0761 - val_loss: 0.4100 - val_mae: 0.4100\n",
      "Epoch 188/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0738 - mae: 0.0729 - val_loss: 0.4035 - val_mae: 0.4035\n",
      "Epoch 189/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0838 - mae: 0.0828 - val_loss: 0.3981 - val_mae: 0.3981\n",
      "Epoch 190/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0728 - mae: 0.0731 - val_loss: 0.4149 - val_mae: 0.4149\n",
      "Epoch 191/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0703 - mae: 0.0709 - val_loss: 0.3923 - val_mae: 0.3923\n",
      "Epoch 192/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0844 - mae: 0.0858 - val_loss: 0.4547 - val_mae: 0.4547\n",
      "Epoch 193/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0912 - mae: 0.0924 - val_loss: 0.4135 - val_mae: 0.4135\n",
      "Epoch 194/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0830 - mae: 0.0852 - val_loss: 0.4201 - val_mae: 0.4201\n",
      "Epoch 195/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0818 - mae: 0.0820 - val_loss: 0.4128 - val_mae: 0.4128\n",
      "Epoch 196/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0741 - mae: 0.0694 - val_loss: 0.4151 - val_mae: 0.4151\n",
      "Epoch 197/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0903 - mae: 0.0925 - val_loss: 0.4077 - val_mae: 0.4077\n",
      "Epoch 198/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0747 - mae: 0.0746 - val_loss: 0.3969 - val_mae: 0.3969\n",
      "Epoch 199/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0749 - mae: 0.0743 - val_loss: 0.4170 - val_mae: 0.4170\n",
      "Epoch 200/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0806 - mae: 0.0824 - val_loss: 0.4409 - val_mae: 0.4409\n",
      "Epoch 201/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0827 - mae: 0.0835 - val_loss: 0.4121 - val_mae: 0.4121\n",
      "Epoch 202/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0661 - mae: 0.0663 - val_loss: 0.3886 - val_mae: 0.3886\n",
      "Epoch 203/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0612 - mae: 0.0610 - val_loss: 0.4165 - val_mae: 0.4165\n",
      "Epoch 204/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0675 - mae: 0.0687 - val_loss: 0.4174 - val_mae: 0.4174\n",
      "Epoch 205/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0940 - mae: 0.0955 - val_loss: 0.4514 - val_mae: 0.4514\n",
      "Epoch 206/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0956 - mae: 0.0955 - val_loss: 0.4070 - val_mae: 0.4070\n",
      "Epoch 207/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0830 - mae: 0.0840 - val_loss: 0.3928 - val_mae: 0.3928\n",
      "Epoch 208/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0745 - mae: 0.0743 - val_loss: 0.3895 - val_mae: 0.3895\n",
      "Epoch 209/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0763 - mae: 0.0772 - val_loss: 0.4484 - val_mae: 0.4484\n",
      "Epoch 210/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0849 - mae: 0.0823 - val_loss: 0.4178 - val_mae: 0.4178\n",
      "Epoch 211/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0867 - mae: 0.0858 - val_loss: 0.4268 - val_mae: 0.4268\n",
      "Epoch 212/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1004 - mae: 0.1016 - val_loss: 0.4099 - val_mae: 0.4099\n",
      "Epoch 213/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0689 - mae: 0.0682 - val_loss: 0.4032 - val_mae: 0.4032\n",
      "Epoch 214/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0729 - mae: 0.0745 - val_loss: 0.4111 - val_mae: 0.4111\n",
      "Epoch 215/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0708 - mae: 0.0710 - val_loss: 0.3833 - val_mae: 0.3833\n",
      "Epoch 216/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0665 - mae: 0.0652 - val_loss: 0.4341 - val_mae: 0.4341\n",
      "Epoch 217/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1176 - mae: 0.1187 - val_loss: 0.4163 - val_mae: 0.4163\n",
      "Epoch 218/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0596 - mae: 0.0597 - val_loss: 0.3856 - val_mae: 0.3856\n",
      "Epoch 219/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0672 - mae: 0.0665 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 220/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0634 - mae: 0.0625 - val_loss: 0.4184 - val_mae: 0.4184\n",
      "Epoch 221/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0833 - mae: 0.0821 - val_loss: 0.4126 - val_mae: 0.4126\n",
      "Epoch 222/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0804 - mae: 0.0793 - val_loss: 0.4150 - val_mae: 0.4150\n",
      "Epoch 223/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0835 - mae: 0.0830 - val_loss: 0.4214 - val_mae: 0.4214\n",
      "Epoch 224/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0635 - mae: 0.0633 - val_loss: 0.4111 - val_mae: 0.4111\n",
      "Epoch 225/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0742 - mae: 0.0752 - val_loss: 0.4165 - val_mae: 0.4165\n",
      "Epoch 226/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0619 - mae: 0.0620 - val_loss: 0.4142 - val_mae: 0.4142\n",
      "Epoch 227/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0698 - mae: 0.0712 - val_loss: 0.4096 - val_mae: 0.4096\n",
      "Epoch 228/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0619 - mae: 0.0606 - val_loss: 0.4426 - val_mae: 0.4426\n",
      "Epoch 229/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0970 - mae: 0.0982 - val_loss: 0.3851 - val_mae: 0.3851\n",
      "Epoch 230/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0625 - mae: 0.0635 - val_loss: 0.4001 - val_mae: 0.4001\n",
      "Epoch 231/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0800 - mae: 0.0797 - val_loss: 0.3958 - val_mae: 0.3958\n",
      "Epoch 232/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0793 - mae: 0.0805 - val_loss: 0.3968 - val_mae: 0.3968\n",
      "Epoch 233/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0828 - mae: 0.0827 - val_loss: 0.4245 - val_mae: 0.4245\n",
      "Epoch 234/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0822 - mae: 0.0822 - val_loss: 0.3926 - val_mae: 0.3926\n",
      "Epoch 235/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0775 - mae: 0.0753 - val_loss: 0.4051 - val_mae: 0.4051\n",
      "Epoch 236/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0849 - mae: 0.0866 - val_loss: 0.3824 - val_mae: 0.3824\n",
      "Epoch 237/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0727 - mae: 0.0721 - val_loss: 0.3880 - val_mae: 0.3880\n",
      "Epoch 238/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0666 - mae: 0.0669 - val_loss: 0.3805 - val_mae: 0.3805\n",
      "Epoch 239/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0667 - mae: 0.0664 - val_loss: 0.3882 - val_mae: 0.3882\n",
      "Epoch 240/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0893 - mae: 0.0921 - val_loss: 0.4011 - val_mae: 0.4011\n",
      "Epoch 241/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0703 - mae: 0.0709 - val_loss: 0.3926 - val_mae: 0.3926\n",
      "Epoch 242/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0642 - mae: 0.0650 - val_loss: 0.4126 - val_mae: 0.4126\n",
      "Epoch 243/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0818 - mae: 0.0829 - val_loss: 0.4484 - val_mae: 0.4484\n",
      "Epoch 244/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0913 - mae: 0.0924 - val_loss: 0.4054 - val_mae: 0.4054\n",
      "Epoch 245/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0678 - mae: 0.0676 - val_loss: 0.3826 - val_mae: 0.3826\n",
      "Epoch 246/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0824 - mae: 0.0834 - val_loss: 0.4222 - val_mae: 0.4222\n",
      "Epoch 247/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0679 - mae: 0.0672 - val_loss: 0.4109 - val_mae: 0.4109\n",
      "Epoch 248/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0596 - mae: 0.0602 - val_loss: 0.3887 - val_mae: 0.3887\n",
      "Epoch 249/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0523 - mae: 0.0525 - val_loss: 0.4022 - val_mae: 0.4022\n",
      "Epoch 250/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0982 - mae: 0.0991 - val_loss: 0.3978 - val_mae: 0.3978\n",
      "Epoch 251/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0621 - mae: 0.0630 - val_loss: 0.4193 - val_mae: 0.4193\n",
      "Epoch 252/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0882 - mae: 0.0882 - val_loss: 0.3945 - val_mae: 0.3945\n",
      "Epoch 253/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0765 - mae: 0.0781 - val_loss: 0.3954 - val_mae: 0.3954\n",
      "Epoch 254/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0649 - mae: 0.0658 - val_loss: 0.3923 - val_mae: 0.3923\n",
      "Epoch 255/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0547 - mae: 0.0539 - val_loss: 0.4046 - val_mae: 0.4046\n",
      "Epoch 256/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0710 - mae: 0.0717 - val_loss: 0.3822 - val_mae: 0.3822\n",
      "Epoch 257/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0618 - mae: 0.0618 - val_loss: 0.3956 - val_mae: 0.3956\n",
      "Epoch 258/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0810 - mae: 0.0806 - val_loss: 0.3960 - val_mae: 0.3960\n",
      "Epoch 259/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0717 - mae: 0.0727 - val_loss: 0.3928 - val_mae: 0.3928\n",
      "Epoch 260/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0605 - mae: 0.0615 - val_loss: 0.4010 - val_mae: 0.4010\n",
      "Epoch 261/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0630 - mae: 0.0639 - val_loss: 0.4066 - val_mae: 0.4066\n",
      "Epoch 262/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0873 - mae: 0.0866 - val_loss: 0.4023 - val_mae: 0.4023\n",
      "Epoch 263/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0683 - mae: 0.0692 - val_loss: 0.4055 - val_mae: 0.4055\n",
      "Epoch 264/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0690 - mae: 0.0684 - val_loss: 0.4121 - val_mae: 0.4121\n",
      "Epoch 265/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0716 - mae: 0.0709 - val_loss: 0.3947 - val_mae: 0.3947\n",
      "Epoch 266/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0756 - mae: 0.0757 - val_loss: 0.4114 - val_mae: 0.4114\n",
      "Epoch 267/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0705 - mae: 0.0711 - val_loss: 0.4246 - val_mae: 0.4246\n",
      "Epoch 268/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0724 - mae: 0.0732 - val_loss: 0.4217 - val_mae: 0.4217\n",
      "Epoch 269/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0870 - mae: 0.0875 - val_loss: 0.4059 - val_mae: 0.4059\n",
      "Epoch 270/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0689 - mae: 0.0698 - val_loss: 0.3978 - val_mae: 0.3978\n",
      "Epoch 271/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0733 - mae: 0.0727 - val_loss: 0.4093 - val_mae: 0.4093\n",
      "Epoch 272/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0718 - mae: 0.0719 - val_loss: 0.4141 - val_mae: 0.4141\n",
      "Epoch 273/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0659 - mae: 0.0667 - val_loss: 0.4104 - val_mae: 0.4104\n",
      "Epoch 274/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0788 - mae: 0.0795 - val_loss: 0.4033 - val_mae: 0.4033\n",
      "Epoch 275/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0694 - mae: 0.0700 - val_loss: 0.3909 - val_mae: 0.3909\n",
      "Epoch 276/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0644 - mae: 0.0656 - val_loss: 0.3867 - val_mae: 0.3867\n",
      "Epoch 277/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0660 - mae: 0.0650 - val_loss: 0.3819 - val_mae: 0.3819\n",
      "Epoch 278/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0708 - mae: 0.0716 - val_loss: 0.3906 - val_mae: 0.3906\n",
      "Epoch 279/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0623 - mae: 0.0624 - val_loss: 0.4087 - val_mae: 0.4087\n",
      "Epoch 280/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0785 - mae: 0.0799 - val_loss: 0.3835 - val_mae: 0.3835\n",
      "Epoch 281/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0664 - mae: 0.0665 - val_loss: 0.3934 - val_mae: 0.3934\n",
      "Epoch 282/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0662 - mae: 0.0660 - val_loss: 0.3733 - val_mae: 0.3733\n",
      "Epoch 283/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0640 - mae: 0.0622 - val_loss: 0.4053 - val_mae: 0.4053\n",
      "Epoch 284/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0746 - mae: 0.0730 - val_loss: 0.4002 - val_mae: 0.4002\n",
      "Epoch 285/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0632 - mae: 0.0637 - val_loss: 0.3965 - val_mae: 0.3965\n",
      "Epoch 286/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0799 - mae: 0.0793 - val_loss: 0.4036 - val_mae: 0.4036\n",
      "Epoch 287/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0649 - mae: 0.0633 - val_loss: 0.3918 - val_mae: 0.3918\n",
      "Epoch 288/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0628 - mae: 0.0619 - val_loss: 0.3979 - val_mae: 0.3979\n",
      "Epoch 289/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0781 - mae: 0.0799 - val_loss: 0.4001 - val_mae: 0.4001\n",
      "Epoch 290/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0705 - mae: 0.0708 - val_loss: 0.3850 - val_mae: 0.3850\n",
      "Epoch 291/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0530 - mae: 0.0529 - val_loss: 0.3953 - val_mae: 0.3953\n",
      "Epoch 292/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0683 - mae: 0.0682 - val_loss: 0.3840 - val_mae: 0.3840\n",
      "Epoch 293/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0569 - mae: 0.0573 - val_loss: 0.4019 - val_mae: 0.4019\n",
      "Epoch 294/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0669 - mae: 0.0669 - val_loss: 0.3850 - val_mae: 0.3850\n",
      "Epoch 295/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0598 - mae: 0.0597 - val_loss: 0.3861 - val_mae: 0.3861\n",
      "Epoch 296/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0653 - mae: 0.0661 - val_loss: 0.4113 - val_mae: 0.4113\n",
      "Epoch 297/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0653 - mae: 0.0667 - val_loss: 0.3811 - val_mae: 0.3811\n",
      "Epoch 298/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0636 - mae: 0.0644 - val_loss: 0.4257 - val_mae: 0.4257\n",
      "Epoch 299/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0711 - mae: 0.0714 - val_loss: 0.4100 - val_mae: 0.4100\n",
      "Epoch 300/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0674 - mae: 0.0684 - val_loss: 0.4131 - val_mae: 0.4131\n",
      "Epoch 301/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0869 - mae: 0.0866 - val_loss: 0.3965 - val_mae: 0.3965\n",
      "Epoch 302/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0569 - mae: 0.0563 - val_loss: 0.4177 - val_mae: 0.4177\n",
      "Epoch 303/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0721 - mae: 0.0728 - val_loss: 0.3851 - val_mae: 0.3851\n",
      "Epoch 304/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0602 - mae: 0.0598 - val_loss: 0.4124 - val_mae: 0.4124\n",
      "Epoch 305/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0673 - mae: 0.0680 - val_loss: 0.4166 - val_mae: 0.4166\n",
      "Epoch 306/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0685 - mae: 0.0692 - val_loss: 0.4265 - val_mae: 0.4265\n",
      "Epoch 307/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0576 - mae: 0.0586 - val_loss: 0.3857 - val_mae: 0.3857\n",
      "Epoch 308/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0619 - mae: 0.0626 - val_loss: 0.3877 - val_mae: 0.3877\n",
      "Epoch 309/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0602 - mae: 0.0604 - val_loss: 0.3983 - val_mae: 0.3983\n",
      "Epoch 310/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0650 - mae: 0.0639 - val_loss: 0.4233 - val_mae: 0.4233\n",
      "Epoch 311/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0736 - mae: 0.0722 - val_loss: 0.4011 - val_mae: 0.4011\n",
      "Epoch 312/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0663 - mae: 0.0665 - val_loss: 0.3924 - val_mae: 0.3924\n",
      "Epoch 313/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0577 - mae: 0.0578 - val_loss: 0.3940 - val_mae: 0.3940\n",
      "Epoch 314/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0858 - mae: 0.0852 - val_loss: 0.3884 - val_mae: 0.3884\n",
      "Epoch 315/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0586 - mae: 0.0589 - val_loss: 0.4558 - val_mae: 0.4558\n",
      "Epoch 316/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0972 - mae: 0.0993 - val_loss: 0.3972 - val_mae: 0.3972\n",
      "Epoch 317/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0560 - mae: 0.0559 - val_loss: 0.3969 - val_mae: 0.3969\n",
      "Epoch 318/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0569 - mae: 0.0555 - val_loss: 0.3969 - val_mae: 0.3969\n",
      "Epoch 319/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0618 - mae: 0.0628 - val_loss: 0.4114 - val_mae: 0.4114\n",
      "Epoch 320/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0727 - mae: 0.0737 - val_loss: 0.4059 - val_mae: 0.4059\n",
      "Epoch 321/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0569 - mae: 0.0558 - val_loss: 0.4230 - val_mae: 0.4230\n",
      "Epoch 322/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0674 - mae: 0.0673 - val_loss: 0.4164 - val_mae: 0.4164\n",
      "Epoch 323/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0664 - mae: 0.0676 - val_loss: 0.4400 - val_mae: 0.4400\n",
      "Epoch 324/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0693 - mae: 0.0709 - val_loss: 0.4028 - val_mae: 0.4028\n",
      "Epoch 325/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0637 - mae: 0.0645 - val_loss: 0.4110 - val_mae: 0.4110\n",
      "Epoch 326/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0613 - mae: 0.0612 - val_loss: 0.3927 - val_mae: 0.3927\n",
      "Epoch 327/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0466 - mae: 0.0471 - val_loss: 0.3977 - val_mae: 0.3977\n",
      "Epoch 328/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0667 - mae: 0.0676 - val_loss: 0.4157 - val_mae: 0.4157\n",
      "Epoch 329/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0753 - mae: 0.0760 - val_loss: 0.4184 - val_mae: 0.4184\n",
      "Epoch 330/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0612 - mae: 0.0618 - val_loss: 0.3991 - val_mae: 0.3991\n",
      "Epoch 331/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0549 - mae: 0.0553 - val_loss: 0.4039 - val_mae: 0.4039\n",
      "Epoch 332/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0827 - mae: 0.0846 - val_loss: 0.4036 - val_mae: 0.4036\n",
      "Epoch 333/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0742 - mae: 0.0746 - val_loss: 0.3972 - val_mae: 0.3972\n",
      "Epoch 334/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0474 - mae: 0.0482 - val_loss: 0.3935 - val_mae: 0.3935\n",
      "Epoch 335/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0713 - mae: 0.0735 - val_loss: 0.4184 - val_mae: 0.4184\n",
      "Epoch 336/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0690 - mae: 0.0689 - val_loss: 0.4141 - val_mae: 0.4141\n",
      "Epoch 337/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0572 - mae: 0.0576 - val_loss: 0.3985 - val_mae: 0.3985\n",
      "Epoch 338/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0569 - mae: 0.0559 - val_loss: 0.4245 - val_mae: 0.4245\n",
      "Epoch 339/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0650 - mae: 0.0664 - val_loss: 0.4060 - val_mae: 0.4060\n",
      "Epoch 340/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0632 - mae: 0.0638 - val_loss: 0.4008 - val_mae: 0.4008\n",
      "Epoch 341/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0599 - mae: 0.0602 - val_loss: 0.4082 - val_mae: 0.4082\n",
      "Epoch 342/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0684 - mae: 0.0695 - val_loss: 0.4011 - val_mae: 0.4011\n",
      "Epoch 343/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0525 - mae: 0.0530 - val_loss: 0.3917 - val_mae: 0.3917\n",
      "Epoch 344/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0637 - mae: 0.0633 - val_loss: 0.4348 - val_mae: 0.4348\n",
      "Epoch 345/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0724 - mae: 0.0732 - val_loss: 0.4192 - val_mae: 0.4192\n",
      "Epoch 346/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0638 - mae: 0.0647 - val_loss: 0.4022 - val_mae: 0.4022\n",
      "Epoch 347/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0580 - mae: 0.0593 - val_loss: 0.4066 - val_mae: 0.4066\n",
      "Epoch 348/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0614 - mae: 0.0621 - val_loss: 0.4136 - val_mae: 0.4136\n",
      "Epoch 349/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0697 - mae: 0.0704 - val_loss: 0.4104 - val_mae: 0.4104\n",
      "Epoch 350/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0606 - mae: 0.0611 - val_loss: 0.4347 - val_mae: 0.4347\n",
      "Epoch 351/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0722 - mae: 0.0740 - val_loss: 0.4223 - val_mae: 0.4223\n",
      "Epoch 352/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0578 - mae: 0.0583 - val_loss: 0.4206 - val_mae: 0.4206\n",
      "Epoch 353/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0581 - mae: 0.0594 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 354/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0572 - mae: 0.0572 - val_loss: 0.4101 - val_mae: 0.4101\n",
      "Epoch 355/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0518 - mae: 0.0524 - val_loss: 0.3965 - val_mae: 0.3965\n",
      "Epoch 356/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0693 - mae: 0.0693 - val_loss: 0.4185 - val_mae: 0.4185\n",
      "Epoch 357/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0561 - mae: 0.0554 - val_loss: 0.4089 - val_mae: 0.4089\n",
      "Epoch 358/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0514 - mae: 0.0516 - val_loss: 0.4029 - val_mae: 0.4029\n",
      "Epoch 359/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0781 - mae: 0.0772 - val_loss: 0.4050 - val_mae: 0.4050\n",
      "Epoch 360/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0485 - mae: 0.0479 - val_loss: 0.4049 - val_mae: 0.4049\n",
      "Epoch 361/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0688 - mae: 0.0681 - val_loss: 0.4037 - val_mae: 0.4037\n",
      "Epoch 362/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0568 - mae: 0.0567 - val_loss: 0.4241 - val_mae: 0.4241\n",
      "Epoch 363/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0638 - mae: 0.0650 - val_loss: 0.3942 - val_mae: 0.3942\n",
      "Epoch 364/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0632 - mae: 0.0645 - val_loss: 0.4042 - val_mae: 0.4042\n",
      "Epoch 365/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0593 - mae: 0.0592 - val_loss: 0.4182 - val_mae: 0.4182\n",
      "Epoch 366/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0513 - mae: 0.0521 - val_loss: 0.4000 - val_mae: 0.4000\n",
      "Epoch 367/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0635 - mae: 0.0649 - val_loss: 0.4111 - val_mae: 0.4111\n",
      "Epoch 368/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0574 - mae: 0.0588 - val_loss: 0.4070 - val_mae: 0.4070\n",
      "Epoch 369/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0671 - mae: 0.0672 - val_loss: 0.4341 - val_mae: 0.4341\n",
      "Epoch 370/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0651 - mae: 0.0647 - val_loss: 0.4031 - val_mae: 0.4031\n",
      "Epoch 371/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0618 - mae: 0.0618 - val_loss: 0.4103 - val_mae: 0.4103\n",
      "Epoch 372/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0489 - mae: 0.0482 - val_loss: 0.4436 - val_mae: 0.4436\n",
      "Epoch 373/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0743 - mae: 0.0726 - val_loss: 0.3966 - val_mae: 0.3966\n",
      "Epoch 374/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0637 - mae: 0.0620 - val_loss: 0.4116 - val_mae: 0.4116\n",
      "Epoch 375/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0748 - mae: 0.0764 - val_loss: 0.4124 - val_mae: 0.4124\n",
      "Epoch 376/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0566 - mae: 0.0563 - val_loss: 0.4305 - val_mae: 0.4305\n",
      "Epoch 377/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0615 - mae: 0.0627 - val_loss: 0.4227 - val_mae: 0.4227\n",
      "Epoch 378/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0495 - mae: 0.0499 - val_loss: 0.4243 - val_mae: 0.4243\n",
      "Epoch 379/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0543 - mae: 0.0550 - val_loss: 0.4225 - val_mae: 0.4225\n",
      "Epoch 380/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0554 - mae: 0.0540 - val_loss: 0.4117 - val_mae: 0.4117\n",
      "Epoch 381/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0491 - mae: 0.0504 - val_loss: 0.4138 - val_mae: 0.4138\n",
      "Epoch 382/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0642 - mae: 0.0633 - val_loss: 0.4078 - val_mae: 0.4078\n",
      "Epoch 382: early stopping\n",
      "Restoring model weights from the end of the best epoch: 282.\n"
     ]
    }
   ],
   "source": [
    "# Обучаю модель с лучшими значениями гиперпараметров \n",
    "# Использую EarlyStopping для предотвращения переобучения\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for _ in range(best_num_layers):\n",
    "    model.add(tf.keras.layers.Dense(best_num_neurons, activation=best_activation_function))\n",
    "model.add(tf.keras.layers.Dense(1))  \n",
    "\n",
    "if best_optimizer == 'sgd':\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=best_learning_rate)\n",
    "elif best_optimizer == 'adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate)\n",
    "elif best_optimizer == 'rmsprop':\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_learning_rate)\n",
    "elif best_optimizer == 'adagrad':\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=best_learning_rate)\n",
    "else:\n",
    "    raise ValueError(\"Invalid optimizer specified\")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=100)\n",
    "history = model.fit(X_train_numeric_scaled, y_train_scaled, epochs=1000, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# График потерь по эпохам \n",
    "# Визуализация предела итераций, после которых модель переобучается\n",
    "\n",
    "stopped_epoch = early_stopping.stopped_epoch\n",
    "restore_epoch = early_stopping.stopped_epoch - early_stopping.patience\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Validation Loss\\nHOMO', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "plt.axvline(x=stopped_epoch, color='red', linestyle='--', label=f'Stopped Epoch: {stopped_epoch}')\n",
    "plt.axvline(x=restore_epoch, color='blue', linestyle='--', label=f'Restored Epoch: {restore_epoch}')\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc='upper left', bbox_to_anchor=(0.09, 0.985))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    }
   ],
   "source": [
    "# Оцениваю обученную модель на тестовой выборке\n",
    "# Провел обратное преобразование предсказанных и тестовых значений таргета, которые ранее были нормализованы\n",
    "\n",
    "y_pred_scaled = model.predict(X_test_numeric_scaled)\n",
    "\n",
    "y_pred_inverse = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data: 0.509688444866814\n",
      "Root Mean Squared Error (RMSE) on test data: 0.7139246773062365\n",
      "R-squared on test data: -0.799196968532891\n",
      "Mean absolute error:  0.5855492234547932\n"
     ]
    }
   ],
   "source": [
    "# MSE, RMSE, R^2, MAE - метрики качества\n",
    "\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "r_squared = r2_score(y_test_inverse, y_pred_inverse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(\"Mean Squared Error (MSE) on test data:\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n",
    "print(\"R-squared on test data:\", r_squared)\n",
    "print(\"Mean absolute error: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtwAAAJDCAYAAAA4mcP/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAADQN0lEQVR4nOzdd1wUx/sH8M/Rm/SuCBbsithbKPYuxli/Ktg1lsQaNRrA2KPGRBM7YIsae+wVwd41SkARQRFB6SAdbn5/zO8OlruDo4ni83697nUyOzs7uwfJs3PPzogYYwyEEEIIIYSQCqFS2R0ghBBCCCGkKqOAmxBCCCGEkApEATchhBBCCCEViAJuQgghhBBCKhAF3IQQQgghhFQgCrgJIYQQQgipQBRwE0IIIYQQUoEo4CaEEEIIIaQCUcBNCCGEEEJIBaKAmxBClBAREQGRSAQPD48S7ScSieDi4iIo8/LygkgkwpUrV8qtf4QQQj5dFHATQj47kuBXJBLB0tISubm5cusFBwdL69nZ2VVIX1xcXCASiSqk7c/RlStXpNdc8tLS0kLt2rUxYcIEREREVHYXPwoPDw+IRKIv5nwJIUVTq+wOEEJIaampqeHdu3c4ffo0+vfvL7N9x44dUFGp3HGF4OBg6OjoVGofKkPLli3Rt29fAEBSUhKuXLmC7du34/Dhw7h9+zbs7e0ruYeEEPLx0Ag3IeSz1aFDBxgYGMDHx0dmW25uLvbs2YOuXbtCXV29EnrHNWjQADVr1qy041eWVq1awcvLC15eXli/fj0ePnyIUaNGITExEcuWLavs7hFCyEdFATch5LOlra2NYcOG4dSpU3j//r1g28mTJ/Hu3TuMHTtW7r5FfeWvbI61SCRCQECA9N+SV8E8b3k53Ir4+PhgwIABsLOzg5aWFoyNjdGjRw/4+/sL6l28eBEikQjffvut3HbCwsKgoqKCHj16CMpTU1Ph6emJxo0bQ1tbG4aGhujRoweuXbsm04YkVSYzMxOLFi1CnTp1oK6uDi8vL6XOpTCRSISpU6cCAO7evSstZ4zBx8cHHTt2hL6+PnR0dNCqVSu5N1EFPxc/Pz+0aNECOjo6guubmpoKb29vNGvWDDo6OjAwMICjoyMWL16MnJwcQXvh4eEYP348atasCU1NTVhZWcHDwwOvXr2S238XFxe8efMGw4cPh6mpKXR0dNCxY0dcvHhRUNfOzg47d+4EANSqVUv6e1Gwn5Kfo6KiMHr0aFhaWkJFRUXwO+fr64u2bdtCT08Penp6aNu2Lfz8/GT6Jknj8fLywr1799CtWzdUq1YNBgYGGDhwIKW1EPIJoJQSQshnbezYsdiyZQt2796N2bNnS8t9fHxgbGwMNze3Cju2p6cn/Pz88OrVK3h6ekrLmzdvXqr2pk6dCgcHB3Tt2hVmZmaIiorCsWPH0LVrVxw5cgQDBgwAAHTp0gV16tTBX3/9hTVr1sikrGzfvh2MMUyYMEFalpCQACcnJwQFBaFjx46YPHkyUlJScPz4cbi6uuLgwYNyr9WgQYPw+PFj9OzZE4aGhqhVq1apzq0gSc47Ywz/+9//sG/fPtjb22PEiBHQ0NDAhQsXMG7cOPz3339Ys2aNzP6//PIL/P39MWDAAHTv3h2qqqoAgPfv38PZ2RkhISFo3rw5pkyZArFYjJCQEKxatQqzZ8+GoaEhAOD27dvo0aMH0tLS0LdvX9jb2yMiIgJ79+7FmTNncPPmTdSuXVtw3MTERHTs2BFmZmYYP348YmNjceDAAfTs2ROHDh2SXr/vv/8efn5+ePz4Mb777jvpMQs/RxAfH4/27dvD2NgYw4YNQ2ZmJvT19QEAM2bMwIYNG1C9enWMGzcOAHD48GGMGTMGDx8+xG+//SZzXe7evYvVq1fD1dUVkyZNwsOHD3Hs2DE8efIET58+hZaWVqk+L0JIOWCEEPKZCQ8PZwBYjx49GGOMNWnShDVu3Fi6PTo6mqmpqbHp06czxhjT1NRktra2gjbc3d0ZABYeHi7TvqenJwPA/P39ZY7p7u4uqOvs7MyK+k8pAObs7Fxs+4wx9vLlS5n93759y6ytrZm9vb2gfNWqVQwA8/PzE5Tn5OQwKysrZm5uzrKzs6XlI0aMYADYtm3bBPXfvXvHbGxsmJmZGcvIyJA5r+bNm7P4+HiF51eYv78/A8AmTZokKBeLxdJrPmbMGMYYY1u3bpX+XLCvWVlZrF+/fgwAu3fvnrRcct10dXXZv//+K3PsQYMGMQBs4cKFMttiYmJYTk4OY4yx7OxsZmdnx6pVq8YePHggqHf16lWmqqrK+vbtKygHwACwESNGMLFYLC1//Pgx09DQYGZmZiw9PV1aXtTvV8H2xowZw3JzcwXbAgICGADWsGFDlpSUJC1PSEhg9erVYwBYYGCgtFxyzQGw/fv3C9oaNWoUA8D27dsntx+EkI+DUkoIIZ+9sWPHIigoCLdv3wYA7Ny5E7m5uQrTST5V8kaPraysMGjQIISGhgpSHcaMGQMNDQ1s375dUP/UqVOIjo6Gu7u7NHc9Li4OBw4cQOfOnTF+/HhBfXNzc8ydOxexsbEyqREA4O3tDWNj4xKfy71796Q53DNnzkSLFi2wc+dOGBsb48cffwQAbNy4Ebq6uvjjjz8EefYaGhrSPO99+/bJtD1x4kQ0bdpUUBYTE4MjR46gTp06ctNeLCwsoKbGv9Q9efIkIiIiMHfuXDg6OgrqderUCQMGDMDp06eRkpIi2Kaqqorly5cLZqVp1qwZRo0ahdjYWJw+fboEV4if5+rVq6Uj9BKSdBQvLy8YGBhIy42MjKTfpMhLLXFycsLQoUMFZZK/gYJpPISQj49SSgghn72RI0fihx9+gI+PD9q2bQtfX184OjqWOrWjsrx8+RIrVqzA5cuXERUVhaysLMH2t2/fwtbWFgBgZmaGr7/+Gvv370dISAgaNGgAANIAvGBgfffuXeTl5SErK0tuMBoaGgoACAkJkc4sItGmTZtSncv9+/dx//59ADywrF69OiZMmIAff/wRtra2SE9Px5MnT2BtbY1Vq1bJ7C/Jtw4JCZHZJq9P9+7dA2MMrq6uxT4ke+vWLQDAs2fP5F6PmJgYiMViPH/+HK1atZKW16xZU3r9C/rqq6+wY8cOPHz4EIMGDSry2AXVqlULpqamMuUPHz4EALm5/66urgCAR48eyWxr2bKlTFmNGjUA8JliCCGVhwJuQshnz8zMDP369cP+/fsxePBgPHv2DBs2bKjsbpXIixcv0KZNG6SkpMDV1RX9+vWDvr6+9EG6gIAAmQB80qRJ2L9/P7Zv3441a9bg7du3OHPmDJydnVGvXj1pvYSEBADA9evXcf36dYV9SEtLkymzsLAo1flMmjQJmzdvVrg9MTERjDFERUXB29u7zH1KTk4GAFSvXr3Yvkmux969e4usV/jYiq6FpFzSB2Upai8lJQUqKiowMzOTu49IJJIZfQcgzf8uSDKqn5eXV6K+EULKFwXchJAqYdy4cThy5Ag8PDygpaWF//3vf0XWl8zPLW/RnJIGTuXh119/RWJiInbv3o2RI0cKtk2ePFk6G0pBLi4uaNCgAXbt2oXly5fD19cXeXl5goclgfxAbPbs2XIfQixKRS3qI+lTy5Ytce/evRLtK69PkgcTo6KilD72iRMnZEb0i/Lu3bsiywumfyhD0bXV19eHWCxGbGwszM3NBdvev38Pxpjc4JoQ8umiHG5CSJXQo0cPVK9eHVFRUXBzc4ORkVGR9SXb5QVokq/0lSHJvy3rCGJYWBgASGcikWCMFTkqPXHiRMTGxuLYsWPw8fGBkZGRTFpD69atIRKJcPPmzTL1sTxVq1YNDRs2RHBwcLmkO7Rq1QoqKirw9/eXmf6vsLZt2wJAia/H69ev5U4ZePXqVQAQ5IOX5fdC0o68aSklZZ9buhQhXzoKuAkhVYKqqiqOHTuGo0ePYsWKFcXWb926NQDZh88OHTokdzRZEckDhZGRkcp3Vg5JbnDhObFXrlyJp0+fKtzP3d0dWlpamDlzJl6+fIlRo0bJTP9maWmJIUOG4MaNG/jll1/AGJNp5/bt20hPTy/TOZTUjBkzkJ6ejgkTJshNHQkPD1d6DmkLCwsMGjQIYWFhclNU3r9/L/02Y8CAAahZsybWrVuHwMBAmbo5OTly5ybPy8vDwoULBdfv33//xe7du2FmZobevXtLy8vye+Hu7g6AP7BaMHUkOTlZem6SOoSQzwOllBBCqoxWrVoJHnIryoABA1CnTh34+fkhMjISjo6OCA4OxuXLl9G7d2+lZ5zo3LkzDh06hEGDBqFXr17Q0tKCg4MD+vXrV6K+T548Gb6+vhg0aBCGDBkCExMT3Lp1Cw8ePECfPn1w6tQpufsZGxtj8ODB2L17NwDIpJNI/Pnnn3j27BnmzZuH3bt3o3379jA0NERkZCTu3buH0NBQREdHf9Rl6CdNmoRbt25h586duH79Orp27Qpra2u8e/cOISEhuH37Nv766y+Z+asV+fPPP/H06VMsW7YMp0+fRufOncEYw/Pnz3H+/Hm8e/cOhoaG0NTUxKFDh9CrVy84Ozujc+fOaNq0KUQiEV69eoWrV6/CxMRE5oHNZs2a4dq1a2jdujW6du0qnYc7NzcXW7duhba2trRu586dsWbNGkycOBGDBg2Crq4ubG1tMWrUqGLPw8nJCdOnT8eGDRvQpEkTDBo0CIwxHD58GG/evMGMGTPg5ORUomtNCKlcNMJNCPkiaWtr4+LFi3Bzc8OdO3ewadMmZGZmIjAwUDr6rYwJEyZg3rx5iIuLw6pVq7B48WIcPny4xP1xdHTE+fPn0aJFCxw5cgQ+Pj4wNDTE9evXi72JkIx2tmvXDk2aNJFbx9jYGDdu3MDq1auhoaGBvXv3YsOGDbh16xYaN26MXbt2yZ0xoyKJRCL4+fnhwIEDaNy4MU6ePIl169bhwoUL0NLSwpo1a9C1a1el2zM1NcWtW7ewePFiZGRkYOPGjdixYwfevHmD+fPnQ1dXV1q3devW0oVpIiMjsXnzZvj4+CAkJARubm74888/Zdo3MjLC9evXUbduXWzbtg1//fUXmjVrhrNnz8osGtSrVy+sXr0aALB27VosXrwYO3bsUPpcfv/9d/j4+MDS0hJbt27Ftm3bYGVlBR8fH7mL3hBCPm0iJu+7RUIIIZ+NNWvWYO7cudixY8dnN/f450IkEsHZ2VluXjUhhBSHRrgJIeQzlpmZiY0bN8LIyAjDhg2r7O4QQgiRg3K4CSHkM3Tt2jUEBATg3LlzePXqFVasWPFR868JIYQojwJuQgj5DF28eBHe3t4wNTXFzJkzMWfOnMruEiGEEAUoh5sQQgghhJAKRDnchBBCCCGEVCAKuAkhhBBCCKlAFHATQj5pHh4eEIlESq84SEhJeHl5QSQSCab7i4iIgEgkgoeHR6X1S1lXrlyBSCSCl5dXZXeFEFIECrgJqWLGjh0LkUgEExMTZGVllbk9eQHJp8zOzk5mafPCRCIRGjRoIHdbXl4efH190a1bN5iZmUFDQwOWlpbo169fkQvaiEQiiEQiaGpqIj4+Xm6dxMREaGtrS+sqcvToUfTv3x9WVlbQ0NCAmZkZunbtCh8fH+Tl5RV5bp8qye9RwZeuri6aNWsGLy8vuUu7f85EIhFcXFwquxsyJDewt27dUljHxcUFIpEIMTExcrf7+/tj6NChsLGxgaamJoyNjdGpUyf8+uuvyMzMLLJNkUiEkydPKjx227ZtpfUU/Tfn4cOHGDNmDGrXrg1tbW0YGBigVatWWLJkCZKTkxWfPCGViAJuQqqQ1NRU/P333xCJREhISMCxY8cqu0uflffv36NTp04YO3YsgoKCMGDAAMyZMwfdu3fHjRs38M0336B///4Kg0M1NTVkZ2dj7969crfv3bsXmZmZUFOTP0FUWloaBgwYgK+//hrXrl1D165dMWfOHLi5uSE4OBjjxo1Dp06dEBsbW27n/LENGjQInp6e8PT0xMiRIxEbGwtvb284OTkhOzu7srsHAKhevTqCg4OxYsWKyu7KJyU3NxeTJk1C586dcerUKbRr1w6zZs3CsGHDEBMTg1mzZsHBwQEvXrxQ2Iaamhp8fHzkbgsKCsKdO3cU/n0AwJIlS9CyZUvs3bsXjRs3xnfffQcPDw/k5OTA09MTDRo0wN27d8t8roSUO0YIqTK2bdvGALBZs2YxFRUV1q1btzK36enpyQAwf3//snewFNzd3RkAFh4erlR9W1tbpqmpWWQdAKx+/fqCsuzsbNa+fXsGgI0bN46lp6cLticmJrI+ffowAGzw4MEK26xXrx5r3ry53OM6Ojqy+vXrs/r16zN5//n95ptvGADWp08flpiYKNiWkZHBxo8fzwCwDh06sJycnCLP8VMj+T3at2+foDwlJYU1aNCAAWC+vr6V1q/y/v0GwJydncu1TXn8/f0ZAObp6alUfcnf082bNxXWcXZ2ZgBYdHS0oHzOnDkMAGvdujV78+aNYFtubi776aefGABWp04dlpycLLfNfv36MXV1dfb+/XuZ486cOZOpqKhI/84KfyYbN25kAFjt2rVZcHCwzP6bN29mqqqqzMTEhL1+/bq4S0HIR0Uj3IRUITt27ICamhrmzZsHV1dXXLp0Ca9evVJYPzAwEG5ubrCwsICmpiZsbGyko6sA/xrY29sbAODq6ir9qtfOzk7aRlFfndvZ2QnqAsDz588xb948tGjRAiYmJtDS0kK9evUwf/58fPjwoUznXxY7d+7EzZs38dVXX2Hbtm3Q1tYWbDc0NMTBgwdRt25dHDx4EJcvX5bbzpgxY/Do0SM8ePBAUP748WPpV+HyXLx4EYcOHYK9vT0OHjwIQ0NDwXYtLS1s3boVnTp1wo0bN7Br165iz6lLly5QUVFR+DswY8YMiEQiXLhwQVp2+PBhODs7w9zcHFpaWrC2tkbXrl2LTKcpi2rVqklzpSUjk35+fhCJRPDz88OJEyfQsWNHVKtWTfC7lJ2djXXr1qFFixbQ1dVFtWrV8NVXX+Gff/6Re5zIyEgMHz4cxsbG0NPTg7OzMwIDA+XWLSqHOzU1Fd7e3mjWrBl0dHRgYGAAR0dHLF68GDk5OdKcagAICAgQpND4+fkJ2jp+/Di6dOkCIyMjaGlpoUmTJlizZo3ctKGMjAzMnz8fNjY20rrbtm0r5uqWn+fPn2PdunUwNjbGiRMnUL16dcF2VVVVeHt7Y8SIEQgLC8OaNWvktjN27Fjk5ORg9+7dgvKcnBzs2bMH3bt3R40aNWT2S0xMxIIFC6ChoYETJ07ITQmbNGkSfvjhB8THx+PHH38sw9kSUv4o4Cakivjvv/9w69YtdO/eHRYWFhg9ejTEYjF8fX3l1v/tt9/g4uKCCxcuoFu3bpg9ezY6d+6Mx48f49ChQwB4vqezszMAwN3dXZoK8P3335e6n0eOHMGOHTtQu3ZtuLu7Y/LkyTA2NsaqVavQrVs35OTklLrtspBcpx9//FFhfrW2tjZmz54NAAq/Fnd3d4eqqqrMdd+xYwdUVVUxevToIo8/e/ZsmWBfQiQSSQMJRccvaNSoUWCMyU1xyc3Nxf79+2FtbY0uXboAADZt2oRvvvkGoaGhGDhwIGbNmoWePXsiJiYGR48eLfZ4ZVX4uh88eBBff/01zM3N8e2336JXr14AgKysLPTo0QOzZ88GYwzjxo3DyJEj8erVKwwYMAAbN24UtBMdHY327dtj//79aNOmDWbMmAFjY2N069atyFzmwt6/f482bdrAy8sLqqqqmDJlCsaOHQtLS0usWrUKaWlpsLOzg6enJwDA1tZW+jfj6emJ5s2bS9tasGAB3Nzc8OzZM3z99df49ttvoa2tjblz52LYsGGC44rFYvTv3x+rVq2CkZERvvvuO7Rr1w4zZ87E2rVrS3KJS23nzp0Qi8WYOHEiLCwsFNZbvHgxAMW/n+3atUOjRo1k/j5OnDiB2NhYjB07Vu5+hw4dQmpqKr7++ms0atRI4fHnzp0LLS0t7N+/H+np6cWdFiEfT2UPsRNCysesWbMEX9mnpqYyXV1dVrNmTZaXlyeo++jRI6aiosKsra1lUjXEYjGLioqS/lzcV+4o4qtzW1tbZmtrKyh78+YNy8rKkqnr7e3NALA9e/YIykuTUqKqqso8PT0VvlAopSQnJ4epq6szNTU1lpGRUWT7z58/l36tXVDBNvv27cuMjY1ZZmYmY4yxzMxMZmxszPr168cYY3JTSuzs7BgAFhoaWuTx09PTmZqaGtPQ0GC5ublF1k1JSWHa2tqsUaNGMttOnDjBALA5c+ZIy1q0aME0NDTYu3fvZOrHxcUVeaziKEopSU1NZY0aNWIA2M6dOxljjPn6+jIATEVFhV24cEGmrYULFzIAbPHixUwsFkvLU1JSWKtWrZiGhobgd1jyO7R06VJBO1u2bGEAZH6/w8PDGQDm7u4uqD9o0CAGgC1cuFCmTzExMYI0n6L+Ls6fP88AsB49erAPHz5Iy8ViMZs8eTIDwA4dOiQtl1yPnj17Cj7zf//9l2loaJQqpWTcuHEK/z5sbW1lUkpcXFwYALmfR2HW1tYMgCCto2Caypo1axgAdufOHen23r17MxMTE5aVlcUmTZok85l4eHgwAGzbtm3FHr9Dhw4MAAsMDFTqmhDyMVDATUgVkJ2dzczMzJi+vr4gYBw5ciQDwM6dOyeoP2XKFAaA+fj4FNt2eQfcisTHxzMAzMPDQ1BemoBbEkQV9SoYcMfExDAAzNLSstj2MzIyGACmra0tKC/Y5pEjRxgAtn//fsYYY/v372cA2NGjRxlj8gNuLS0tBkAapBfFwsKCAZAbGBc2fPhwBoDdv39fUD5kyBAGgD169Eha1qJFC6arq8sSEhKKbbekJL9HgwYNkgZ2kydPlgZnrVq1kt6ISQLMgQMHyrSTl5fHjIyMWJ06dQTBtsQ///zDALANGzYwxhjLyspiWlpazNzcXOZmKi8vj9nb2ysVcEdHRzORSMTq1KnDsrOziz3fov4u+vfvzwCwV69eyWxLSkpiIpGIDRo0SFrm6uoq9zNkjLFx48aVKuBW5lUw4Jbk2YeEhBR7jLZt2zIA7Pbt29KyggH3u3fvmLq6Ops8eTJjjLGoqCimqqrKvvvuO8YYkxtw9+zZkwFgZ8+eLfb4Q4cOZQDYgQMHlLomhHwMih8FJoR8No4fP47Y2FiMGzdOMCXe6NGjsWfPHuzYsQPdu3eXlt+5cwcABGUfC2MMvr6+8PPzw9OnT5GcnAyxWCzd/vbt2zIfQ1NTU+H0ZIBs6kJ569u3L8zNzeHj44OhQ4fCx8cH5ubm6Nu3b4UeV55Ro0Zh37592L17N1q0aAEASElJwYkTJ9C0aVM4ODhI6w4bNgzz5s1DkyZNMGLECLi6uqJTp07Q19cvt/4cPnxYmg+uo6ODOnXqYOLEiZgzZw40NDQEddu0aSOz/7Nnz5CYmAhra2vp8wUFSWZwCQkJkdbPzMxE586dZaaLVFFRQceOHREaGlpsv+/duwfGGFxdXaGurq7cySpw69Yt6OrqKky70NbWlvYf4Pn/urq60s+voK+++go7duwocR9u3ryJdu3ayd3m4uKCgICAErepLHNzc/Tp0wf79+/Hr7/+ip07dyIvL09hOgkhVQEF3IRUAZL/4RbOD+7SpQuqV6+O48ePIyEhAcbGxgCA5ORkiEQiWFlZffS+zpgxAxs3boSNjY10rmlNTU0AgLe3d7nMHV5SJiYmUFdXR1xcHDIzM4ucxzsyMhIAirx26urqGDlyJNavX48bN27g4sWLmDlzZpHTnVlaWiIiIgKRkZGoW7euwnoZGRmIj4+HhoYGTExMij03SU7//v37sWbNGqiqquLQoUPIyMjAqFGjBHXnzJkDExMTbNq0CWvXrsWaNWugpqaGPn364Ndff0WtWrWKPV5x9u3bJ5OjrIi8XOGEhAQAfAq5oKAghftKpm6UzMtsbm6u9DHkkbRT+GHB0khISEBubq7cGwaJglNPJicnw8bGRm49ZftfVpaWlggJCUFkZCTq169fZF1l/kbGjh2LY8eO4fDhw/D19UXLli3RrFmzIo9fsO2yHp+Qj40emiTkMxcZGYnz588DAJydnQWzIqiqqiIqKgpZWVnYs2ePdB9DQ0MwxhAdHV3m44tEIuTm5srdVngRivfv3+OPP/5As2bNEBISAj8/P6xYsQJeXl6YPHlymftSWmpqamjdujVyc3OLHdm7dOkSAKB9+/ZF1hs3bhzEYjGGDBkCsViMcePGFVm/Q4cOgvYVCQgIQG5uLlq3bg1VVdUi6wJ89ojhw4cjJiYGFy9eBADs3r0bKioqGDFihKCuSCTC2LFjcffuXcTGxuLo0aP4+uuvcfz4cfTt2/ejL7oj75sIyWj7oEGDwHhapNyX5KE8AwMDAPx3T553794p1RfJrDFRUVElPQ0Z+vr6MDExKbL/4eHh0voGBgYK515Xtv9lpezvZ0hICN6+fYvq1asrvEkAgN69e8PKygo//PADQkNDy+3vIykpCQ8ePICGhgZatmxZZF1CPiYKuAn5zPn5+UEsFqNTp04YN26czMvd3R0ABF87S76qlwTqRZEEdYqCLSMjI7lBSEREBJKSkgRlL1++BGMMXbt2hY6OjmDb1atXi+1LRZJMAbdixQowxuTWyczMxLp16wCg2K+/GzVqhLZt2yIqKgrt2rVDw4YNlTr+unXrFKbDMMaki7GU5Ot3yUj2nj17EBkZiYCAALi6uhY5WmtiYgI3NzccOHAAnTt3xn///VfkgiYfS8OGDaGvr4979+4pNaNNvXr1oKWlhXv37slcV7FYjBs3bih13FatWkFFRQX+/v5KHVdFRUXh30zbtm0RHx+vVCoLADg4OCAtLU1mqkng4/3djB49GioqKti2bVuRCy8tW7YMQPG/n5IZe6KioqClpYXhw4cXWf+bb76Bnp4ejhw5Iki3KWzt2rXIzMzE0KFDZf4bQ0il+rgp44SQ8iQWi1mtWrWYSCRiYWFhCutJFnS5e/cuY4zPbqCqqsqsra1ZRESETJsFZ3iQLDahaFGSHj16MADsypUr0rKsrCw2cOBABkDw0OTbt28ZANauXTvBzCmRkZGsTp06ch80+5gL30ge9po4caLMA3ZJSUnSh92KWvimoKCgIHb06FEWFBQkKFe08M3XX38tXRwkKSlJsC0zM1P6MFlpFr5p1KgR09XVZT/++CMDwPz8/GTq+Pv7yzyImJ2dzZo3b84ACH5XXr16xYKDg1laWppSx1c0S4k8kocmFf3O/fDDDwwAmzFjhtwHGJ88eSJ4oHT06NHlMkvJ4MGDGQD2448/yhzz3bt3gs/E1NSU2dnZye3/mTNnGADWqVMnubO/REdHs//++0/6s4+PT7nPUlKahW9mzpzJALC2bduyt2/fCrbl5eWxJUuWMBSz8E3BNt+9e8eOHj3KAgICBHXlPTTJGGO//fYbA8Dq1q3Lnj17JtPv7du308I35JNFOdyEfMYuX76M8PBwODs7o3bt2grrjRkzBjdv3sSOHTvQqlUrNG3aFOvXr8eMGTPQuHFjuLm5wdbWFjExMQgMDESfPn2wfv16APkL3ixcuBBBQUEwMDCAoaEhpk2bBgCYNWsWzp8/j969e2P48OHQ0dHBhQsXYGhoKJNDaWVlhUGDBuHw4cNo1aoVunTpgnfv3uHkyZPo0qULwsLCKuxaFUddXR3Hjx9H//79sXXrVpw8eRK9e/eGubk5oqKicPLkScTHx6Nv374K5zYvrFGjRkXOGVzYzp07kZmZiRMnTqB27dro06cPbGxsEBsbi9OnTyMqKgpt27bF0aNHi8wHl2fUqFFYsGABVq9eDR0dHQwaNEimjpubG/T19dGuXTvY2toiJycHFy5cwH///YdvvvkGtra20rqjR49GQEAA/P39FS58VFG8vb3x4MED/P777zh16hScnJykn9OTJ0/w+PFj3Lx5U5q3vXLlSly6dAmLFi3CtWvX4OjoiODgYJw+fRrdu3dX6pseAPjzzz/x9OlTLFu2DKdPn0bnzp3BGMPz589x/vx5vHv3Tpp60rlzZ/z9999wc3ODo6MjVFVV0b9/fzRr1gw9e/bE4sWL8fPPP6Nu3bro2bMnbG1tER8fjxcvXuDq1atYunSp9FsRd3d3/PXXXzh79iwcHR3Rq1cvJCQkYN++fejevTtOnjxZIde5sNWrVyM5ORk+Pj6wt7dHnz59UKdOHaSkpOD8+fMIDQ2Fvb09Tp8+rdSDtubm5nBzc1P6+DNmzEBcXBx+/vlnNG3aFD179kTDhg2RmZmJK1eu4PHjx7CwsMA///xTZDoLIZWisiN+QkjpSaZ8K25J7OTkZKatrc0MDAwES5b7+/tL54zW0NBgNWrUYIMGDWLXr18X7O/n58eaNm3KNDU1ZUatGWPs4MGDrGnTpkxDQ4NZWlqy6dOns9TUVLnTAqamprLZs2czOzs7pqmpyezt7dnPP//MsrOzK3WEWyInJ4dt376dde7cmZmYmDB1dXVmbm7O+vTpww4ePFiqNgtTNMLNGP+G4eDBg6xPnz7MwsKCqaurMxMTE9a5c2e2ffv2Ui/p/vr1a6aiosIAsOHDh8ut8+eff7L+/fszW1tbpqWlxUxMTFibNm3Ypk2bZEaSJSOWyi6JXp4j3IzxpcS3bNnCOnbsyPT19ZmmpiarWbMm69mzJ9u0aZNgfmvG+Ij80KFDmaGhIdPR0WFfffUVCwgIkDvtpaIRbsb439LixYtZgwYNmKamJjMwMGDNmzdnP/30k+AaRUdHsyFDhjBTU1PpdS98PhcuXGD9+vVjZmZmTF1dnVlaWrL27duzn3/+WWaENi0tjc2bN49Vr16daWpqskaNGrGtW7d+1KXdC/Z78ODBzNramqmrqzNDQ0PWvn17tnbtWsF/X0rSZkGKRrgl7t27x0aPHi39W69WrRpzdHRkXl5eLDExsdj2CakMIsYUJCsSQgghhBBCyowemiSEEEIIIaQCUcBNCCGEEEJIBaKAmxBCCCGEkApEATchhBBCCCEViAJuQgghhBBCKhAF3IQQQgghhFQgWvimkojFYrx9+xbVqlWDSCSq7O4QQgghhJBCGGNITU2FtbU1VFRKP05NAXclefv2La2ERQghhBDyGYiMjESNGjVKvT8F3JWkWrVqAPgHqMwSuIQQQggh5ONKSUmBjY2NNG4rLQq4K4kkjURfX58CbkIIIYSQT1hZ03/poUlCCCGEEEIqEAXchBBCCCGEVKAqEXD7+flBJBLJfcXExCjVRnBwMHr27Ak9PT0YGxtj1KhRiI2NlaknFouxevVq1KpVC1paWmjWrBn27dtX3qdECCGEEEKqiCqVw71kyRLUqlVLUGZoaFjsfm/evIGTkxMMDAywfPlyfPjwAWvWrMGTJ09w584daGhoSOv++OOPWLlyJSZMmIDWrVvj+PHjGDFiBEQiEYYNG1bep0QIIYQQQj5zVSrg7tWrF1q1alXi/ZYvX460tDTcv38fNWvWBAC0adMG3bp1g5+fHyZOnAgAiIqKwtq1azF16lRs3LgRADB+/Hg4Oztj7ty5GDx4MFRVVcvvhAghhBBCyGevSqSUFJSamoq8vLwS7XP48GH07dtXGmwDQNeuXVGvXj38/fff0rLjx48jJycH3377rbRMJBJhypQpePPmDW7evFn2EyCEEEIIIVVKlQq4XV1doa+vDx0dHfTv3x+hoaHF7hMVFYX379/LHRlv06YNHj58KP354cOH0NXVRcOGDWXqSbYTQgghhBBSUJVIKdHR0YGHh4c04L5//z7WrVuHDh064MGDB0Wu6BgdHQ0AsLKyktlmZWWFhIQEZGVlQVNTE9HR0bCwsJCZi1Gy79u3bxUeJysrC1lZWdKfU1JSSnSOhBBCCCHk8/TJBdxisRjZ2dlK1dXU1IRIJMKQIUMwZMgQabmbmxt69OgBJycnLFu2DJs3b1bYRkZGhrStwrS0tKR1NDU1pe9F1VNkxYoV8Pb2Vuq8CCGEEEJI1fHJpZQEBgZCW1tbqdezZ88UttOpUye0bdsWFy9eLPJ42traACAYfZbIzMwU1NHW1laqnjwLFixAcnKy9BUZGVlkvwghhBBCSNXwyQXcDRo0gK+vr1IveWkgBdnY2CAhIaHIOpI2JKklBUVHR8PY2Fg6qm1lZYWYmBgwxmTqAYC1tbXC42hqakqXcf9klnN3cQFEIv46eVJxvbZt8+tduSLclpsLbNwItG8PGBgAGhqAlRXfZ+ZMoHBeu4dHfluKXn5+ZT+3rCxgyRLA3h7Q0gKsrYGJE4H370vWjp2d4n66uJTPsRMTgTlzgLp1AU1NwMwM+OYbICio6L5duQIMGACYm/P9bGyAgQOBx49l6759C3z3HdCoEaCrC1hYAJ06Abt3A/IeMs7NBXx8+OdqZgZUq8b3nTcPUHJue0IIIYRwn1xKiaWlJTw8PMqlrZcvX8LMzKzIOtWrV4eZmRnu3bsns+3OnTto3ry59OfmzZtj+/btCA4ORqNGjaTlt2/flm7/LKmp8eCqb1/ZbUFBwJ07vE5urnBbXh7Qqxdw8SIPKgcP5oFcUhLw4AHw++88uHN0lG133DigRg35/SnrdRSLeSB67hzQrh0waBAQGgps3w5cugTcusWDSGUZGADffy9bbmdX9mPHx/OgNjSUvw8YAERHA4cPA2fOAJcv85uXwpYtAxYt4tfdzQ0wNQXevQOuXweePAEcHPLrvnzJ24iPB3r0APr1A1JSgGPHgNGj+TF8fYXtDx0KHDnCbwKGDeMB/a1bwC+/AHv28M/X0lL5a0gIIYR8yVgV8P79e5myU6dOMQBsxowZgvIXL16wFy9eCMomT57MtLW12evXr6VlFy9eZADYpk2bpGWRkZFMXV2dTZ06VVomFovZV199xapXr85yc3OV7nNycjIDwJKTk5Xep9w5OzMGMNavH2Pq6ozJuY5s5kzGVFQY69OH1/X3z9+2axcv69mTsexs2X2joxm7f19Y5u7O97l5sxxPpBAfH36M4cMZE4vzyzdt4uUTJyrflq0tf1XUsadO5eWzZgnLb9xgTFWVsUaNGMvLE247dozv4+bGWHq6bB9ycoQ/T5nC669fLyxPTGSsZk2+LSIiv/z2bV7Wpo3s5zpjBt/m7a3wEhBCCCFVRXnFa1Ui4K5bty4bPHgwW7VqFdu8eTObOHEiU1NTYzY2NiwmJkZQ19bWltkWCqBev37NTExMWJ06ddjvv//Oli9fzoyMjFjTpk1ZZmamoO7cuXMZADZx4kS2bds21qdPHwaA7d27t0R9/qQC7qNH+fvatcLt2dmMmZnxgHrSJNmAWxLIHT2q/DE/RsDdvr1sEMkYD4Br12ZMV1d+oCpPSQPukh67Rg1+Q5OaKtuWmxtv6/JlYXmDBoxVq8ZYUpJyferRg7fz/LnsthEj+LZ79/LL9u3jZQsXytY/f55vmzZNuWMTQgghn7Hyitc+uRzu0hg6dChCQ0OxfPlyTJ8+HWfPnsWECRNw9+5dWFhYFLu/jY0NAgICUKdOHcyfPx+rV69G7969ceHCBZlZSVauXInly5fj3LlzmDp1KiIiIrBnzx6MGDGiok6v4rVrx/NzC6cVnDgBxMYCY8fK38/EhL8/f16x/ZPkTCsjMxO4fRuoXx+wtZVtp1s3IC0NkJNCpFBWFs8rX76c56v/fwpRuRw7Joang+jpybZXqxZ/v3w5v+zxYyAkhLelp8fTTlatAjZskJ+7DQBNmvD306eF5UlJPAXF0pJ//hKNG/P3ixeBnBzhPpJc/y5d5B+LEEIIUVJyejbC3n/Aw9eJCIv9gOR05Wap+xx9cjncpbF06VIsXbpUqboRERFyyxs3boxz584Vu7+KigoWLFiABQsWlKSLn76xY/mDe3fvAq1b87IdO3hQPWAAzz8ubOBAYOVKYPFinifcpw/QqhV/aLI427cDZ8/K3zZ/Pn/YsDTCwngetb29/O2S8tBQ4KuvlGszJgYYM0ZY1ro1sG8fUKdO2Y5tasofpvzwQTboDg/n7wVvaO7f5+/GxkDHjrLB///+x/PxNTTyy+bO5TdPM2fya96sWX4Ot44OcPQoUHCGnaZN+QOWv/3GA/FevXgO982b/Pje3jxvnBBCCCmlt0kZ+OHwv7gaGictc7I3xcpBzWBtqHjWt89VlQi4STkYNQpYsIAHa61b81ktzp0Dpk0TBm8FtWgB7NzJg7MtW/gL4A9Ddu3K923ZUv6+O3Yo7sv33wsD7uBg5c8jOZm/GxjI3y6ZHUZSrzhjxvDguEkTHhA/fw6sW8dn9+jShT+gWK1a6Y/dqxf/ZsHbmz+QKHH7dv5oclJSfrlkphNfXz4Cfvky/7xCQ4GpU4G9e4Hq1fmot4SFBQ+WR47kI+KSGx1tbWDyZOEDlhLr1/P2587lo+cS/foBX39d1BUjhBBCipScni0TbANAYGgc5h/+FxuGO8JAR0Hs8ZmqEiklXzwvL9lXwSBNGebmfIR6/36eGrFzJ5+FRFE6icSIEUBkJPDPP3zKuC5d+GwYfn5AmzaAokWHbt4EeDaw7MvQUFi3QQP+qgyenkDnzvz66OjwGVR27eI3KK9eAdu2la39JUv4NwJr1vBp+ubM4aPUTk75aR4qBf5MxeL89wMHAFdXfiPg6MhHrPX0eNpLwfniX7zgo+GxscDVq0BqKv/MfvoJ+Pln/pkVnBpQLObTGC5axIPt6Gh+k3D6NL/5adeOfxNCCCGElELch2yZYFsiMDQOcR+qXmoJBdxVgbe37KukATfAg+ukJD4lna8vH51u1qz4/bS0+MjnqlU87zchgQdyYjEf/f6Y8zZLRpcVjWCnpAjrldakSfz9+vWyHbtGDR68jhvHU0h+/51Pv7dkCbBwIa9jbi57jBo1+DcMBZmb82A4PV34rYCHB785OHGCB/V6enz/+fOB6dP5zc/+/fn1fXz4jcSyZfw8LS356HyvXsChQzwPXdI3QgghpIRSMnOK3J5azPbPEQXcVYG8UWJ5c0QXp3dvPtr6ww88RWHcuNL1R0uLj446OQHZ2cKgtKLVrs1HhEND5W+XlCvKs1aWqSl/T0sr+7GrV+c57VFR/HqFhfHPQBI0t2qVX7d+ff5e+FsACUl5RgZ/T03l179hQ/nzZru68veCCxSdOSPcVpCDA2BkJLugESGEEKIkfS31IrdXK2b754gCbpJPVZUvhBIVxYPm4cPL1p68mTcqmrY2T2V59oyP6hbEGHDhAl+Mp2AQWxqShxUL3tiU57Hz8vios5oaXzxHol07fpyXL3nqT2H//SfsV/b/fy0XJ/+rO8TG8veCs/FI9pFsKygriwfxhWbvIYQQQpRlqqcBJ3tTuduc7E1hqle18rcBCrhJYbNm8Vkrzp1TPIoqsX8/f2iv0FL3AHhahL8/DxjbtStbn0JC+EtZEyfy9wULhH3bsoUHqv/7n3BWjpwc3n5YmOxx09Pl9+eHH/i/C08HWZpjS0ajJcRinsv97BlP+bC2zt+mp8fzx9PSgMIz8+zezQPuTp3yZ4oxMeGj4q9f81H0gpKSeO44IBzN7tiRvy9fLswFB/jzAbm58ke/CSGEECUY6Ghg5aBmMkG3k70pVg1qVuUemAQAEWPyoiVS0VJSUmBgYIDk5GToS2av+NhcXICAAP5QXHHLdE+ezINGf3++H8BnE/ntN54S4eQE1KzJR0eDg4Hz53nguHJlfnAK8HzinTuLXtq9XTugZ8/8nyVzcCv7qyoW8/QYyfLqzs78wcEjR/jI7+3bwuXVIyL4jBy2tvzfEl5efEYSJye+TVeXz1Jy+jQPlBcs4EFpWY795g2f97p7d96H7Gy+b0gIf4j18GHZ0eT4eKBDB94XZ2c+Yh4aynO0DQ2Ba9eE82qfOQP0788D5S5d+AOWiYn8QdfYWD6CfuhQfv2UFL4UfEgI73PPnvwm4fp14M4d3v9bt3gKDSGEEFJKyenZiPuQjdTMHFTTUoepnsYnF2yXW7xWLsvwkBL7pFaajI4uvq68lSZfv2Zswwa+NHzdunwVRQ0Nvlz44MGMXbok245kpcmiXt99J9xHUl4SmZmMeXkxVqcO75OlJWPjxzNWaOVRxhhj4eG8/cIrSl65wtiQIYzZ2zOmr8+YmhpvZ8AAxs6dK59jp6QwNmoUX4VSS4uvINm+PWPbtsku6V5QfDxfZt3GhjF1dcYsLHg7YWHy69+5wz8TKyt+Hnp6jLVuzT+/3FzZ+klJjC1YwJeW19Lix6hVi7HJkxmLjFTcL0IIIaQKKa94jUa4K8knMcJNCCGEEEIUKq94jXK4CSGEEEIIqUAUcBNCCCGEEFKBKOAmhBBCCCGkAlHATQghhBBCSAWigJsQQgghhJAKRAE3IYQQQgghFYgCbkIIIYQQQioQBdxfMhcXvoqj5KWiwlcq7NiRryopFsvuc+VKfv1WrRS3feZMfj3JypQFPX0KuLvzlQw1NQEDA6BuXeDrr/nqlQWnh4+IEPZT3svOrgwXooC7d/lKkYaGfGXJdu2Av/8uWRt2dsX39+pV4T4bNgBjxgDNmgFqarzOlSuKj/H773wlSjs73k9DQ8DBga+OmZAgf583b4BJk/iKoBoafMn4MWOAyEjZulFRwPr1fAVMSX1LS74q5e3b8tv38ir6nAuu4kkIIYR8QdQquwPkEzB7NqCnB+TlAa9e8WXIJ08GHjzggbc8amrA/fvAv//yILGwHTt4ndxc2W0XLgB9+/JtXbsCAwcCWlpAWBhfav7oUWDqVL5/QXXqACNHyu+PoWGJTlkuf3+gRw/el2HDgGrV+NLqQ4fyoHT2bOXa+f57IClJtjwuDvjjD8DICGjdWrhtxgz+bmXFl06PiSn6GDt28HdnZx4IZ2byQNjbG/Dx4UuwW1rm1w8L48vBv3/Pg+ihQ/ly8Dt38qXqb9zg11diwwZg1Spe1r0771NoKHDsGH/99RdvQx7JjVRh5fEZEUIIIZ+jcln3kpTYJ720e2goX6ZdJJJdKtzfn+/Tpw9jKiqyy7AzxlhsLF/SvH9/XtfZWbi9Th3GVFUZu3xZdl+xmLGzZ/m7hGTp9R49SnyKSsvJ4f3S1GTs4cP88qQkxurV4+cTEVG2Y6xZw89j+nTZbSdP5n8Okybxev7+itvKyJBfvmgR33fOHGF5nz68/LffhOV//y3/2h4+zJe2LywwkC/zbmTEl7AvyNOz+H4TQgghn5HyitcopYTIqluXj5wyxke55alRA+jWDdi7F8jOFm7bs4eXjR0ru9/793y0tUkTwNVVdrtIxEeZRaKyn0dJXL7M+zViBNC8eX65gQGwcCE/n507y3YMyaj0uHGy2/r0EY5IF0dLS3754MH8/cWL/LLMTODcOcDCApg+XbZ+8+Z8+8uX+eVff81/Bwr76iv+uSUmAk+eKN9fQggh5AtGATcpWuG0joLGjuVpEidOCMt9fIDGjYG2bWX3MTDgbUZHA2lp5dvXgiT5xF5eytWX5Et37y67rUcP/h4QUPr+3LgBBAfzvHcHh9K3U5xTp/h7kyb5ZfHxPH3H1lb+jUytWvzd31+5Y6ir83dFvxuBgTwd5ZdfePrJhw/KtUsIIYRUUZTDTWS9eMGDS3V1oE0bxfXc3AATEx5gDxrEy+7e5SOfa9fK30dTE+jfn+eJt28PTJjAc4ubNuUP5hXXL0UBdLt2QM+exZ2ZYqGh/N3eXnabpSXPcZfUKQ3J6Pb48aVvQ56tW4G3b4HUVP5txJUrgKMjMGtWfh0jI0BVlefnMyYbdIeH8/fnz4s/3uvXwMWLPNe8aVP5dTw9hT8bGvIHYUePVvasCCGEkCqFAm4CrFmT/9Dk69f8QcG0NB40W1sr3k9DA/jf//iDgG/f8ro+PjxQHzWKtyfP1q1ATg4fGZc8LKihwUd/hw7lQbi2tux+YWH8oUB5vvtOGHBPm8YffDQ1Ve4aJCfzdwMD+dv19fPrlNSHD3ymEx0dYPjw0rWhyNat/OFVie7dgd27eZAtoaMDODnxEew//+QPpEocOQI8esT/Le9Bz4JycvjnmpXFR7BVVYXbHRz45+/iwgPymBjg5Engp58ADw8eePfvX+pTJYQQQj5XFHBXBfJGfb//XvlZIeSNRm/YwIPW4owdy6eo27kTmDkT2L+fz0BS1EwbJibAP//wEeOzZ/mMGrdu8bSLGzeAbdv4CLuxsXC/Hj14fWWYmiofbFe0Awd40O3uzgP38nTvHn+PiwNu3gTmzwdatOAzjxScPebXX4FOnfhneuIE3/biBXD8OP/3v//yaSEVEYt50BwYyG+IRo2SrTNwoPBnOzt+vIYNeb7/okUUcBNCCPkiUcBdFcgb9ZWMKCojOpqnTWRk8Knlxo3jwbO9fX7+siIODjzA8/Xl8zUnJcl/WFIee3thCsejR3zav6dP+Tn99pty7ZQHyci2olHslBThqHFJVFQ6SUGmpkC/fvwBSHt7HhQXnC/bwYGn+3h68pFuf3/+cOyWLfwzmzsXMDeX37ZYzD/Tv/7in8/mzSXrW5cufHrBJ0/4dSzvmw5CCCHkE0cPTVYFfDI24as0C8Foa/N0gFOneJ7v2LFAenrx+40bx0erf/iBp5X06lXyYwM8WNywgf/78uXStVFaksBfXp52TAwfoZaX312c//7jI88NGvAR5opmY8NHlO/elf3sGjTgo+3v3/O0kKAgfhPw9CnfLm8hI7GYL46zcydPh/HzK3okXBHJtw3K/D4RQgghVQwF3ERWgwY8z/ftW77aYHFGjODT1EVF8QfjCuf2loSeXun3LQvJFHjnz8tuO3dOWKckipoKsKJER/MbJmU+h9RUnmJiYsLTPgqSBNu7dvHc+t27S/fZpqXx4F5X99NJ8yGEEEI+Igq4iXzz5/MR7zVreBpAUQwNeVB69ChPRSlKWhqwbBnPOS4sN5dPJQeUfTQ4Lg4ICZF/HHm6dAFq1+ZpE5KHCAGeYrJ8OX+os/AsG9HR/BiK0lBycniQqq5evjN0REfzm5vCGOP5/O/e8fPR1MzflpEhu+pnVha/EUhI4A82FpzbW5JGsmsXn6t7z56ig+3UVPmznGRk8PSW1FRgyJCip5kkhBBCqij6vx+Rz8ICmDIFWLeOP3BXeKq3wpyclGs3J4c/POflxacFdHDgOb3v3vGg/c0bPi+0vOMVNS0gwG8SJEHjxo08D9zTU7m5uNXUgO3bec66k5NwafdXr/iNR+E0nQULeKqFry/PmS/sn3+A2Fi+iIyi/GiJlSt58A7wFBRJmZ8f/7ebG38BwLNnfDS6XTue5mJhwW8srl7l26yt+cwxBd2/z/vRrRtPO0lJ4alDr1/zgLjwgjhLlvBz09MD6tUDli6V7bObW/4iQfHx/JuR1q15SoulJf9ML17kn2nTpvk3U4QQQsgXhgJuoti8efwBuV9/5dP3lfahwYL09fkMGufOAdeuAQcP8mBNR4cHdhMm8Cn+5E3PV9S0gACfmUXRCozKcHXlffL05LnOOTk8UFy1iqdUlFRJHpY8e1Z2YR1JKgvAg31JwN2gAZ9nOyCAT7uXmMi/jbC3BxYv5teh8AwvNWvy/PyrV3kgrKPDH3Zdty5/DvWCIiL4+4cP/BsJeezs8gNuY2Pg22/5jDOnT+f3qWFD/rszbZr8qR4JIYSQL4CIMcYquxNfopSUFBgYGCA5ORn6NGsDIYQQQsgnp7ziNcrhJoQQQgghpAJRwE0IIYQQQkgFooCbEEIIIYSQCkQBNyGEEEIIIRWIAm5CCCGEEEIqEAXchBBCCCGEVCAKuAn5mLy8+LLrV65Udk8IIYQQ8pFQwP0lc3HhwZ/kpaLCl2nv2BHYsoUv700+X+fOAc7OfMVMfX2+sM+lSyVro+Dvh6JXZKRwn8REYM4coG5dvry8mRnwzTdAUJDi4/z1F/+909MDdHX5ipWSVTYLCwvjNy79+wPVq/M+FF4FtLDMTODnn4FGjfjiSEZGQK9ewPXrsnUjIoo/56KWuSeEEEIKoZUmCTB7Ng908vL4MuZHjgCTJwMPHvDAm3x+9uwBRo3iwa5k2fkDB/jS7n//zQNgZXh6yi9/8QLYu5cHsDY2+eXx8UD79kBoKH8fMACIjgYOHwbOnAEuXwbathW2NXs2X/HS0hL43/8AdXW+WuWYMcDTp8CaNcL6V6/yFUdVVflKljExRZ9DZibQpQtw4wbQrBkwZQqQlMT75OzM3wcMyK9vaKj4vO/dA06dAnr0KPqYhBBCSEGMVIrk5GQGgCUnJ1deJ5ydGQMYi44WloeGMqary5hIxFhYWKV0rcry9OTX3N+/4o6RkMCYoSFjpqaMRUbml0dG8jJTU8ZSUsp2jGnT+HmsXSssnzqVl8+aJSy/cYMxVVXGGjViLC8vv/zuXV6/bl3G4uPzyz98YKx1a77txg1hW2FhjN28yVh6Ov9ZU5MxW1vFff3lF97O4MGM5ebml794wZi+PmNmZspfj759eVuHDytXnxBCyGetvOI1SikhsurW5SN/jPFRbmU9eMBHTmvWzE8laN0aWLZMtu61a/wYurqAiQkwdChPTZCkuRTk4cHLIiJk25GXE52dDWzYwEchbWx4X8zNga+/Bh4+lG3Dz4+34ecHnDjBUxuqVROmKWRn81HYFi14n6tVA776CvjnH/nXIjISGD4cMDbm3x44OwOBgYqvXXk6eJCP4E6fDtSokV9eowYwbRoQFwccPVr69jMz+ei2hgYfRS/o+HGemuTtLSxv3x7o1w/47z8gIEBYHwBmzuTXSkJXF/jxR/7vzZuFbdWuDbRrB2hrK9dfyTG8vISpIHXqAGPHArGxwKFDxbfz9i0fpTc35+dCCCGEKIkCblI0NSWzjh49Ajp04AFJp07ArFk8+NbRAbZuFda9dAno3Bm4fZvXmTgRCA/ngW5iYtn7nJAAfP89kJUF9O7NgzkXF56m0KEDcPeu/P0OHuRBubk58O23PMcX4O306MFTHxgDxo0DRo7k6TcDBgAbNwrbiY7mAeb+/UCbNsCMGTyY7NYNuHVL/rElNxrl8TClpI3u3WW3SVIhCga9JXXkCP+c+vfnN1UFxcQApqb8JqOwWrX4++XLwvoFtxVXvzTK6xh+fjztavRonvZCCCGEKKlK5HD7+flhzJgxcrdFR0fD0tJS4b5isRi7du3CkSNH8PDhQyQkJKBWrVoYNmwY5syZAy0tLUF9UeHR1/+3YsUKzJ8/v/Qn8Sl58YIHZOrqPGBUxu7dPDA9dkyYDwvwvF4JsZgH2Lm5fMS3UydezhgPYv/6q+z9NzICXr/mD9QVFBTER0YXLgQuXJDd7+xZ/qBh167C8iVLeBC7eDEfuZX8DqSm8huH2bN5oG5tzcsXLACiooClS/NHaQF+4zFpUtnPrzihofzd3l52m6RMUqc0duzg7+PHy24zNQXevwc+fJANusPD+fvz58L6BbfJq//mDZCezm/eSsPUlP9Oh4fznPPi+iQPY4CPD/+3vPMmhBBSJsnp2Yj7kI2UzBzoa6vDVFcDBjoald2tclMlAm6JJUuWoFahUSxDQ8Mi90lPT8eYMWPQrl07TJ48Gebm5rh58yY8PT1x6dIlXL58WSbI7tatG0aPHi0oc3R0LJdzqBRr1uQ/NPn6NX+ILC0NWLs2P4hUlryv+U1M8v997Rrw8iX/Sl4SbAM8iF2+nD/Yl5dXuvOQ0NSUDbYBoHFjPlPHuXNATo7sKOWAAbLBtlgMbNrE0w8KBtsATyv56Sc+0nvkCE/XyM7m52BuzgPxgsaP59daXrC7axcPKmvWLN05F5SczN8NDGS36esL65RUeDjg78/72a2b7PZevQBfX36tfvklv/z2beDkSf7vpCRh/ZUrgfXrgREj+AOLAL8WK1YIz6m0AXevXvybhSVLeCqMJK0kPJz3tXCf5AkI4LOjdOoE1K9fun4QQgiR621SBn44/C+uhsZJy5zsTbFyUDNYGyqZPviJq1IBd69evdCqVasS7aOhoYHr16+jQ4cO0rIJEybAzs5OGnR3LRSE1atXDyNHjiyXPpcLLy/Zsu+/zw9eirN2rWzZhg08gCxo/XrZwMTDg+c6DxnCtw8cyPOxu3UDnJxkA9/Hj/n7V1/JHtPWludcy8vVLqlHj4DVq3mAHxPDA+yC4uIAKythmbzR/GfPePqEtbVsXjLA838BICQkv35mJh/5LvTtCFRUeNqMvIC7JIF2RITslHmGhvwzr2g+Pny0d8wYfj6FLVnCvylYswa4eZN/oxAdzXOkGzUC/v1XuJ+TE88D372bb+/fP3+WktxcftOQnCz/WMqaOZPfBB04wD+nzp3zZymxs5PtkzySUf1x40rfD0IIITKS07Nlgm0ACAyNw/zD/2LDcMcqMdJdpQJuAEhNTYWOjg5UlZwnV0NDQxBsSwwcOBCenp4IDg6WCbgBICMjAyKRSCblpFLICwQ9PJQPuKOj+ZRsGRl8JHLcOB6k2NsLpz9bv57nLRfk4sKDlrZtedrF8uU8LUQycti6NbBqFR9ZBvJHVs3N5ffFwqLsAfeNGzyoAnges709H8EXiXjKy+PHPP1F3rELS0jg70FBRc8jnZbG35U5v7KKiJD9zG1t8wNuych2crLw2wUASEkR1ikJsZgH+ioq/GFDeWrU4Dnynp48n//OHX4TtWQJ/z0ZNkz22vj5Aa1a8aDWz49/S9KjB79hatyYP0dQ8IHKkqpWjc+3vWQJf1h040beh8mTgb59edCv6PMC+HU8fJh/OzBkSOn7QQghREbch2yZYFsiMDQOcR+yq0TAXaUemnR1dYW+vj50dHTQv39/hJYhTzXm/x+0MpXkmBbg5+cHXV1daGtro1GjRvhLibzjrKwspKSkCF7lhk9UJnwVtxCIPNraPIA+dYoHp2PH8q/2JSIiZI/j4pK//auveJCVmMjTDmbNAp48Afr04WkkQH6g9/69/D68eydbJhl9zM2V3SYvNWLZMh5QX7zIZxFZu5YHqF5e/MZCEXn5+ZIUjEGD5F9nyUtyg1Ga8yspFxfZ4xe8SSkqT7uo/O7inD3L86m7dSt6RL56dWD7dp7Hnp3NUzF++AEIDubbC38LpaLCHyx9/Jh/O5CYyB84FYt5LnizZmV/SNHQkM8yEx7O+/TmDb8RDAuT36eC9u7lN6PDh5c+rYUQQohcKZk5RW5PLWb756JKBNw6Ojrw8PDAH3/8gaNHj2LevHm4dOkSOnTogMjCq+ApafXq1dDX10cvyUwV/69Dhw5YtmwZjh07hk2bNkFVVRX/+9//sGnTpiLbW7FiBQwMDKQvm4KLhXxqGjQApk7l06CtX1/y/SWB+9q1/AHFjIz8hxQdHPj71auy+716JbtqIcAfggR4AFeYvGn+wsL4iGjBHHGA3zyUZJpDgC+soq/PFzwpnJYiT716PJXk3j0ePBYkFvPR94rm7Mzfz5+X3XbunLBOSRT1sGRx8vJ4EK2mxm9elLF3L38fNqzkx1OWMscoy3kTQggpkr5W0QMq1YrZ/tkop3nBy01eXh7LyMhQ6iUWixW2c/XqVSYSidikSZNK3Idly5YxAOzPP/8stm5WVhZr0qQJMzQ0ZOmShTjkyMzMZMnJydJXZGTkp7vwDWOMxcQwpq3NmJERY8r08cYNxjIyZMslC6H4+fGf8/IYq1WLL6pz9Wp+PbGYsREj8sdsC9q3j5d5eAjLDx7Mr19wIZnu3Xn7T5/ml+XmMjZlSn798PD8bb6+vMzXV/65/fAD3z5jBmPZ2bLbnzxh7N27/J9Hj+b1ly4V1tuyRX5/GWPs1SvGgoMZS0uT34eSSEhgzMCgZAvfFHf89+8ZU1fni8RkZSk+dnZ2/oI0Enl5jH3/PT/vmTNl95H3+xUYyBdfsrUtflGa4ha+UXSMdet4nwYOVLzfw4e8TrNmRbdPCCGkVJLSstio7beY7Q8nZV6jtt9iSWlF/D/nIyivhW8+uRzuwMBAuEryfYsRHByMBg0ayN3WqVMntG3bFhcvXizR8Q8cOIBFixZh3LhxmDJlSrH1NTQ0MG3aNEyePBn3799Hp8Kjqv9PU1MTmpqaJepLpbKw4Etgr1sH/Pqr4qWuJVat4mkkTk58bmMtLT6afOkSX6hk4EBeT0WFT4/XuzefEWToUP5A4uXLPJe8WTP+EFtBAwbwWUL8/PgIuKMjT0+4fJm3c/q0sP706Xx0t1MnnnOrpcXzy6Oi+Mh7See69vbm5/L77zzdRpLzGxXFU2YeP+YPCErygFeu5Oe9aBF/aFPS39OneU65vJHn0aP5TBj+/sI0ndIwMuJ5yqNG8YV6hg7l5QcO8CkaDxzgec0lOf6uXXyEf9QovuCNIu/e8bzr7t3570F2Nh9VDwnhqUUFZx6R+OYb/i1Is2b824QnT3hqkrExz7kv3Ne4OGDOnPyfc3J4mWQJe4A/tFkwHax6df4cgb19/nzn9+/n544rQqPbhBBSoQx0NLByUDPMP/wvAgvNUrJqULMqkb8NfIIPTTZo0AC+knzYYlgVnmWiEBsbGzx79kzpY1+4cAGjR49Gnz59sLnw6nbFHAcAEiQP2FUV8+bxVf5+/ZXn2EpSO+SZMoXnL9++zQM3xnie78KF/AFMSS40wANtSUB68CBPQenShf+70HSLAPj2ixd5O5cu8Sne2rXj83ifPCkbcPfty2fFWL4c2LOH59127swfmFuypOTXQVOTB4A7dvDA8/BhniNuYcFn1pg8GWjaNL++lRVPHZk3jwebgYFAy5Y8rebyZfkBd3kbOZIHnMuX8/xykYj3YdEi2akPlaFs4GlgwG+Qrl/nn426OtCkCbBtG38mQN5sIG5u/GZKkittY8NvmhYskP+Q6YcPwM6dwrK0NGGZl5cw4B45kt9MXLrEr0W9enzawunT+ecrj2RFTS0tvj8hhJAKYW2ojQ3DHRH3IRupmTmopqUOU72qNQ+3iDHGKrsTFaVVq1ZITU1VKui+ffs2unTpAgcHB1y8eBHayi4bDWDjxo2YPn06bty4gfbt2yu1T0pKCgwMDJCcnAz9gsHol87FJT9gJ4QQQgipROUVr1WJhyZjJXMhF3D69Gncv38fPXv2FJSHhYUhTDIzwf8LDg5Gnz59YGdnh5MnTyoMtuUdJzU1FevXr4epqSlatmxZhrMghBBCCCFV0SeXUlIaHTp0gKOjI1q1agUDAwM8ePAAPj4+sLGxwcKFCwV1u3TpAgCI+P9p1FJTU9GjRw8kJiZi7ty5OHXqlKB+nTp1pKPWf/zxB44dO4Z+/fqhZs2aiI6Oho+PD16/fo3du3dDo6jcVkIIIYQQ8kWqEgH30KFDcerUKZw/fx7p6emwsrLChAkT4OnpCYtiFhqJj4+XTh04f/58me3u7u7SgLtjx464ceMGtm/fjvj4eOjq6qJNmzbw8fFBZ8lCK4QQQgghhBRQpXO4P2WUw00IIYQQ8mmjHG5CCCGEEEI+AxRwE0IIIYQQUoEo4CaEEEIIIaQCUcD9JXNx4YuAiER8kRJF2rbNr1fUKo1LlvA66upATIzieh4e+e0pevn5le6cCsrK4n2yt+eLl1hbAxMnAu/fl6wdOzvF/VS0KuTbt8B33/GFcXR1+QIunToBu3cDeXmy9Yu6FgVXUJTw8ip6n/+fhadIU6bk11f0eZ09yxfKMTTkCxA1bcpXH5V3DoQQQgiRq0rMUkLKSE0N8PHhKzQWFhQE3LnD6+TmKm6DsfwVDXNz+ap/P/xQ9HHHjQNq1JC/rXlzpbsvl1jMVzw8d46vSjloEBAaCmzfnr9apZmZ8u0ZGADffy9bbmcnW/byJb9JiY8HevQA+vUDUlL4MuWjR/PVJuWtpmprKz+4LupauLvL74OhoeJ9AL7q5ebN/GYgLU1+nd9/5zcN+vrA11/zNi9eBGbP5kvZHzxY9DEIIYQQwjFSKZKTkxkAlpycXHmdcHZmDGCsXz/G1NUZe/9ets7MmYypqDDWpw+v6+8vv60LF/j2iRMZ09dnrF49xcd1d+d1b94sh5NQwMeHH2P4cMbE4vzyTZvy+6ksW1v+UtaUKfwY69cLyxMTGatZk2+LiBBuA/jnoSxPz6I/j6IkJTFWowZj33yT/zsQHS2sExXFmKYmY0ZGwr7m5DA2YADfZ9++kh+bEEII+YyUV7xGKSUEGDsWyMnh6Q4F5eQAe/YA3bsrHomW2LGDv0+cCAweDDx/Dly9WjH9Vca2bfx9xQo+6i4xaRJQuzawdy+QkVExx375kr/37i0sNzTkaSUAEBdXMcdWxnff8XP/4w/Fdc6c4Sk548fzkXcJNTXA25v/e9Omiu0nIYQQUkVQwE14ykWjRrJpDidOALGxPCAvSkICcPQob6NlS542AeQH4eVBkmusjMxM4PZtoH59YbAoaadbN55Gce+e8sfPyuJ55cuXAxs38vYVadKEv58+LSxPSgKuXwcsLfm1KiwpCdi6lR9j82bgyZPi+xUYCKxaBfzyC09Z+fCh6PonTvB0nw0bAHNzxfUkOd21asluk5TduMGvCyGEEEKKRDnchBs7FpgzB7h7F2jdmpft2AGYmPBc6EuXFO+7dy8PvEaN4j9/9RXPKz54kOcBK5oofvt2/lCePPPn8wcdSyMsjOdw29vL3y4pDw3lfVVGTAwwZoywrHVrYN8+oE4dYfncuTywnTmTn1+zZvk53Do6/OZEW1v2GI8f8xH4gnr25AGyouDY01P4s6Eh8Ntv+Tc9BcXHAxMmAG5uwPDhRZwsAFNT/h4eLrtNUpaby0fzGzYsui1CCCHkC0cj3IQbNYrPLuLjw39++5Y/cDhyJKChUfS+O3YAKiq8LsBHkUeOBNLTgf37i97P21v+KzNTWDc4mL+UkZzM3w0M5G+X3ABI6hVnzBh+w/HuHR8Zf/iQX6+7d4EuXYDUVGF9Cwv+UGHPnjzgXr2aj1gnJ/NA2MFB9hizZ/MR47g4HpzfuAH06sX379tXdlYQBwf+Wb18ydNDwsP5qLVkVpN//pE9xrffAtnZyqWCdO8OqKryzygyMr88Nzc/pQTgo/KEEEIIKRKNcFcFXl6yZd9/X/xMFQWZmwN9+vAA+ddf+ahqXl7x6ST37vGR2S5dhHneo0cDS5fygG3iRPn73rzJ01mU0aCBcvUqQuFR5ObNgV27+L937+b54rNm5W9/8YLPTKKnx/PYmzfngemePcCiRfxG5upVHtBKrFkjPEb79nyqxs6dgYAA4PhxPlOIxMCBwvp2dsC0aXy0uVs3fpz+/fO3HzgA/P0377elZfHnXKsWsHAh8PPPfCrAr7/mNzCXLgGvXwM1a/J3FbpnJ4QQQopD/7esCuSNEJdm5HHsWL7f4cM8n7tlS54OURRJnnbhFAZ7ex5M37nDpxb8mCQj24pGsFNShPVKS5L+cf26sNzDA3j1iqeVdOrEA+8aNXiazPTp/EajqJF/CRUVngIi7xiKdOnCU1yePMk/z4QEYOpUfkMlSftRxpIl/Iaifn3e3x07+Hlcv57/LUFReeCEEEIIAUABd9XAJ2kTvuTNzVyc3r0BKys+f3ZoKJ8nuygZGTyHGeDzQRdefOXWLb6tPB+eVEbt2jxYDQ2Vv11SrijHW1mSPOeC81inpvKAtGFD+SPJrq78/eHD0h9D2X3S0/n769c8f/vUKdnPKCCA17Gy4j8/eiRsa+RI/oBoejoP4E+fBurW5dfQxET+Q5WEEEIIEaCUEpJPVZWPVK9axR9YLO7BukOH+Chy8+Z8NFyevXv5KOnKlcXngpcXbW2gTRse8L96JZyphDG+6IuuLtCqVdmOI5mppODNTXY2f1c07V9sLH/X1Cz9MYqSlsa/UdDVzQ+8TUwU3zydOsUfCB0xgl83E5Pij3HoEH9ItrgbMkIIIYQAoICbFDZrFk8FMTYuPgdcMnK9bl3+yG1h6el8FPyff4Bvvil9v0JC+LuyudwTJ/KAe8ECHvRLphTcsoU/aDhxonCmkJwcPruJurpw1pGQEJ6vrKMj2x/JSpojRuSXm5jwFIxnz/gsLOPH529LSsrP1S54vZ484eelri48xo0b/OZHXZ3PbS6RmgpERwP16gnrZ2TwFJTUVP6gp9r//3nb2PC+yOPiwgPutWtlR+RTUmRnmAkO5g946uvzFBlCCCGEFIsCbiJkbs6njSvOixd8Dmg7Ox60KTJmDA+4d+yQDbiLmhawXTs+y4eEZOo5xorvG8BTXA4c4McODwecnXmfjxzhaRBLlwrrR0XxY9jaAhER+eX79/MbCicnvk1Xly/qc/o0D9IXLODbCvr1V/7A4oQJfH9HRyAxkd90xMbyZea7ds2vv3YtH2nu1IkHx+rqfJT6/Hl+o/DHH8KbgPh4HqC3bp2fuvLuHV92/c0b/pDjL78od52KMns28OABP46xMU8jOXGCp+scO8b7SgghhJBiUcBNSsfHhwe/ktxtRbp04YHZ+fN8ermCQVpRud3ffScMuEtKRYXP7LFyJU9p+fVXHjSOG8eDbTMz5dpxdeWjug8f8plF0tN5qkbv3nyave7dZffp1YuPTv/yC3DtGs+T1tLiwfFPPwFTpgjrDxjAR78fP+bpLtnZPIgeNozPNtOmjbC+sTE/9p07PPBPTOSj9Q0bAjNm8NlK5M3zXVLdu/OR/IMH+ai5pSXwv//x2UsKzz1OCCGEEIVEjCk7ZEjKU0pKCgwMDJCcnAx9RQvDEEIIIYSQSlNe8RrNUkIIIYQQQkgFooCbEEIIIYSQCkQBNyGEEEIIIRWIAm5CCCGEEEIqEAXchBBCCCGEVCAKuAkhhBBCCKlAFHATQgghhBBSgSjgJoQQQgghpAJRwE0IIYQQQkgFooCbfDJc/Fwg8i5imfgy8nvkB5G3CH6P/CrsGIQQQgghhVHATQAA/uH+GHpoKGx+tYHmUk0YrzJGJ59O+PXmr8jMzSyXY3hd8YLIW4QrEVfKpT1CCCGEkM+BWmV3gFSuXHEupp6aiq0PtkJXXRe97HuhrlFdJGcl43zYecw6Pwub72/GqRGnUNe4boX2ZdfAXUjPSa+w9gc2GIh2NdrBSs+qwo5BCCGEEFIYBdxfuAUXF2Drg61obd0aR4ceRXX96tJteeI8LAlYgiWBS9BzT088mPQA+pr6FdaXmgY1K6xtADDQMoCBlkGFHoMQQgghpDBKKfmCPY9/jnW31sFY2xgnhp8QBNsAoKqiCm9Xb4xoOgJhiWFYc2ONYLvdejvYrbdDUmYSJp2YBMs1ltBaqgXHLY7Y92SfoK6Lnwu8A7wBAK47XSHyFkHkLYLdejtBncI53AXzrk88O4G229tCZ5kOqq+rjsWXF0PMxACAnY92wmGzA7SXaaPmrzXxy/VfZM5XXg63xzEPaV/kvVz8XARtZOdlY93NdWixpQV0l+ui2opq+Mr3K/zz7B+Z40nafpn4EmtvrEWjPxpBc6kmPI55yP08ihORFAGRtwgexzwQHBuMgQcGwmS1CUTeIkQkRZTrtcrMzcTaG2vhsNkBBisNoLtcF3br7TDk4BA8jnks95oeDzmONtvaQGeZDsx+McPY42Px7sM7mbYl1zUqJQqjj46G5RpLqHirCFKNfB/6ou32ttBbrge95Xpou72t3Nz7KxFXIPIWweuKF669vgYXPxdUW1ENhisNMejvQXiR8KJU15oQQggpTzTC/QXb+WgnxEyMiS0mwkLPQmG9xU6L8deTv+Dz0AdLXJcItmXnZaPrrq74kP0Bo5qNQlpOGv4O+hsjjoxAXHocpredDgDwaO4BAAh4FQB3B3fYGdoBAAy1DJXq69GQozgfdh5uDdzQ0aYjToWewtKrS8HAYKBpgKVXl2JA/QFwsXXB4eDDmHdxHiz0LDDaYXSR7bo1cJP2paCbb27ifNh56KjrSMuycrPQc29PXIm4guaWzTHOcRxy8nJwKvQUBuwfgA29NmBam2kybU0/Mx233txCH/s+6FevH8x1zQHwYHXM8TFwd3CHn5ufUtcBAF4kvEC7He3Q1LwpPBw8EJ8RDw1VjXK9Vu7H3PF30N9oZtEMY5qPgaaqJiJTIuEf4Y+7b+/CwdJB0KfDwYdx7sU5fNPoG3St3RW33tyC7yNfXH19FXfG34GRtpGgfnxGPNrvaA9jbWMMazIMmbmZ0m9PZpyZgQ13NqB6teoY5zhO2v6Y42PwMPohfuv1m8w1ufXmFlZcW4GedXtiepvpCIoNwtHgo7j66ipujb+F2ka1lb6+hBBCSLljpFIkJyczACw5ObnS+uDi58LgBXYh7EKxda3XWjN4gb1Oei0ts/3VlsELzMnXiWXlZknLI5MjmelqU6b5syZ7k/xGWu7p78ngBeYf7i/3GM6+zgxewl9J34e+DF5g6kvU2Z03d6TlKZkpzPwXc6azTIdZrrFkYQlh0m2vk14zjZ81WNM/m8pty/ehb5HnGhIbwgxXGjLjVcbsedxzafnCiwsZvMAWX17MxGKxoC+ttrZiGj9rsKiUKGm5+1F3Bi+wGutqsFdJr2SOI+mP+1H3IvsjEZ4YzuAFBi+wny7/pLC9sl6rpIwkJvISsZZbWrLcvFzBMXLzclliRqLMMeEFdjb0rKDu/AvzGbzApp2aJiiX1B9zbIxM+wERAQxeYA03NmRJGUnS8oT0BFZvQz0GL7DAiEBpuX+4v7S9zXc3C9rafHczgxdY37/6ylwrQgghRBnlFa9RSskXLOZDDADARt+m2LqSOtEfomW2Le+8XDDCWkO/Br5r+x2y8rKw/+n+cunryGYj0bp6a+nP1TSroa99X6TnpGNKqymCEUwbAxt0qtkJ/8X+h1xxbomOE5cehz5/9UF6TjqODj0KexN7AICYibHp3ibUMaoDbxdviET5qS/VNKvhJ6efkJ2XjSPBR2TanNthrtz89IENBiJ4ajBWdFlRoj5a6lniR6cfFW4v67USiURgYNBS04KKSPifCFUVVbnfSnSt3RU96vYQlP3o9CMMtQyx699d0nQWCQ1VDazuthqqKqqC8p2PdgIAvFy8BPn2RtpG8HT2BAC5qSX1TOphQssJgrIJLSfA3tgep56fQmxarMw+hBBCyMdCKSWkTNRU1NDepr1M+Vc1vwIAPIx5WC7HaW7ZXKbMqpqV4m16VshjeXj34Z1MbroiWblZGHhgIMISw+A3wA9Otk7Sbc/iniExMxHW1ayluegFSQK6kLgQmW1tqreRe7zSPsTpYOEguMEprKzXSl9TH73te+N06Gm02NoCgxsNhoudC1pbt4a6qrrcY0o+74L0NPTQ3LI5rkRcwcvEl4JZbmoZ1oKpjqnMPpLfFxc7F5ltrnauAIBH7x7JbOto01Hm5kBFpIKONTsiNCEUj989RtfaXeX2nRBCCKloFHB/wSz1LBESF4LIlEjUN61fZN3IlEgAkJlSz1THVCbQASDNCU/OSi6XvsqbHUVNRa3YbTniHKWPMe6fcbj2+hoWdloI9+bugm0JGQkAgKDYIAQFBClsIy0nTabMQldxfnxpFJVvD5TPtTo4+CCWX12Ov578hR8v/yjdd0zzMVjeZbkgtx1QfI6S8uRM4e+BonNIyUqBikgFZjpmsm3pWUAEEVKyUhQeR9njE0IIIR8TBdxfsA41OuBKxBVcenmpyNG/kLgQvE19i+rVqsPGQJh+EpceBzETywTdktkpDDQ/j2n4vK94Y++TvRjcaDCWdl4qs10SqA5qOAiHhhwqUdsF00/KgwgVtxqnhI66DpZ2XoqlnZciPDEc/hH+2HxvM367/RsycjKwpd8WQf13abKzkRQsLzySr+gc9DX1IWZixKbHSh8ulXif9h4MTO5NQ0mPTwghhHxMlMP9BRvtMBoqIhVse7CtyBzXZVeXAQDGOo6V2ZYrzsXNyJsy5VdfXwUAOFo6SstURTxfN0+cV6Z+l7d9T/bBK8ALbaq3wU63nXID5IZmDaGvqY97b+8hJ0/5UfOqoJZRLYx1HIsAjwDoaejhn+eyUyBKPu+CPmR/wKOYR9DX1Fd6lhDJ74u81UglZc0tmstsux55XSZPXMzEuBF5AyKI4GDhILMPIYQQ8rFQwP0Fq29aH9+1/Q7xGfHot68folOFD0SKmRg/B/yMPf/uQR2jOpjTYY7cdhZeXojsvGzpz29S3uC3279BU1UTw5oMk5YbaxsDyE9P+RTciLyBMcfHoKZBTfwz7B9oq2vLraemooYprabgVfIrzDk/R27Q/fT9U7xPe6/0sZMzkxESFyJz3StbbFosnr5/KlOemJmIrNwsaKlpyWy7+PIizr04JyhbFrgMSZlJGN1stNy0I3kkqTzeAd6C1JHkzGRp7nzhdB+Azym/7f42Qdm2+9vwPP45+tTrAzNd2RQVQggh5GOhlJIv3Opuq5GcmQyfRz6w32CPPvX6oI5RHaRkpeB82HmEJoTC3tgep/93Wu5X+VZ6VkjLTkOzTc3Qr14/6Tzc8Rnx+L3n74IHFl1ruUIEERZeWoig90Ew0DKAoZah3LmrP5bx/4xHVl4W2lRvg033NslstzO0k84h7u3ijQfRD/D7nd9xKvQUnGydYK5rjqjUKDx59wSP3z3GzXE3ZVIhFDkacrRU83BXtKjUKDhucYSDhQOaWTRD9WrVEZ8Rj+PPjiNHnIM57WVvvPrW64t++/rhm0bfwM7QDrfe3IJ/hD/qGNWRmbu9KE62TpjeZjo23NmAJn82waCGg8DAcDj4MN6kvMGMNjMED7NK9KjTAzPOzsDpF6fR2KwxgmKDcOLZCZjqmOK3nrLzdhNCCCEfEwXcXzg1FTXsGLADw5sOx9b7W3Ht9TUcDT4KXQ1dNDRtiMmtJmNKqykKR341VDVwYdQFzL84H7v/3Y2kzCQ0MG2ADb02YHjT4YK6jcwawXeAL9beXIsNdzYgKy8Ltga2lRpwp+ekAwAO/XcIhyCbm+1s6ywNuDXVNHHmf2ew4+EO7Hq8C4eDDyMrNwsWehZoZNYIk1tNRlPzph+z+xXCztAOXs5euBxxGRdfXkR8RjxMdUzRwqoFvmv7HXrW7Smzz6CGgzDecTyWXV2GYyHHoKOuA4/mHljRZYXMojfF+b3X73C0dMSme5uw9cFWAEBjs8ZY4rIEYxzHyN2nXY12WOS0CIsuL8Lvt3+Hqooq3Bq4YXW31bToDSGEkEonYoyxyu5EWfn5+WHMGPn/I46OjoalpWWR+3t4eGDnzp0y5fXr10dIiHCaN7FYjDVr1mDTpk2Ijo5GvXr1sGDBAgwfPlxm/6KkpKTAwMAAycnJ0NeXHTn+HEiWZY/4PqJS+0Eqj2S1TN8BvtIbk4/pSsQVuO50haezJ7xcvD768QkhhFRt5RWvVakR7iVLlqBWrVqCMkNDQ6X21dTUxPbt2wVlBgayMxv8+OOPWLlyJSZMmIDWrVvj+PHjGDFiBEQiEYYNGyZTnxBCCCGEfNmqVMDdq1cvtGrVqlT7qqmpYeTIkUXWiYqKwtq1azF16lRs3LgRADB+/Hg4Oztj7ty5GDx4MFRVVYtsgxBCCCGEfFmq3CwlqampyMsr3bRzeXl5SEmRXVRD4vjx48jJycG3334rLROJRJgyZQrevHmDmzdlp8cjhBBCCCFftioVcLu6ukJfXx86Ojro378/QkNDld43PT0d+vr6MDAwgLGxMaZOnYoPHz4I6jx8+BC6urpo2LChoLxNmzbS7V+SiO8jKH/7C+fR3APMk1VK/jbAl4BnnozytwkhhHzSqkRKiY6ODjw8PKQB9/3797Fu3Tp06NABDx48gI2NTZH7W1lZYd68eWjRogXEYjHOnj2LP//8E48fP8aVK1egpsYvU3R0NCwsLGQWRrGy4sudv337VuExsrKykJWVJf25qJF0QgghhBBSdXxyAbdYLEZ2dnbxFcEfdBSJRBgyZAiGDBkiLXdzc0OPHj3g5OSEZcuWYfPmzUW2s2LFCsHPw4YNQ7169fDjjz/i0KFD0ochMzIyoKmpKbO/lpaWdHtRx/D29lbqvAghhBBCSNXxyaWUBAYGQltbW6nXs2fPFLbTqVMntG3bFhcvXixVP2bOnAkVFRXB/tra2oJRaonMzEzpdkUWLFiA5ORk6Ssy8tNZbfFzkpKVgu/OfIdav9WC+s/qEHmL8CjmUWV3q0T8HvlB5C2C3yO/yu7KRxWRFAGRtwgexzwquyuEEELIR/XJjXA3aNAAvr6+StWVpHIoYmNjU2RQXhRtbW2YmJggISFBcDx/f38wxgRpJdHRfGlua2trhe1pamrKHR3/lIw9Pha+j3xhrG2Mt7PeQlOtbP31uuIF7wBv+Lv7w8XOpVz6OO/CPGy5vwV96/XFyKYjoaqiCku9oudZ/9gikiJQ67dan9wKkoQQQgipHJ9cwG1paQkPD49yaevly5cwMzMr1b6pqamIi4sT7N+8eXNs374dwcHBaNSokbT89u3b0u2fq9SsVPwd9DdEECEhIwHHQo5haJOhld0tGSefn0Q9k3o4MfxEZXel1AY2GIh2NdrBSq/oG0ZCCCGEVA2fXEpJacTGxsqUnT59Gvfv30fPnsJlqMPCwhAWFib9OTMzE6mpqTL7//zzz2CMCfYfMGAA1NXV8eeff0rLGGPYvHkzqlevjg4dOpTH6VSKA0EHkJaThpntZkJFpIIdD3dUdpfkepv69rMPVA20DNDAtAEMtGQXViKEEEJI1VMlAu4OHTpgyJAhWL16NbZs2YJJkyZhwIABsLGxwcKFCwV1u3Tpgi5dukh/jomJQc2aNfHtt9/i999/x++//44+ffrgl19+Qc+ePTFgwABp3Ro1auD777/HH3/8gUmTJmH79u3o168frl69itWrV3/Wi97seLgDaipqmNdxHlztXHEp/BJeJb1SWD/wVSDc9rvBYo0FNJdqwuZXG3x94Gtce30NAODi5wLvAP6QqOtOV4i8RRB5i6TLwZeUxzEPiLxFYGAIeBUgbc/FzwUAT18ReYtwJeKKzL7ycqYL5hO/SHiBgQcGwmiVEXSX66Lrrq54HPNYbj/ep73H7HOzUX9jfWgv04bxKmO03d4Wa26skR6r1m98tdOdj3dK+1mwb0XlcF9/fR19/uoD41XG0FqqhQYbG8DT3xPpOekydSXn/+7DO7gfc4fpalNoL9NGu+3t5F6H0pAcIyolCqOPjoblGkuoeKsI2g98FYh++/rBdLUpNJdqwn6DPRZdXiS3z/LYrbdT+Hvh4ucCkbdI7jZCCCHkc/HJpZSUxtChQ3Hq1CmcP38e6enpsLKywoQJE+Dp6QkLC4si9zU0NETfvn1x4cIF7Ny5E3l5eahbty6WL1+OOXPmQEVFeE+ycuVKGBkZYcuWLfDz84O9vT327NmDESNGVOQpVqj/Yv/DrTe30Nu+Nyz0LDDaYTQuhV+C7yNfufMb/3brN8w8NxPa6toY2GAgahrURFRqFK69voZD/x1Cp5qdpPMyB7wKgLuDO+wM7QAAhlqGpeqjWwM32BnawTvAG7YGttL2Je2WVkRSBNptb4fG5o0xtvlYhCWG4fiz43Dd6YrgqcGw0Mv//XkW9wyuO10R/SEanWp2glt9N6TlpCEoNgjLry7HnA5z0NyyOb5r+x1+u/0bHCwc4NbATbp/cX09GHQQww8Ph6aaJoY2HgpzXXOcDzuPJYFLcC7sHK54XIGWmpZgn6TMJHTy7QQDTQOMajYK79Pf48DTA+ixpwfuT7yPJuZNpHWvRFyB605XONs644rHFaWvUXxGPNrvaA9jbWMMazIMmbmZ0NfUBwBsursJU09PhaGWIfrV7wdzHXPci76HZVeXwT/CH/7u/tBQ1VD6WIQQQkiVxEilSE5OZgBYcnJyZXeFzTo7i8ELbN+TfYwxxlKzUpnuMl1W89eaLE+cJ6j7KPoRU/FWYdZrrVl4Yrhgm1gsZlEpUdKfPf09GbzA/MP9y62v8AJz9nWWKS/qWL4PfRm8wHwf+krLwhPDGbzA4AW28upKQf1FlxYxeIGtuLpCUN5qaysGL7Ct97bKHCMyOVKmbfej7nLPQV5/kjOTmcEKA6b5syZ7HPNYWp4nzmNDDw5l8AJbcmWJoB1J/789+a3gc9p+fzuDF9ikE5ME9f3D/RVeP0UkxxhzbAzLzcsVbAt6H8TUlqgxh00OLC4tTrBtxdUVDF5ga66vkZYpui62v9oy219t5R7f2deZwYv+M0UIIaRylFe8ViVSSkjp5eTlYPe/u6GvqS8djdXT0MPAhgPxOvk1Lr4UTqu45f4WiJkYS12XyozYikQiWFdTPFPLp6iWYS3M7ThXUDauxTgAwN23d6Vld6Lu4N7be3CydcKElhNk2qmhX6NM/TgechzJWckY6zgWzSyaSctVRCpY3W011FTU4PfYT2Y/XXVdrOq2Ciqi/D9l9+buUFNRE/QfANpUb4PgqcHYNXBXifqmoaqB1d1WQ1VFmDK15d4W5IpzsaHXBpjomAi2zes4D2Y6Ztj3dF+JjkUIIYRURVUipYSU3vFnxxGbHotxjuME6Qqjm43Gnn/3YMfDHehep7u0/E7UHQAQlH3Omls2FwSrQH7wnJSZJC2TnnftijnvhzEPAUDu9Ik1DWqitlFtPI9/jtSsVFTTrCbdVs+kHvQ09AT11VTUYKFrIeg/AOio66CBaYMS962WYS2Y6pjKlN+KugUAOBd2DpfCL8lsV1dVR0hcSImPRwghhFQ1FHB/4SSzkYx2GC0o71K7C6pXq47jIceRkJEAY21jAEByVjJEEMGq2uc9U4iEJBe5IDUV/meRJ86TliVnJgMAqutXr5B+pGSlAAAsdOU/c2ClZ4Xn8c+RkpUiCLjl9R/g51Cw/2VRMI+9oIQMPkf9sqvLyuU4hBBCSFVFAfcXLDI5EufDzgMAnP2cFdbb8+8ezGg7AwB/6JGBITo1usKCz9KQjFLninNltkmC5bKQPOwZlRJV5rbkkQTO79Leyd0e8yFGUO9jEkH+LCGSvqTMF94ElJSKSAXZedlytyVnlf2zI4QQQiob5XB/wfwe+UHMxOhUsxPGOY6Tebk7uAOAYE7uNtZtAEAaqBdFVcRzfstrpLUoRlpGAOQHxJJ0jbJoU/3/z/tlCc6bKX/ejpaOACB3Or/I5EiEJYahtlHtMgW25a1t9bYAgFtvbpWpHSNtI7xPey9zs5SWnYbQ+NAytU0IIYR8Cijg/kIxxuD7yBciiLDTbSe2998u8/Jz80P7Gu3x77t/ce/tPQDA5FaToSpSxSL/RTLzdDPG8Db1rfRnSRpKZEqk3D7EpcchJC4EcelxZT6f1tVbAwB2/bsLYiaWlt+MvIm9T/aWS/utrVsj8FUgtt3fJrO9YKBvpG0EEUSITJZ/3vIMaDAABpoG8H3ki6D3QdJyxhh+uPgDcsW58HDwKNM5pOekIyQuBK+TX5epHYlvW38LNRU1TD8zXW6bSZlJeBhd/M1Oa+vWyBHnYO+/+Z8TYwwLLi1AWk5aufSVEEIIqUyUUvKFuhx+GeFJ4XC2dUZto9oK641pPgY339zEjgc70Mq6FZpaNMX6nusx48wMNP6zMdwauMHWwBYxH2IQ+DoQfez7YH3P9QAA11quEEGEhZcWIuh9EAy0DGCoZYhpbaYBADbe2QjvAG94OnvKne+7JNrVaIeONh1xOfwy2u9oD6eaTniV/ArHnx1Hv3r9cDTkaJnaB4C9X++Fy04XTDw5Ebv/3Y32NdojMzcTQbFBeBjzEPHz4gHwWV5aV+fB+aijo2BvbA8VkQpGNRsFW0NbuW3ra+pjW79tGH54ONpub4uhjYfCTNcMF19exP3o+2hTvY3MbColdSfqTqnm4VakiXkT/Nn7T0w5NQX1N9ZHb/veqGNUB6lZqXiZ9BIBEQHwaO6BzX03F9nOtDbT4PvIF+NPjMeFlxdgpmOGq6+vIikzCQ4WDnj8Tv4iRIQQQsjnggLuL5QkTUSygIwiQ5sMxXdnv8O+p/uwrsc6aKtrY1qbaWhi3gRrb67FmRdn8CH7A8x1zdG2elsMaTxEum8js0bwHeCLtTfXYsOdDcjKy4Ktga004C5vx4cdx6zzs3Dy+Uk8efcEDpYOODH8BN6mvi2XgNvexB4PJj7AimsrcOL5Cay/vR56GnqwN7bHoq8WCeruHrgbM8/NxMnnJ5GcmQwGhk41OykMuAFgcOPBsNSzxIprK3Ak5AjSc9JhZ2iHxU6L8UPHH2QWvfkUTGg5Ac0tm2PdrXUIfBWIE89OwEDLADUNamJmu5lwb+5ebBtNzJvg7P/OYsGlBTj03yHoaeiht31vrOm+BkMODil2f0IIIeRTJ2KMscruxJcoJSUFBgYGSE5Ohr7+x38QjhBCCCGEFK284jXK4SaEEEIIIaQCUcBNCCGEEEJIBaKAmxBCCCGEkApEATchhBBCCCEViAJuQgghhBBCKhAF3IQQQgghhFQgCrhJleTi5wKRt6iyu0EIIaSKSU7PRtj7D3j4OhFhsR+QnJ5d2V0inwEKuL9gEUkREHmL0HNPz8ruCikhryteEHmLcCXiSqX243n8cww5OASmq02hvUwbDpsdsOnuJpR0en8xE2PD7Q1ouqkptJdpw+wXMww/PBwvE18q3Ofci3Nw9nNGtRXVoL9CH647XXHp5SW5dU88O4Hpp6ejo09H6C7XhchbBK8rXiXqIyGEvE3KwLR9D9FlXQAG/nkDXdYGYPq+h3iblFHZXSOfOAq4SZW0a+AuBE8NruxuVGn/xf6HNtva4Piz4+hl3wsz2sxAnjgP357+FjPOzChRW5NOTMKMszPAGMOMNjPQs25PHAk+gtbbWiM0PlSm/p5/96Dn3p4Ijg2Gh4MH3B3cEfQ+CN12d8Oh/w7J1F97cy023t2Ip++fwrqadanPmRDy5UpOz8YPh//F1dA4QXlgaBzmH/6XRrpJkWhpd1Il1TSoWdldqPKmnJqC5KxknB5xGr3sewEAfu78M7ru6oqNdzdiRNMRaG/Tvth2/MP9sf3hdjjZOuHCqAvQUNUAAIxoMgK9/+qNaWem4dzIc9L6iRmJmH5mOkx1TPFg0gPU0K8BAPih0w9w3OKIKaemoEedHqimWU26z8+uP8NSzxJ1jeviQNABDD88vDwvBSHkCxD3IVsm2JYIDI1D3IdsGOhofORekc8FjXATpaVmpcLT3xON/2wM7WXaMFxpiB57euDa62syde+/vY9pp6ehyZ9NYLDSANrLtNF0U1OsvLYSOXk5MvXt1tvBbr0dkjKTMO30NNj8agO1JWrwe+QnTX3xOOaBFwkvMPDAQBitMoLucl103dUVj2Mey7QnL4fb75EfRN4i+D3yw/mw8+iwowN0lunAZLUJ3I+5Iz49Xu55b7m3BY3/bAytpVqw+dUG8y7MQ2ZuJkTeIrj4uSh17QqmgPg98kOLLS2gs0xHun9yZjJWXVsFZz9nWK+1hsbPGrBea43RR0cjLCFM5ty8A7wBAK47XSHyFkHkLYLdejtBvfdp7zHz7EzU/b0uNJdqwnS1KQb9PQhP3z9Vqs9FeR7/HIGvAuFq5yoNtgFAQ1UDP7v+DADY9mCbUm1J6v3s+rM02AaAXva94GLngvNh5/E6+bW0/OB/B5GUmYTpbaZLg20AqKFfA9NaT0NcehyOhhwVHOMr269gb2IPkYjy+gkhpZOSKfv/roJSi9lOvmylHuEWi8VQURHG6zdv3sTJkyehpaWFMWPGoEaNGgr2Jp+bhIwEOPk6ISg2CB1tOmJyy8lIyUrB8WfH4brTFQcHH4RbAzdp/W0PtuHE8xNwsnVCb/veSM9Jx5WIK1hwaQHuvr2Lw0MOyxwjKy8LnXd2xofsD+hfrz/UVNRgoWsh3R6RFIF229uhsXljjG0+FmGJYdLjB08NhoWehUyb8vzz7B+cCj2FfvX6oYNNBwS+CsSux7sQlhCGa2OFNw8/+f+EnwN/hoWuBSa0mAB1VXX8HfQ3QuJCSnUdf7nxC/zD/TGgwQB0r9MdqiJVAEBwXDB+uvITXO1cMbDBQOhq6CIkLgR/PfkLp0JP4cHEB7A1tAUAeDT3AAAEvAqAu4M77AztAACGWobS44QlhMFlpwvepLxB9zrd4dbADe/T3uNw8GGce3EOl0ZfQtsabaX1/R75YczxMXB3cIefm1+x5yHJHe9ep7vMtk41O0FXXRcBrwKUuiZXIq5AV10XHW06ymzrUacHrkRcQUBEAEY5jCr22D3q9oBXgBcCIgIw2mG0UscnhBBl6GupF7m9WjHbyZetVAH3zJkzsWnTJsTExMDQ0BAAcOjQIQwbNgxisRgAsGHDBjx48ICC7ipi+pnpCIoNwrZ+2zC+xXhp+Yq0FWi1tRUmnpiInnV7QktNCwCw8KuF+KP3H1BVUZXWZYxh/D/j4fPIB9dfX0fHmsIAK+ZDDBwsHHB97HVoq2tLyyOSIgDwAHNll5X4odMP0m2LLy/G0qtL4fvIF/M7zVfqXE48P4Er7lekx88T56Hr7q64EnEFt97cQrsa7QDwUdzlV5ejerXqeDDpAcx1zQEA3i7eaLejnbKXTiAgIgC3x99GU4umgvKGpg0RPTsaxtrGgnL/cH903d0VSwOXYlt/PhLs0dwDEUkRCHgVAI/mHnCxc5E5zuhjoxGdGo2z/zuLHnV7SMsXOS1Cq62tMOHEBPw75d9SnQMAaV61vbG9zDZVFVXUMqqF/2L/Q644F2oqiv8zk5adhugP0Whi3kTwuyIhaT80IT+PW/JveceWV58QQsqDqZ4GnOxNESgnrcTJ3hSmepROQhQrVUqJv78/OnfuLA22AeCnn36CgYEBdu3ahdWrVyMxMRFr1qwpr36SShSXHocDTw+gc63OgmAbAMx1zTG3w1zEpsfi4suL0vKaBjVlAiiRSISpbaYCgKBuQau7rRYE2wXVMqyFuR3nCsrGtRgHALj79q7S5zOi6QhBsK+qogp3B3feTlR+O/ue7EMey8Ps9rOlwTYAVNOshkVfLVL6eAVNbDlRJtgGAAMtA5lgGwBca7misVljXAyXf73keRj9EDcib8DdwV0QbANAPZN6mNBiAp68fyJILRnYYCCCpwZjRZcVSh0jOStZ2m959DX1IWZipGalKteOpuJ2AJ5yI90nU/GxpfWzkmW2EUJIWRjoaGDloGZwsjcVlDvZm2LVoGaUv02KVKoR7sjISDg7O0t/Dg8PR0hICDw9PTFy5EgAwNWrV3H27Nny6SWpVHej7iKP5SErN0vuVGqS0cSQuBD0rdcXAJCdl42NdzZi/9P9CIkLwYfsD2DInyrubepbmXa01LTQ1Fw2GJVobtkcKiLhPaIkhzcpM0np82lp1VKmTF47j9/x3PBONTvJ1C88Oq+sNtXbKNx2JeIK1t9aj9tRtxGXHodcca50W8Hc5uLcenMLAPAu7Z3czysknqfDhMSFoIl5EwA8eFUUPBNCCOGsDbWxYbgj4j5kIzUzB9W01GGqp0HBNilWqQLutLQ06OrqSn8OCAiASCRCr175D081atQIly7JnxOXfF4SMhIAANcjr+N65HWF9dKy06T//ubvb3Di+QnUM6mHoY2HwlzXHOqq6kjKTMJvt39DVl6WzP7muuZFPtQmGb0sSJKukCfOU/p8imyH5beTkpUi7VdhBXPLS0LRfgeDDmLooaHQ09BDj7o9YGdgBx11HYhE/CHPV8mvlD6G5PM6FXoKp0JPKaxX8PMqKcmIdMGR54JSslIggkgwU0iR7SgYkZZ8BgVvBiT/Ts5MhomOifz6CkbMCSGkrAx0KMAmJVeqgNva2hrPnj2T/nz27Fno6emhZcv8kcOUlBRoamqWvYek0kkC1NntZ2NN9+LThO5G3cWJ5yfQo04PnBpxSpBacuvNLfx2+ze5+4nwac0gITnv92nvpQ8sSrxLe1eqNhXdUHgFeEFLTQv3J96HvYkwN3n/0/0lOoak3xt6bcC0NtNK1c/iSPooL1c6T5yH8MRw1DKqVWT+NgDoaujCSs8K4YnhyBPnyaQhycvXtje2x7239xCaECoTcBeV300IIYRUllLlcDs7O+PUqVPYuHEjtm/fjiNHjqBHjx5QVc3/n2VYWBg9MFlFtK7eGiKIcPPNTaXqhyXyaez62PeRCaCuvrpa7v2rKA4WDgAgd1T/RuSNcj1WWEIYGpo1lAm2o1Oj5a62KJndRN7IvmT2EWU/r9JwtuUpZefDzstsu/b6GtJy0qR1im3LzhlpOWlyr/O5MD7/tpOtk1LHPvfinLRNQggh5FNRqoD7xx9/hLa2Nr777jtMnDgRmpqa8PLykm5PTU1FYGAgOnYsXZ4r+bRY6lliSOMhuBF5A79c/0Xust2339xGek46AMDWgI8GX4sUTrEX9D4IK64p91Dep2BYk2FQEalg7c21iEvPfyo9LTsNy64uK9dj2Rra4kXCC7z7kD9ynpmbiSmnpiBHLDu3q+QBy8iUSJltbaq3QdvqbbHvyT4ceHpAZruYiREQIZyyLzkzGSFxIYhOjVaqv/VN68PJ1gn+Ef44E3pGWp6dl43F/osBQOYB27j0OITEhQiuJQBMbDERALDYfzGy8/JXajsTegZXIq6ge53ugm8YhjQeAgNNA2y4swFvUt5Iy9+kvMHGuxthqmOKgQ0GKnUehBBCyMdQqpSSunXr4r///sPhw3wu5X79+sHWNv9/iKGhoZg0aRJGjBhRPr0kFerJ+yfwOOYhd1sD0waY32k+/uzzJ57FP8O8i/Ow+9/daF+jPQy1DBGZEin9ej96djR01HXQpnobtKneBn8H/Y3o1Gi0q9EOr5Nf459n/6BPvT5yl97+FNU3rY/5Hedj+bXlaLqpKYY0GgI1FTUcCTmCpuZN8fT9U5mHOEtrepvpmH5mOhy3OOKbRt8gV5yLCy8vgDEGBwsH6QOcEq61XCGCCAsvLUTQ+yAYaBnAUMtQmkKyb9A+uO50xbDDw7D+9nq0sGwBbXVtvE5+jZtvbiI2LRaZizKl7R0NOVqiebgB4M/ef6KjT0e4HXDD0MZDYaVnhVOhpxAUG4Rpraehg00HQf2NdzbCO8Abns6e8HLxEpzLeMfx2P5wO1psaYE+9n0Q/SEaB4IOwFjbGBt6bRC0Y6RthI29N2LU0VFosaUFhjYeCgA4EHQA8enxOPDNAZnc8WMhx3As5BgAIDwpXFommXJS8ntOCCGEVIRSL3xjZWWFadPk54e2aNECLVq0KHWnyMf1NvUtdj7eKXebs60z5neaD2NtY9wYewMb72zEgaAD2PtkL8RMDEs9SzhYOmCx02KY6vCpklRVVHFy+EnMvzgfZ8PO4u7bu7A3tsea7mvQq26vzybgBoBlXZahhn4NbLizAZvvb4a5rjmGNR6G79p9hxPPT8h9ALM0praeCnUVdWy4swHbHmyDoZYh+tj3wYouKzD44GCZ+o3MGsF3gC/W3lyLDXc2ICsvC7YGttKAu5ZRLTyc9BDrbq7DsWfH4PvIF6oqqrDSs4KTrRO+afhNmfvc2Lwxbo+/jUX+i3Aq9BTSstNQz6Qe/uj9B6a0mlKitrb024KmFk2x9f5W/Hb7N+hp6GFgg4FY1nkZ6hjXkak/stlImOqYYvnV5fB95AuRSISWVi2xyGkRutbuKlP/Ucwjmd/xx+8eS29kJL/nhBBCSEUQMXn5ASXw33//ISQkBGlpaRg1alR59avKS0lJgYGBAZKTk6GvXz5BG/l4Lr68iG67u2Feh3lY1W1VZXeHEEIIIRWgvOK1Un8ffvfuXTRv3hxNmzbF4MGD4eHhId0WGBgIHR0d/PPPP6XuGCGfgti0WJkHE5Myk7Dg0gIAECxnTwghhBAiT6lSSoKCgtC5c2eoqKhg5syZCAkJwZkz+Q9OffXVVzA1NcXBgwfRv3//cussIR/b3id7sebGGnSu1RnW1awR/SEaZ1+cxfu09/Bo7oH2Nu0ru4uEEEII+cSVKuD29PQEANy/fx9169aFt7e3IOAWiURo37497t5VfrltQj5FHWw6oKV1S1x8eREJGQlQVVFFQ9OGWOy0GN+2/rayu0cIIYSQz0CpAu6AgAAMGjQIdevWVVinZs2atLQ7+ey1qd4Gx4cdr+xuEEIIIeQzVqoc7tTUVJibyy53XVBGRgby8pRfbpsQQgghhJCqqFQBt42NDZ48eVJknQcPHqBOHdnpvAghhBBCCPmSlCrg7tu3L86fP4+LFy/K3f7333/j1q1bcHNzK0vfCCGEEEII+eyVah7u2NhYtGjRAu/evYO7uztiYmJw+vRpbNiwATdv3sS+fftQs2ZNPHz4EAYGBhXR788ezcNNCCGEEPJpK694rdQL37x8+RKjRo3CzZs3Zba1bdsW+/btg52dXak7VtVRwE0IIYQQ8mkrr3it1Eu7165dG9evX8ejR49w69YtJCQkQF9fH23btkXr1q1L3SFCCCGEEEKqklIH3BLNmzdH8+bNy6ErhBBCCCGEVD2lXtqdEEIIIYQQUrxSjXCPHTtWqXoikQg7duwozSFKxM/PD2PGjJG7LTo6GpaWlkXuLxKJFG7r2rUrLly4AACIiIhArVq15Nbbt28fhg0bpmSPCSGEEELIl6JUAbefn1+R20UiERhjHy3glliyZIlMQGxoaFjsfrt375Ypu3fvHn777Td0795dZtvw4cPRu3dvQVn79u1L1llCCCGEEPJFKFXAHR4eLrc8OTkZDx48wLJly+Do6IjVq1eXqXMl1atXL7Rq1arE+40cOVKm7MqVKxCJRBg+fLjMthYtWsjdhxBCCCGEkMJKFXDb2toq3NasWTP06tULTZs2xalTpzB16tRSd640UlNToaOjA1VV1VK3kZWVhcOHD8PZ2Rk1atSQWyctLQ3q6urQ0NAo9XEIIYQQQkjVVyEPTVpYWKBfv37YuHFjRTSvkKurK/T19aGjo4P+/fsjNDS0VO2cPn0aSUlJ+N///id3u7e3N/T09KClpYXWrVvj/PnzxbaZlZWFlJQUwYsQQgghhFR9ZZ4WUJFq1aohIiKiopoX0NHRgYeHhzTgvn//PtatW4cOHTrgwYMHsLGxKVF7e/fuhaamJr755htBuYqKCrp3746BAweievXqePnyJdatW4devXrhn3/+QZ8+fRS2uWLFCnh7e5fq/AghhBBCyOer1CtNFiUpKQmOjo5gjJU46BaLxcjOzlaqrqampsIZRq5duwYnJydMnDgRmzdvVvr4KSkpsLCwQK9evXDkyJFi6yckJKBRo0YwNDRESEiIwnpZWVnIysoSHMfGxoZWmiSEEEII+URV6kqTS5YskVuem5uLqKgo/PPPP0hISICXl1eJ2w4MDISrq6tSdYODg9GgQQO52zp16oS2bdvi4sWLJTr+4cOHkZmZqTCdpDBjY2OMGTMGK1euxJs3bxTmfGtqakJTU7NEfSGEEEIIIZ+/UgXcxQXS1apVw4IFC7B48eISt92gQQP4+voqVdfKyqrI7TY2Nnj27FmJjr93714YGBigb9++Su8jSVlJSEhQGHATQgghhJAvU6kCbn9/f7nlKioqMDIyQv369aGurl6qDllaWsLDw6NU+xb28uVLmJmZKV0/Ojoa/v7+8PDwKNFo9MuXLwGgRMcihBBCCCFfhlIF3M7OzuXdjzKJjY2VCXZPnz6N+/fvY8aMGYLysLAwAECdOnVk2tm/fz/EYrHCdBJ5x4mKioKPjw+aNWtW7Ig7IYQQQgj58lTIQ5Mfm729PRwdHdGqVSsYGBjgwYMH8PHxgZWVFe7evQsLCwtpXTs7OwCQ+zBnq1atEB0djcjISKioyM6YOGbMGISFhaFLly6wtrZGREQEtmzZgtTUVJw7dw4uLi5K97m8kvAJIYQQQkjF+KgPTQYGBpb6AE5OTqXeV1lDhw7FqVOncP78eaSnp8PKygoTJkyAp6enINguyrNnz3D//n3MmjVLbrANAN27d8fmzZvxxx9/IDExEYaGhnBycsKiRYvQokWL8jwlQgghhBBSRSg1wq2ioqJw+r3i5OXllWq/qo5GuAkhhBBCPm0fdYT7p59+KnXATQghX7Lk9GzEfchGSmYO9LXVYaqrAQMdjcruFiGEkI+oSuRwf45ohJuQqu9tUgZ+OPwvrobGScuc7E2xclAzWBtqV2LPCCGEKKO84jX5ycqEEELKJDk9WybYBoDA0DjMP/wvktOVW1GXEELI548CbkIIqQBxH7Jlgm2JwNA4xH2ggJsQQr4UpZqHGwBSU1OxceNGXLx4EW/fvkVWVpZMHZFIJJ33mhBCviQpmTlFbk8tZjshhJCqo1QBd2xsLDp06ICwsDDo6+tL81uys7ORkZEBALC2ti71apOEEPK509cq+r9/1YrZTgghpOooVUqJl5cXwsLCsGvXLiQmJgIAZs6cibS0NNy+fRtt2rSBnZ0dgoKCyrWzhBDyuTDV04CTvancbU72pjDVo5lKCCHkS1GqgPv06dPo0qULRo4cKTNdYOvWrXHmzBlERETA29u7XDpJCCGfGwMdDawc1Ewm6HayN8WqQc1oakBCCPmClCqlJDo6GoMHD5b+rKqqKk0lAQAjIyP06tULf//9N1atWlX2XhJCyGfI2lAbG4Y7Iu5DNlIzc1BNSx2mejQPNyGEfGlKFXAbGBggJyf/gR8jIyO8efNGUEdfXx/v3r0rW+8IIeQzZ6BDATYhhHzpSpVSUrt2bUREREh/dnR0xIULFxAfHw8AyMjIwIkTJ1CzZs1y6SQhhBBCCCGfq1IF3N27d8elS5eQnp4OAJg0aRLev38PBwcHDB48GE2aNEFYWBg8PDzKs6+EEEIIIYR8dkoVcE+ePBnbtm2TBtxff/01fvnlF6SlpeHw4cOIiYnBrFmzMHfu3HLtLCGEEEIIIZ8bEWOMKVOxYcOGGDduHEaPHg1zc3O5dfLy8hAXFwdzc3OZ2UuIkGTu8uTkZOjr61d2dwghhBBCSCHlFa8pPcL97Nkz/PDDD7CxscHXX3+NU6dOQSwWC+qoqqrCwsKCgm1CCCGEEEL+n9IB9/PnzzFv3jyYmZnh2LFj6N+/P2xsbLBw4UKEhoZWZB8JIYQQQgj5bCmdUiIhFotx5syZ/2vvzuOjrg79/78nITvJYAhLAmFRo7gRMYKCGtYgW0UuVUlpgYhQcb16K2jFr+KlELTa4sIFrxAsIGKFgldpFRAIAmK1XNEKNAIRSMISyEr2zPn9wS9zGScJk8kMSSav5+MxjwdzPufz+Zw5mRze88mZ81FaWpo++ugjVVRUyGKx6I477tDUqVP185//XCEhId5qr89gSgkAAEDz5qm81uDAfaEzZ85oxYoVSktL07fffiuLxaLw8HAlJyfr/vvvV9++fd1umK8jcAMAADRvzSJwX+jrr7/WsmXL9N577ykvL08Wi0XXX3+9vvnmG08c3ucQuAEAAJq3Zhe4a2RlZWnChAnauXOnLBaLqqurPXl4n0HgBgAAaN4u+SolF/PJJ5/o3nvv1ZVXXmkP24MHD/bU4QEAAIAWqU1jdj5y5IiWLVumP/3pTzp+/LiMMerSpYumTJmi+++/Xz179vRUOwEAAIAWqcGBu6ysTH/+85+1bNky7dixQzabTQEBARo3bpymTp2qO++8U35+HrtwDgAAALRoLgfuPXv2aNmyZVqzZo2KiopkjFGvXr3sd5/s0KGDN9sJAAAAtEguB+7+/ftLktq2bauUlBRNnTrVXgYAAACgdi4H7ltvvVUPPPCA7rvvPoWFhXmzTQAAAIDPcDlw79q1y5vtAAAAAHwS324EAAAAvIjADQAAAHgRgRsAAADwIgI3AAAA4EUEbgAAAMCLCNwAAACAFxG4AQAAAC9yaR3uyy+/3K2DWywWHTp0yK19AQAAAF/gUuC22WyyWCwOZRUVFcrJyTl/kDZt1L59e505c0ZVVVWSpOjoaAUGBnq4uQAAAGhJCkoqlFtcocKySkWEBCgqLFDW0NaVEV0K3JmZmQ7P8/PzNWzYMMXFxel3v/ud+vfvLz8/P9lsNu3atUuzZ8/WuXPntHnzZm+0GQAAAC1Adn6pZq3dpx0ZufayxLgopY7vrZh2IU3YskvLrTncs2bNUllZmbZs2aLbbrtNfn7nD+Pn56fbb79dmzdvVklJiWbNmuXRxgIAAKBlKCipcArbkpSekaun1+5TQUlFE7Xs0nMrcG/YsEFjxoyRv79/rdvbtGmjMWPGaMOGDY1qHAAAAFqm3OIKp7BdIz0jV7nFBO56FRYWqqCgoN46BQUFF60DAAAA31RYVlnv9qKLbPclbgXu6667Tu+9916dK5BkZGTovffe0/XXX9+oxgEAAKBliggOqHd7+EW2+xKXvjT5U7Nnz9a4cePUp08fTZ06Vbfffrs6duyoU6dOaceOHVq2bJnOnTun2bNne7q9AAAAaAGi2gYqMS5K6bVMK0mMi1JU29azUolbV7jHjh2r5cuXy2KxaOHChbr33ns1aNAg3XvvvXrttdfk5+entLQ03XXXXZ5ub702b96sIUOGyGq1Kjw8XAkJCVqzZo1L++7fv18jRoxQ27ZtFRkZqV/96lc6ffq0Uz2bzaaXXnpJPXv2VHBwsHr37q3Vq1d7+qUAAAC0aNbQQKWO763EuCiH8sS4KC0Y37tVLQ1oMcYYd3cuKirS+vXr9c0336igoEBWq1Xx8fEaO3asIiIiPNnOi0pLS9PUqVOVlJSku+66S/7+/jp48KC6dOmi3/zmN/Xue/z4cfXp00dWq1WPPfaYiouL9fvf/17dunXTl19+6bCe+DPPPKPU1FRNmzZNffv21YYNG/Txxx9r9erVmjBhgsvtLSwslNVqVUFBwSXvKwAAgEulZh3uorJKhQcHKKpty1mH21N5rVGBu7nIzMzUtddeq2nTpmnhwoUN3v+hhx7S8uXLdeDAAXXr1k3S+avlSUlJWrJkiaZPny5JysrKUs+ePTV9+nS98cYbkiRjjAYOHKgjR44oMzOzzpVbforADQAA0Lx5Kq+5NaXkQsXFxfrHP/6hHTt2NPZQblu8eLGqq6v14osv2tvUkM8Ra9eu1ZgxY+xhW5KGDRumq666Su+//769bMOGDaqsrNRDDz1kL7NYLJoxY4aOHz+u3bt3e+DVAAAAwJe4HbgzMzM1duxYXXbZZerbt68GDx5s37Zz505de+212rZtmyfaeFGbN29Wr169tHHjRnXt2lXh4eFq3769nnvuOdlstnr3zcrK0qlTp3TzzTc7bevXr5/27t1rf753716FhYXpmmuucapXsx0AAAC4kFurlBw9elS33nqrzpw5o7Fjx+rEiRMOV3dvueUW5ebmavXq1Ro0aJCn2lqnjIwM+fv7KyUlRTNnzlR8fLzWrVunuXPnqqqqSvPnz69z35ycHElSdHS007bo6GidPXtW5eXlCgoKUk5Ojjp16iSLxeJUT5Kys7PrPE95ebnKy8vtzwsLCxv0GgEAANAyuXWF+/nnn1deXp62b9+uDz74QElJSQ7b27RpozvuuEM7d+5s8LFtNpvKyspcetRMGykuLlZeXp7mzJmjF198UePHj9eqVas0YsQILVy4UEVFRXWer7S0VJIUFBTktC04ONihTmlpqUv1ajN//nxZrVb7IzY21sUeAQAAQEvmVuD+5JNPNG7cOA0YMKDOOt27d1dWVlaDj52enq6QkBCXHgcPHpQkhYSESJKSk5MdjpWcnKzS0tJ6p3rU7Hvh1ecaZWVlDnVCQkJcqlebZ555xn73zYKCAh07dqzOugAAAPAdbk0pOXv2rHr06FFvHWNMreH0Ynr16qW0tDSX6tZM5YiJiVFGRoY6derksL1jx46SpLy8vIseo2ZqyYVycnIUGRlpv6odHR2trVu3yhjjMK2kZt+YmJg6zxMUFFTr1XEAAAD4NrcCd6dOnZSRkVFvnW+//dZh1Q9Xde7cWVOmTGnQPgkJCcrIyFBWVpYuv/xye3nNnOoOHTrUuW+XLl3UoUMHffXVV07bvvzyS91444325zfeeKPefvtt7d+/X9dee629fM+ePfbtAAAAwIXcmlKSlJSkjz76SPv27at1+44dO/TZZ59p1KhRjWqcq+677z5J0tKlS+1lNptNaWlpioyMVEJCgr380KFDOnTokMP+48eP10cffeQwzWPLli3617/+pXvuucdeNnbsWAUEBGjRokX2MmOMFi9erC5dutQ7xQYAAACtk1s3vsnMzLRfzX3qqad04MABvfvuu/roo4+0a9cuvfrqqwoLC9M333xT6+ofnmaMUVJSkj777DNNmzZN8fHxWr9+vTZt2uRw4xpJ9qkwmZmZ9rJjx46pT58+ateunR5//HEVFxfr5ZdfVteuXfX3v//dYSrIzJkz9fLLL2v69Onq27ev1q9fr48//lirVq3SL37xC5fbzI1vAAAAmrcmv9Pknj17NGHCBP3444+yWCz2ec3GGHXr1k0ffPBBrWtbe0txcbFmz56tNWvW6OzZs7r66qs1a9YsTZw40aFebYFbkv75z3/qySef1Oeff67AwECNHj1ar7zyitO8cJvNpgULFmjJkiXKyclRXFycnnnmGafzXAyBGwAAoHlr8sAtSVVVVfqf//kf7dmzR2fPnlVERIRuueUWjR07VoGBgW43qjUgcANA81RQUqHc4goVllUqIiRAUWGBsobyfxrQGjWLwA33EbgBoPnJzi/VrLX7tCMj116WGBel1PG9FdOu7qVfAfgmT+U1t740OWTIEP3pT3+qt87KlSs1ZMgQtxoFAMClVlBS4RS2JSk9I1dPr92ngpKKJmoZgJbOrcC9bds2pznQP/Xjjz9q+/bt7hweAIBLLre4wils10jPyFVuMYEbgHvcCtyuOHfunAICArx1eAAAPKqwrLLe7UUX2Q4AdXH5xjdHjx51eJ6fn+9UJknV1dU6duyY1q5de9G7UQIA0FxEBNd/kSj8ItsBoC4uB+4ePXrYb2dusVi0cOFCLVy4sM76xhi9/PLLjW8hAACXQFTbQCXGRSm9lmkliXFRimrLSiUA3ONy4J40aZJ9ne0//elPio+Pr/VW5v7+/oqMjNSQIUM0YsQIT7YVAACvsYYGKnV8bz29dp9D6E6Mi9KC8b1ZGhCA29xaFrBnz5564okn9Nhjj3mjTa0CywICQPNUsw53UVmlwoMDFNWWdbiB1spTec3lK9wXOnLkiNsnBACgObOGErABeJZbq5R8//33eu2113T69Olat586dUqvvfaa9u/f36jGAQAAAC2dW1NKJk2apC1btujYsWPy83PO7NXV1erRo4eGDRumtLQ0jzTU1zClBAAAoHlr0iklO3bs0NChQ2sN29L5L04OHTpU6enpbjcMAAAAzV/N9x4KyyoVERKgqDCmZf2UW4H7xIkTio2NrbdOly5dlJOT41ajAAAA0Pxl55fq/234Tr2iI9Qntp1yCsp0KjRA3SJD1eWy0KZuXrPhVuAOCwvTqVOn6q1z6tQpBQcHu9UoAAAANG8FJRX6fxu+04R+3ZS284je+OwH+7bbr2yv1H/rra6RhG7JzS9N3nTTTVq/fr3y8/Nr3Z6Xl6e//OUvuummmxrTNgAAADRTucUV6hUdobSdR7TzhzMO2z7/4Yye+cu3KiipaKLWNS9uBe6HH35YZ86c0eDBg53maW/fvl2DBw9WXl6eHnnkEY80EgAAAM1LYVml+sS2cwrbNXZk5Cq3mMAtuTmlZOzYsXriiSf0hz/8QYMHD1ZQUJA6d+6sEydOqLy8XMYYPfXUU7r77rs93FwAAAA0BxHBAcopKKu3TlFZ5SVqTfPm1hVuSXrllVf04Ycf6s4771RYWJiOHz+utm3bauTIkfr444+1YMECT7YTAAAAzUhU20C1Cwmot054cP3bWwu3rnDXGDNmjMaMGeOptgAAAKCFsIYGqnv7UN1+ZXt9Xsu0ksS4KEW1ZXlAqRFXuAEAANC6dbksVKn/1lt3xEU5lCfGRWnB+N6sx/3/a9QVbgAAALRuXSND9UZyH+UWV6iorFLhwQGKasvNby7kUuD28/OTn5+fvv/+e1111VXy8/OTxWK56H4Wi0VVVVWNbiQAAACaL2soAbs+LgXuxMREWSwWhYaGOjwHAMCTuEU0AF9kMcaYpm5Ea1RYWCir1aqCggJFREQ0dXMAoMll55dq1tp92pGRay9LjItS6vjeimkX0oQt8ww+TAAtj6fyGnO4AQBNrqCkwilsS1J6Rq6eXrtPryf3adHh1Nc/TACoH6uUAACaXG5xhVPYrpHewu9Wd7EPE9z6GvB9Ll3hvv/++906uMVi0dKlS93aFwDQehRe5G50Lfluda58mGjJV+8BXJxLgXv58uW1llssFtU2BbymnMANAHBFxEXuRteS71bnyx8mALjGpcB95MgRh+c2m02PP/64vvjiCz3++OO644471KlTJ508eVLp6el67bXX1L9/f/3hD3/wSqMBAL4lqm2gEuOilF7LleCWfrc6X/4wAcA1LgXu7t27OzxPTU3Vnj179M033yg6OtpefvXVVysxMVEpKSnq06ePPvjgA82cOdOzLQYA+BxraKBSx/fW02v3OYRuX7hbnS9/mADgGreWBYyLi9OIESP0+uuv11nnkUce0SeffKKMjIxGNdBXsSwgADirWTrP1+5Wl51fWueHiWhWKQGarSZdFvD48eMKDg6ut05wcLCOHz/uVqMAAK2Tr96tLqZdiF7n1tdAq+X2FW5jjL777rtag3dJSYluuOEG+fn5cYW7DlzhBgAAaN48ldfcWof7gQce0OHDh3Xbbbdpw4YNOnPmjCTpzJkzWr9+vW6//XZlZmZq2rRpbjcMAAAA8AVuXeG22WyaNm2a0tLSZLFYJEl+fn6y2WySJGOMUlJS9Pbbb9u3wxFXuAEA3sSt5IHG81Recytw19i+fbveeecd7du3TwUFBbJarYqPj9evfvUrDRo0yO1GtQYEbgCAt3ArecAzmkXghvsI3AAAbygoqdAjq/fWenfLxLgovZ7chyvdgIuadA43AABonly5lTyAS8vtwF1VVaU//OEP6tevnyIiItSmzf+tMPi///u/euihh/Svf/3LI40EAACu4VbyQPPj1jrcpaWlGj58uHbt2qWoqChFRETo3Llz9u09e/ZUWlqaIiMjNXfuXI81FgAA1I9byQPNj1tXuOfNm6edO3dq/vz5OnHihB544AGH7VarVQMHDtQnn3zikUYCAADX1NxKvjbcSh5oGm4F7jVr1mjw4MGaOXOmLBZLrUv/XX755Tp69GijGwgAAFxnDQ1U6vjeTqG75lbyfGESuPTcmlJy9OhRjRs3rt464eHhKigocKtRAADAfdxKHo3FOu6e5dYV7vDwcJ06dareOocOHVKHDh3capS7Nm/erCFDhshqtSo8PFwJCQlas2ZNvfvYbDYtX75cd911l2JjYxUWFqbrr79ec+fOVVlZmVP9miv6P32kpqZ662U1WkFJhQ6dKtbeo3k6dLpYBSV8Qx2A72Gsc2QNDdQVHdvqxm6X6YqObQlLcFl2fqkeWb1XQ1/drnGLdmnoK9v16Oq9ys4vbeqmtVhuXeG+9dZb9T//8z/Kz89Xu3btnLYfO3ZMGzduvOhVcE9KS0vT1KlTlZSUpHnz5snf318HDx7UsWPH6t2vpKREKSkpuvXWW/Xggw+qY8eO2r17t55//nlt2bJFn332mdOUmaSkJE2aNMmhrE+fPh5/TZ7AzQ8AtAaMdYBnFJRUOP0uSeeXlHx67T7WcXeTW4H7qaee0uDBgzV06FC99tprqqqqknQ+vO7evVuPPvqoqqqq9OSTT3q0sXXJzMzUww8/rEcffVQLFy5s0L6BgYHauXOnBgwYYC+bNm2aevToYQ/dw4YNc9jnqquu0i9/+UuPtN2b+KUB0Bow1gGe48o67vw+NZxbU0oSExP1xhtv6Ntvv1ViYqLmzZsn6fxUk+HDh+uHH37QokWLlJCQ4NHG1mXx4sWqrq7Wiy++KEkqLi6WqzfQDAwMdAjbNWquzu/fv7/W/UpLS2udctKccPMDAK0BYx3gOazj7h1u3/hmxowZ+uabb/TII4+ob9++uuKKK9SnTx89+OCD2rt3r9NSgd60efNm9erVSxs3blTXrl0VHh6u9u3b67nnnpPNZnPrmCdOnJAkRUU5L620fPlyhYWFKSQkRNdee63efffdix6vvLxchYWFDg9v45cGQGvAWAd4Duu4e4dbU0rS09MVERGhG2+8scFTOLwhIyND/v7+SklJ0cyZMxUfH69169Zp7ty5qqqq0vz58xt8zJdeekkREREaOXKkQ/mAAQN07733qmfPnsrOztabb76piRMnqqCgQDNmzKjzePPnz9ecOXMa3I7G4JcGQGvAWAd4Ts067um1/NWIddzdZzGuzr24gL+/v379619r0aJFHm+QzWZTRYVrf/4LCgqSxWKRv7+/bDabUlNTNWvWLPv2kSNHavv27Tp58qTCw8NdbsO8efP07LPPatGiRfWGaEmqqKhQQkKCjh8/ruzsbIWE1P7lnPLycpWXl9ufFxYWKjY2VgUFBYqIiHC5bQ1RUFKhR1fvrfOXhnmNAHwBYx3gWdn5pXp67T6H36maddyjW9mXkAsLC2W1Whud19yaUtKxY0cFBwe7fdL6pKenKyQkxKXHwYMHJckecpOTkx2OlZycrNLSUu3du9fl869Zs0azZ8/W1KlTLxq2pfNzwB955BHl5+fr66+/rrNeUFCQIiIiHB7exs0PALQGjHWAZ9Ws477lyYFa/9AAbXlyoF5P7tPqwrYnuTWlJCkpSdu2bZMxpta7TDZGr169lJaW5lLd6OhoSVJMTIwyMjLUqVMnh+0dO3aUJOXl5bl0vE2bNmnSpEkaPXq0Fi9e7HKbY2NjJUlnz551eZ9LhZsfAGgNGOsAz7KG8vvjSW4F7tTUVPXv31/Tp0/XggULFBkZ6bEGde7cWVOmTGnQPgkJCcrIyFBWVpYuv/xye3l2drYkuXQDnj179mjcuHG6+eab9f7776tNG9e75vDhwy6fpynwSwOgNWCsA9BcuTWHe8iQITpz5oy+++47BQYGqmfPnurUqZPT1W6LxaItW7Z4rLF1Wb9+vcaNG6ff/va3+t3vfifp/FzwgQMH6vvvv1d2draCgoIknb8DpiRdccUV9v3379+vO+64Q507d9aOHTt02WWX1Xqe06dPO4XqoqIi9enTRwUFBcrKylJgoGuDvafmBAEAAMA7PJXX3LrCvW3bNvu/y8vLdeDAAR04cMCpnqenm9Rl7NixGjp0qObPn6/c3FzFx8dr/fr1+vzzz7VkyRJ72JakoUOHSjp/sxzpfGC+8847lZeXp6eeekoff/yxw7GvuOIK9e/fX5L05ptvav369frZz36mbt26KScnR8uWLdPRo0e1YsUKl8M2AAAAWg+3Are7a1t7i8Vi0fr16zV79mytWbNGy5cv19VXX62VK1dq4sSJ9e575swZ++3fn376aaftkydPtgfu2267Tbt27dLbb7+tM2fOKCwsTP369dOyZcs0ZMgQz78wAAAAtHhuTSlB4zGlBAAAoHlrkmUBd+/erSFDhig8PFwRERFKSkrSnj173D45AAAA4OtcvsL97bff6pZbblFZWZlDeUhIiL788ktdd911Xmmgr+IKNwAAQPN2ya9wp6amqqysTM8++6xOnDihEydO6LnnnlNpaakWLFjgdgMAAAAAX+byFe5u3bqpR48eSk9PdygfOHCgMjMz9eOPP3qlgb6KK9wAAADN2yW/wn3y5EndeuutTuW33HKLTp486XYDAAAAAF/mcuCurKxU27ZtncrDwsJUWVnp0UYBAAAAvqJBq5QAAAAAaJgG3fhm5cqV+uKLLxzKfvjhB0nSqFGjnOpbLBanOzcCAAAArYnLX5r082v4xXCLxaLq6uoG79ca8KVJAACA5s1Tec3lK9xHjhxx+yQAAABAa+Vy4O7evbs32wEAAAD4JL40CQAAAHgRgRsAAADwIgI3AAAA4EUEbgAAAMCLCNwAAACAFxG4AQAAAC8icAMAAABeROAGAAAAvIjADQAAAHgRgRsAAADwIgI3AAAA4EUEbgAAAMCL2jR1AwAAAHxVQUmFcosrVFhWqYiQAEWFBcoaGtjUzcIlRuAGAADwguz8Us1au087MnLtZYlxUUod31sx7UKasGW41JhSAgAA4GEFJRVOYVuS0jNy9fTafSooqWiilqEpELgBAAA8LLe4wils10jPyFVuMYG7NSFwAwAAeFhhWWW924sush2+hcANAADgYRHBAfVuD7/IdvgWAjcAAICHRbUNVGJcVK3bEuOiFNWWlUpaEwI3AACAh1lDA5U6vrdT6E6Mi9KC8b1ZGrCVYVlAAAAAL4hpF6LXk/sot7hCRWWVCg8OUFRb1uFujQjcAAAAXmINJWCDKSUAAACAVxG4AQAAAC8icAMAAABeROAGAAAAvIjADQAAAHgRgRsAAADwIgI3AAAA4EUEbgAAAMCLCNwAAACAF/lU4N68ebOGDBkiq9Wq8PBwJSQkaM2aNRfdb8qUKbJYLE6PXr16OdW12Wx66aWX1LNnTwUHB6t3795avXq1N14OAAAAfIDP3No9LS1NU6dOVVJSkubNmyd/f38dPHhQx44dc2n/oKAgvf322w5lVqvVqd6zzz6r1NRUTZs2TX379tWGDRv0i1/8QhaLRRMmTPDIawEAAIDvsBhjTFM3orEyMzN17bXXatq0aVq4cGGD958yZYo++OADFRcX11svKytLPXv21PTp0/XGG29IkowxGjhwoI4cOaLMzEz5+/u7dM7CwkJZrVYVFBQoIiKiwW0GAACAd3kqr/nElJLFixerurpaL774oiSpuLhY7nyOqK6uVmFhYZ3bN2zYoMrKSj300EP2MovFohkzZuj48ePavXt3wxsPAAAAn+YTgXvz5s3q1auXNm7cqK5duyo8PFzt27fXc889J5vN5tIxSkpKFBERIavVqsjISD388MNOV7z37t2rsLAwXXPNNQ7l/fr1s2+vS3l5uQoLCx0eAAAA8H0+MYc7IyND/v7+SklJ0cyZMxUfH69169Zp7ty5qqqq0vz58+vdPzo6WjNnztRNN90km82mv/3tb1q0aJG++eYbbdu2TW3anO+mnJwcderUSRaLxWl/ScrOzq7zHPPnz9ecOXMa+UoBAADQ0jS7Odw2m00VFRUu1Q0KCpLFYpG/v79sNptSU1M1a9Ys+/aRI0dq+/btOnnypMLDwxvUjnnz5unZZ5/V6tWr7V+GHDp0qHJycvT99987tdnf31+PP/64/vjHP9Z6vPLycpWXl9ufFxYWKjY2ljncAAAAzZTPzuFOT09XSEiIS4+DBw9KkkJCQiRJycnJDsdKTk5WaWlpvVM96vLEE0/Iz89PmzdvtpeFhIQ4hOYaZWVlDu2oTVBQkCIiIhweAAAA8H3NbkpJr169lJaW5lLdmqkcMTExysjIUKdOnRy2d+zYUZKUl5fX4HaEhISoffv2Onv2rMP5tm7dKmOMw7SSnJwcezsAAACACzW7wN25c2dNmTKlQfskJCQoIyNDWVlZuvzyy+3lNXOqO3To0OB2FBUVKTc312HfG2+8UW+//bb279+va6+91l6+Z88e+3YAAADgQs1uSok77rvvPknS0qVL7WU2m01paWmKjIxUQkKCvfzQoUM6dOiQ/XlZWZmKioqcjvmf//mfMsZoxIgR9rKxY8cqICBAixYtspcZY7R48WJ16dJFAwYM8OjrAgAAQMvX7K5wu2Ps2LEaOnSo5s+fr9zcXMXHx2v9+vX6/PPPtWTJEgUFBdnrDh06VNL5m+VI0okTJ9SnTx8lJyfbb+X+ySefaOPGjRoxYoTGjh1r37dr167693//d7388suqrKxU3759tX79eu3YsUOrVq1y+aY3AAAAaD2a3Sol7iouLtbs2bO1Zs0anT17VldffbVmzZqliRMnOtTr0aOHpP8L3Pn5+Xr00Uf1xRdfKDs7W9XV1bryyis1ceJE/eY3v1FAQIDD/jabTQsWLNCSJUuUk5OjuLg4PfPMM07nuRjuNAkAANC8eSqv+UzgbmkI3AAAAM2bzy4LCAAAAPgSAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRT4VuDdv3qwhQ4bIarUqPDxcCQkJWrNmzUX3s1gsdT6SkpLs9TIzM+us995773nzpQEAAKCFatPUDfCUtLQ0TZ06VUlJSZo3b578/f118OBBHTt27KL7rlixwqnsq6++0sKFCzV8+HCnbcnJyRo1apRDWf/+/d1vPAAAAHyWTwTuzMxMPfzww3r00Ue1cOHCBu//y1/+0qls27ZtslgsSk5Odtp200031boPAAAA8FM+MaVk8eLFqq6u1osvvihJKi4uljHG7eOVl5dr7dq1GjhwoLp27VprnXPnzqmiosLtcwAAAKB18InAvXnzZvXq1UsbN25U165dFR4ervbt2+u5556TzWZr8PE2btyo/Px8TZw4sdbtc+bMUdu2bRUcHKy+ffvq008/bexLAAAAgI/yiSklGRkZ8vf3V0pKimbOnKn4+HitW7dOc+fOVVVVlebPn9+g461atUpBQUH6+c9/7lDu5+en4cOHa9y4cerSpYsOHz6sV199VSNHjtSHH36o0aNH13nM8vJylZeX258XFhY27EUCAACgRbKYxsy98AKbzebyVI2goCBZLBb5+/vLZrMpNTVVs2bNsm8fOXKktm/frpMnTyo8PNylYxYWFqpTp04aOXKk1q1bd9H6Z8+e1bXXXqt27drpwIEDddZ74YUXNGfOHKfygoICRUREuNQ2AAAAXDqFhYWyWq2NzmvNbkpJenq6QkJCXHocPHhQkhQSEiJJTl9wTE5OVmlpqfbu3evy+deuXauysrI6p5P8VGRkpFJSUnTw4EEdP368znrPPPOMCgoK7A9XVk8BAABAy9fsppT06tVLaWlpLtWNjo6WJMXExCgjI0OdOnVy2N6xY0dJUl5ensvnX7VqlaxWq8aMGePyPrGxsZLOX+2u60uWQUFBCgoKcvmYAAAA8A3NLnB37txZU6ZMadA+CQkJysjIUFZWli6//HJ7eXZ2tiSpQ4cOLh0nJydHW7du1ZQpUxoUjg8fPtyg8wAAAKD1aHZTStxx3333SZKWLl1qL7PZbEpLS1NkZKQSEhLs5YcOHdKhQ4dqPc57770nm81W53SS06dPO5VlZWVp2bJl6t27t/2KOwAAAFCj2V3hdsfYsWM1dOhQzZ8/X7m5uYqPj9f69ev1+eefa8mSJQ5Xq4cOHSrp/M1yfmrVqlWKiYnRoEGDaj3PzJkzdejQIQ0dOlQxMTHKzMzUkiVLdO7cObduuAMAAADf5xNXuC0Wi9avX6/HHntMH374oZ544gmdOHFCK1eu1PTp0106xsGDB/X1119rwoQJ8vOrvVuGDx8ui8WiN998Uw899JDeeustJSYmavfu3XWGdAAAALRuzW5ZwNbCU8vMAAAAwDt8dllAAAAAwJcQuAEAAAAvInADAAAAXkTgBgAAALyIwA0AAAB4kU+sww0AAADfVlBSodziChWWVSoiJEBRYYGyhgY2dbNcQuAGAABAs5adX6pZa/dpR0auvSwxLkqp43srpl1IE7bMNUwpAQAAQLNVUFLhFLYlKT0jV0+v3aeCkoomapnrCNwAAABotnKLK5zCdo30jFzlFhO4AQAAALcVllXWu73oItubAwI3AAAAmq2I4IB6t4dfZHtzQOAGAABAsxXVNlCJcVG1bkuMi1JU2+a/UgmBGwAAAM2WNTRQqeN7O4XuxLgoLRjfu0UsDciygAAAAGjWYtqF6PXkPsotrlBRWaXCgwMU1ZZ1uAEAAACPsYa2nID9U0wpAQAAALyIwA0AAAB4EYEbAAAA8CICNwAAAOBFBG4AAADAiwjcAAAAgBcRuAEAAAAvYh1uAACAJlJQUqHc4goVllUqIiRAUWEtd61p1I3ADQAA0ASy80s1a+0+7cjItZclxkUpdXxvxbQLacKWuY8PELUjcAMAAFxiBSUVTmFbktIzcvX02n16PblPiwuqvvgBwlOYww0AAHCJ5RZXOIXtGukZucotrrjELWqci32AKChpWa/H0wjcAAAAl1hhWWW924susr258bUPEJ5G4AYAALjEIoID6t0efpHtzY2vfYDwNAI3AADAJRbVNlCJcVG1bkuMi1JU25Y1f9vXPkB4GoEbAADgErOGBip1fG+n0J0YF6UF43u3uC9M+toHCE+zGGNMUzeiNSosLJTValVBQYEiIiKaujkAAKAJ1CyjV1RWqfDgAEW1bbnL6GXnl+rptfuU/pNVShaM763oFrpKiafyGssCAgAANBFraMsN2D8V0y5Eryf38ZkPEJ5E4AYAAIBH+NIHCE9iDjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8iMANAAAAeBGBGwAAAPAiAjcAAADgRQRuAAAAwIsI3AAAAIAXEbgBAAAALyJwAwAAAF5E4AYAAAC8qE1TN6C1MsZIkgoLC5u4JQAAAKhNTU6ryW3uInA3kaKiIklSbGxsE7cEAAAA9SkqKpLVanV7f4tpbGSHW2w2m7KzsxUeHi6LxdLUzXFbYWGhYmNjdezYMUVERDR1c5oF+sQZfeKMPnFGnzijT5zRJ87oE2ee6hNjjIqKihQTEyM/P/dnYnOFu4n4+fmpa9euTd0Mj4mIiOCX/CfoE2f0iTP6xBl94ow+cUafOKNPnHmiTxpzZbsGX5oEAAAAvIjADQAAAHgRgRuNEhQUpOeff15BQUFN3ZRmgz5xRp84o0+c0SfO6BNn9Ikz+sRZc+sTvjQJAAAAeBFXuAEAAAAvInADAAAAXkTgBgAAALyIwA0AAAB4EYEbtRo0aJAsFkutj4CAgIvuP2XKlFr37dWrl1Ndm82ml156ST179lRwcLB69+6t1atXe+NlNUpj+sRms2n58uW66667FBsbq7CwMF1//fWaO3euysrKnOrXdZ7U1FRvvTy3NPZ9Ikn79+/XiBEj1LZtW0VGRupXv/qVTp8+7VSvpbxPamzevFlDhgyR1WpVeHi4EhIStGbNmovuV1d/WiwWJSUl2etlZmbWWe+9997z5ktzm7t94ovjSQ13+sRXx5Ma7r5PJN8bT5YvX17nz+/EiRMX3d8Xx5PG9klTjSfcaRK1evbZZ/XAAw84lJ07d04PPvighg8f7tIxgoKC9PbbbzuU1Xa3pmeffVapqamaNm2a+vbtqw0bNugXv/iFLBaLJkyY4P6L8LDG9ElJSYlSUlJ066236sEHH1THjh21e/duPf/889qyZYs+++wzWSwWh32SkpI0adIkh7I+ffp45sV4SGPfJ8ePH1diYqKsVqvmzZun4uJi/f73v9e3336rL7/8UoGBgQ7nagnvE0lKS0vT1KlTlZSUpHnz5snf318HDx7UsWPHLrrvihUrnMq++uorLVy4sNY+TU5O1qhRoxzK+vfv737jvaQxfSL53ngiud8nvjqeSI17n/jqeCJJL774onr27OlQ1q5du4vu56vjieR+n0hNNJ4YwEUrVqwwksyqVasuWnfy5MkmLCzsovWOHz9uAgICzMMPP2wvs9ls5o477jBdu3Y1VVVVjWqzt7naJ+Xl5Wbnzp1O5XPmzDGSzKZNmxzKJTn0SUvSkPfJjBkzTEhIiPnxxx/tZZs2bTKSzJIlS+xlLel9cuTIERMSEmIee+wxjx1z6tSpxmKxmGPHjjmcR5J5+eWXPXYeb2lsn/jieNKYPvHV8aSx7xNfHE/S0tKMJPP3v//dY8ds6eNJY/ukqcYTppTAZe+++67CwsI0duxYl/eprq5WYWFhnds3bNigyspKPfTQQ/Yyi8WiGTNm6Pjx49q9e3ej2uxtrvZJYGCgBgwY4FQ+btw4Sef/DFqb0tLSWv9E3Jw15H2ydu1ajRkzRt26dbOXDRs2TFdddZXef/99e1lLep8sXrxY1dXVevHFFyVJxcXFMo243UF5ebnWrl2rgQMHqmvXrrXWOXfunCoqKtw+h7d5qk98aTxpTJ/46njS2PeJL44nFyoqKlJ1dXWjjuEL48mFGtMnl3o8IXDDJadPn9amTZt09913KywszKV9SkpKFBERIavVqsjISD388MMqLi52qLN3716FhYXpmmuucSjv16+ffXtz5U6f/FTNfLOoqCinbcuXL1dYWJhCQkJ07bXX6t13321Uey+FhvRJVlaWTp06pZtvvtlpW79+/Rx+9i3pfbJ582b16tVLGzduVNeuXRUeHq727dvrueeek81ma/DxNm7cqPz8fE2cOLHW7XPmzFHbtm0VHBysvn376tNPP23sS/A4T/SJr40nnn6fSC1/PGlMn/jqeFJj8ODBioiIUGhoqO666y5lZGS4dRxfGE9qNKZPmmI8YQ43XLJmzRpVVVXV+Uv6U9HR0Zo5c6Zuuukm2Ww2/e1vf9OiRYv0zTffaNu2bWrT5vxbLycnR506dXKabxgdHS1Jys7O9uwL8aCG9kltXnrpJUVERGjkyJEO5QMGDNC9996rnj17Kjs7W2+++aYmTpyogoICzZgxo7FN95qG9ElOTo6k//tZXyg6Olpnz55VeXm5goKCWtT7JCMjQ/7+/kpJSdHMmTMVHx+vdevWae7cuaqqqtL8+fMbdLxVq1YpKChIP//5zx3K/fz8NHz4cI0bN05dunTR4cOH9eqrr2rkyJH68MMPNXr0aE++rEZpbJ/44nji6feJ1PLHk8b0ia+OJ6GhoZoyZYo9XH799dd69dVXNWDAAP3jH/9QbGxsg47nC+NJY/ukycaTBk1AQYtUXV1tSktLXXrYbLZaj9G/f3/ToUMHU1lZ6XY7fve73xlJZvXq1fayIUOGmGuuuabWNksyjz/+uNvnq09z6JOa/li0aNFF65aXl5vrr7/etGvXzpSUlLh1vou51H2Snp5uJJk1a9Y4bXvuueeMJJOXl2eMaVnvEz8/PyPJpKamOhxrxIgRJiQkxBQWFrp8/oKCAhMcHGzGjRvnUv0zZ86YTp06mauvvtr1F9lATd0nNVr6eOLpPvGF8aQxfeKr40ltduzYYSwWi/n1r3/doPP7ynhSG3f7pMalGE+YUtIKpKenKyQkxKXHwYMHnfY/fPiwdu/erfvuu8/+yc8dTzzxhPz8/LR582Z7WUhIiMrLy53q1swzDAkJcft89WnqPlmzZo1mz56tqVOnunSFKTAwUI888ojy8/P19ddfN/h8rrjUfVLzs3Xl59+S3ic1bUlOTnY4VnJyskpLSxv0Z8i1a9eqrKzM5b+iREZGKiUlRQcPHtTx48ddPk9DNHWf1Gjp44kn+8RXxpPG9Imvjie1uf3223XLLbc4vPdd4SvjSW3c7ZMal2I8YUpJK9CrVy+lpaW5VLe2P8fVzPVrzNQJ6fybs3379jp79qzD+bZu3SpjjMOfbWr+PBgTE9Ooc9alKftk06ZNmjRpkkaPHq3Fixe7vF/Nn8ku7D9PutR9UnOMmp/1hXJychQZGamgoCB73ZbyPomJiVFGRoY6derksL1jx46SpLy8PJfPv2rVKlmtVo0ZM8blfS58n9T1pajGaOo+qdHSxxNP9YkvjSeN6RNfHU/qEhsbW28ArY2vjCd1cadPalyS8cSta+9oVa655hpzxRVXNPo4hYWFxmKxmOnTp9vL3njjDSPJ/POf/3Sou2rVKiPJpKenN/q83uBun3zxxRcmLCzMDBgwoMF/yn399deNJLNr164Gn/dScKdPOnToYO655x6n8quuusoMGTLE/rwlvU8mTJhgJJlDhw45lC9dutRIqnU5t9pkZ2cbPz8/c//99zfo/P/xH/9hJJns7OwG7edNnuqTC7X08cQTfeJr40lj+8QXx5O6JCQkmKuuusrl+r40ntSloX1yoUsxnhC4Ua9//OMfRpJ57rnn6qzzww8/mB9++MH+vLS0tNa5dk899ZSRZNatW2cvO3bsWJ3rXHbp0qVZrYdaw50+McaY77//3rRv395cd9115uzZs3Xue+rUKaeywsJCc8UVV5ioqChTXl7ufuO9xN0+efDBB01ISIg5evSovWzz5s1Gkvmv//ove1lLep/85S9/MZLMb3/7W3tZdXW1uf32201kZKQpKyuzl9fWJzVeffVVI8ls2bKl1u21vU+OHz9uLrvsMtO7d+9GvgrPakyf+Op40tj3iS+OJ43tE18cT2r7+X388cdGktN65a1lPGlMnzTleELgRr1qPt0eOHCgzjrdu3c33bt3tz8/cuSIadeunZkxY4ZZuHChWbhwoRk1apSRZEaMGGGqq6sd9q95o0+fPt3893//txk9erTLN05pCu70SWFhoYmNjTV+fn4mNTXVrFixwuFx4VWm559/3sTHx5vZs2ebt956y8yZM8d0797dWCwWs3LlSm++NLe50yfGGHP06FHTvn17c8UVV5jXXnvNzJs3z1x22WXmhhtucPjP1ZiW8z6x2Wxm6NCh9qslb775pklKSnK6+YYxtfdJjYSEBBMTE+P0+1JjypQp5o477jAvvPCCeeutt8xvf/tb0759exMYGGi2bt3q4VfVOI3pE18dTxrTJ746njT2d8cXx5Mrr7zS3HPPPWbBggVm8eLFZvr06aZNmzYmNjbWnDhxwqFuaxlPGtMnTTmeELhRp+rqatOlSxdz00031Vvvp2/ovLw888tf/tJceeWVJjQ01AQFBZnrrrvOzJs3z1RUVNR6nnnz5pnu3bubwMBAc9111zW7/whquNsnNXfxqusxefJke91PP/3UJCUlmc6dO5uAgADTrl07M3z48DqvTDQ1d/ukxnfffWeGDx9uQkNDTbt27czEiROdBs2a87SU90lRUZF5/PHHTefOnU1gYKC54YYbam1rXX1y4MABI8k8+eSTdZ7j3XffNYmJiaZDhw6mTZs2JioqyowbN858/fXXnnwpHuNun/jyeOJun/jyeNLY3x1fG0+effZZc+ONNxqr1WoCAgJMt27dzIwZM2p9Ta1lPGlMnzTleGIxphG3QAMAAABQL5YFBAAAALyIwA0AAAB4EYEbAAAA8CICNwAAAOBFBG4AAADAiwjcAAAAgBcRuAEAAAAvInADAAAAXkTgBgAAALyIwA0AcNmUKVNksViUmZnZ1E1RZmamLBaLpkyZ0tRNAYB6EbgBwIvuv/9+WSwWtW/fXuXl5Y0+3gsvvCCLxaJt27Y1vnFe9NZbb8lisejXv/71Revedtttslgs2rVr1yVoGQBcegRuAPCSoqIivf/++7JYLDp79qzWr1/f1E26ZCZMmKDQ0FC99957Ki0trbPewYMHtWvXLvXq1UsDBgy4hC0EgEuHwA0AXrJmzRqdO3dOTzzxhPz8/LR06dKmbtIlExERoXvuuUeFhYX64IMP6qy3bNkySdLUqVMvVdMA4JIjcAOAlyxdulRt2rTRzJkzNXjwYG3ZskU//vhjnfXT09N19913q1OnTgoKClJsbKz+7d/+TZ9//rkkadCgQZozZ44kafDgwbJYLLJYLOrRo4f9GBaLRYMGDar1+D169HCoK0n/+te/NHPmTN10001q3769goODddVVV+npp59WcXFxo15/TYiuCdU/VV1drRUrViggIECTJk2y1x07dqx69Oih4OBgRUZG6s4779TWrVtdPm9tr7PGoEGDZLFYnMqNMVq2bJluu+02RUREKDQ0VDfffHOtbS8rK9Mrr7yi+Ph4Wa1WhYWFqUePHrr33nv1zTffuNxOAK1Hm6ZuAAD4ou+//15ffPGFRo0apU6dOmnSpEnasmWL0tLS9MILLzjVX7hwoZ544gmFhIRo3Lhx6tatm7KysvT555/rgw8+0O23327/cuD27ds1efJke6hs166d2+1ct26dli5dqsGDB2vQoEGy2Wz64osvtGDBAm3fvl3p6ekKCAhw69h33HGHrrrqKm3fvl2HDx/W5Zdf7rD9r3/9q3JycjRu3Dh17NhRkvTwww8rPj5ew4YNU4cOHZSVlaX169dr2LBhWrduncaOHev2a62LMUYTJ07U6tWrFRcXp1/84hcKDAzUpk2bNHXqVH3//ff6/e9/b68/efJkvf/+++rdu7dSUlIUFBSkY8eOaevWrfr73/+u+Ph4j7cRQAtnAAAe9+STTxpJZvXq1cYYY4qKikxYWJjp1q2bqa6udqj7v//7v8bPz8/ExMSYI0eOOGyz2WwmKyvL/vz55583kszWrVtrPa8kM3DgwFq3de/e3XTv3t2h7Pjx46a8vNyp7pw5c4wks3LlSofyyZMnG0lO7axLamqqkWRmz57ttG3cuHFGkvnoo4/sZYcPH3aql52dbWJiYkxcXJxD+ZEjR4wkM3nyZIfy2l5njYEDB5qf/tf31ltvGUkmJSXFVFRU2MvLy8vNz372MyPJfPXVV8YYY/Lz843FYjEJCQmmqqrK4ThVVVUmLy+v1vMCaN2YUgIAHlZZWakVK1YoIiJCd999tySpbdu2GjdunI4eParNmzc71F+yZIlsNpvmzp3rNBXCYrEoJibGa23t0qWLAgMDncofeeQRSXJqa0NNnjxZbdq00TvvvCObzWYvP336tD766CPFxMRoxIgR9vKePXs6HSM6Olrjx49XRkZGvVNy3PXGG28oLCxMb775psPV/MDAQP3ud7+TJK1evVrS+Z+HMUbBwcHy83P8L9Tf379Rf20A4LuYUgIAHrZhwwadPn1aU6dOVXBwsL180qRJWrlypZYuXarhw4fby7/88ktJcii7VIwxSktL0/Lly/Xdd9+poKDAIRhnZ2c36vidO3fW6NGjtWHDBm3atEl33nmnJGnFihWqrKzU5MmT5e/vb69/+PBhzZ8/X5999pmysrKcllLMzs5W9+7dG9WmC5WUlOjbb79VTEyMFixY4LS9srJSknTgwAFJ578MOmrUKG3cuFE33XST7rnnHg0aNEh9+/Z1e+oNAN9H4AYAD6tZjaTmi4A1hg4dqi5dumjDhg06e/asIiMjJUkFBQWyWCyKjo6+5G197LHH9MYbbyg2NlZ33XWXoqOjFRQUJEmaM2eOR9YOnzp1qjZs2KBly5bZA3daWpqk8+uU1/jhhx/Ur18/FRYWavDgwfrZz36miIgI+fn5adu2bdq+fbtH2nOhvLw8GWOUlZVl/0Jqbc6dO2f/95///GfNmzdP7777rp599llJ54N4SkqK5s2bp9DQUI+2EUDLR+AGAA86duyYPv30U0nSwIED66y3cuVKPfbYY5LOf+nRGKOcnBx16dKlUee3WCyqqqqqdVtBQYGsVqv9+alTp/Tmm2+qd+/e2r17t0NQPHHiRL0BtCFGjRql6Oho+weNQ4cO6bvvvtPAgQN15ZVX2uv94Q9/UF5enlasWKFf/vKXDsd48MEHtX37dpfO5+fnp4qKilq3FRQUODyPiIiQJCUkJOirr75y6fihoaGaO3eu5s6dqyNHjmjr1q1avHixFi5cqNLSUi1ZssSl4wBoPQjcAOBBy5cvl81m0+23366rr77aaXtVVZXeeecdLV261B64+/Xrp6+++kqffvqpUlJS6j1+zfSL6urqWrdfdtllysrKcirPzMxUfn6+Q+A+fPiwjDEaNmyY01XZHTt21P9CG8Df31+TJ09WamqqVq5cqf3790tyXnv70KFDkuS0EokxRjt37nT5fJdddpm+/fZbVVVVqU2b//tv7ty5c8rIyHCoGx4ermuuuUb79+9Xfn5+g+dg9+zZUz179lRycrI6duyoDz/8kMANwAlfmgQAD6mZD22xWPTOO+/o7bffdnosX75c/fv31759++xXVB988EH5+/tr9uzZTl8KNMY4zKOumYZy7NixWtvQt29fZWZmOlwNrqio0JNPPulUt2Yu9K5duxzmbR8/flzPPPOMm71Qu5qpI2+99Zbee+89Wa1W/fznP6+1PTXrjtdITU3Vd9995/K5+vbtq8rKSq1atcpeZozRM8884zA1pMZjjz2mkpISTZs2rdbtR44cUWZmpqTzX/asrS15eXkqLy93mLMPADW4wg0AHvLZZ5/pyJEjGjhwoNOa0xdKSUnR7t27tXTpUt1888264YYb9Mc//lGPPfaYrrvuOt19993q3r27Tpw4ofT0dI0ePVp//OMfJf3fDW9++9vf6p///KesVqvatWtnX1XkySef1KeffqpRo0YpOTlZoaGh2rRpk9q1a+c0R7xm9Y+1a9fq5ptv1tChQ3Xy5El99NFHGjp0qP2KsyfExcUpMTFR6enpks5/yAgJCXGo8+CDDyotLU3jx4/Xvffeq/bt2+uLL77QP/7xD40ePVoff/yxS+d65JFHlJaWpgceeECbNm1Shw4dtGPHDuXn5ys+Pt7p5jS//vWv9cUXX+idd97Rzp07NWzYMMXExOjkyZM6cOCA9uzZo3fffVc9evRQVlaW+vTpo/j4ePXu3VtdunTRmTNntGHDBlVWVuo3v/mNZzoMgG9psgUJAcDHJCcnG0kmLS2t3noFBQUmJCTEWK1WU1JSYi/funWrGTNmjImMjDSBgYGma9euZvz48Wbnzp0O+y9fvtzccMMNJigoyEhyWnP6z3/+s7nhhhtMYGCg6dy5s3n00UdNUVFRretTFxUVmf/4j/8wPXr0MEFBQSYuLs7853/+p6moqKh1Te+GrsN9oXfeecdIMpLMl19+WWudrVu3mttuu82Eh4ebdu3amVGjRpmvv/661vXH61qH2xhjPvvsM3PLLbeYoKAg0759e/OrX/3KnDx5stZ1uGusWbPGDBs2zFx22WUmICDAdOnSxQwaNMi88sor5vTp08YYY/Ly8swLL7xgEhMTTXR0tAkMDDQxMTFmxIgR5q9//WuD+wRA62AxxpgmS/sAAACAj2MONwAAAOBFBG4AAADAiwjcAAAAgBcRuAEAAAAvInADAAAAXkTgBgAAALyIwA0AAAB4EYEbAAAA8CICNwAAAOBFBG4AAADAiwjcAAAAgBcRuAEAAAAv+v8A2lbXE8NPX1kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test_inverse.flatten(), y=y_pred_inverse.flatten())\n",
    "#plt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2) \n",
    "plt.xlabel('Actual Values', fontsize=14)\n",
    "plt.ylabel('Predicted Values', fontsize=14)\n",
    "plt.title('Multilayer Perceptron\\nActual HOMO vs. Predicted HOMO', fontsize=14)\n",
    "\n",
    "plt.text(0.028, 0.80, f'- MSE: {mse:.6f}\\n- RMSE: {rmse:.6f}\\n- R-squared: {r_squared:.6f}\\n- MAE: {mae:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "plt.text(0.050, 0.65, f'Optimizer: {best_optimizer}\\nAct. function: {best_activation_function}\\nLearning rate: {best_learning_rate}', color='green', fontsize=14, ha='left', transform=plt.gca().transAxes)\n",
    "\n",
    "#plt.text(0.028, 0.30, f'- MSE: {mse:.6f}\\n- RMSE: {rmse:.6f}\\n- R-squared: {r_squared:.6f}\\n- MAE: {mae:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "#plt.text(0.050, 0.05, f'Optimizer: {best_optimizer}\\nAct. function: {best_activation_function}\\nLearning rate: {best_learning_rate}\\nNum Layers: {best_num_layers}\\nNum Neurons: {best_num_neurons}', color='green', fontsize=14, ha='left', transform=plt.gca().transAxes)\n",
    "\n",
    "\"\"\"\n",
    "plt.text(0.55, 0.1, f'- MSE: {mse:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "plt.text(0.55, 0.15, f'- RMSE: {rmse:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "plt.text(0.55, 0.2, f'- R-squared: {r_squared:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "plt.text(0.55, 0.25, f'- MAE: {mae:.6f}', color='red', fontsize=14, transform=plt.gca().transAxes)\n",
    "plt.text(0.57, 0.3, f'Optimizer: {best_optimizer}\\nAct. function: {best_activation_function}\\nLearning rate: {best_learning_rate}\\nNum Layers: {best_num_layers}\\nNum Neurons: {best_num_neurons}', color='green', fontsize=14, ha='left', transform=plt.gca().transAxes)\n",
    "\"\"\"\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                SINGLE LAYER PERCEPTRON                \u001b[0m\n",
      "                TOP-5 Predicted Values\n",
      "                \n",
      " Actual HOMO  Predicted HOMO  Absolute Difference\n",
      "     -6.1902       -6.206456             0.016256\n",
      "     -6.2925       -6.155572             0.136928\n",
      "     -6.4384       -6.246896             0.191504\n",
      "     -5.8804       -5.621495             0.258905\n",
      "     -5.5673       -5.245937             0.321363\n"
     ]
    }
   ],
   "source": [
    "# ТОП-5 лучших предсказанных значений и величина абсолютной разницы\n",
    "\n",
    "y_test_flattened = y_test_inverse.flatten()\n",
    "y_pred_flattened = y_pred_inverse.flatten()\n",
    "\n",
    "comparison_df = pd.DataFrame({'Actual HOMO': y_test_flattened, 'Predicted HOMO': y_pred_flattened})\n",
    "comparison_df['Absolute Difference'] = abs(comparison_df['Actual HOMO'] - comparison_df['Predicted HOMO'])\n",
    "\n",
    "best_results = comparison_df.sort_values(by='Absolute Difference').head(5)\n",
    "print(\"\\033[1m{:^55}\\033[0m\".format(\"MULTILAYER PERCEPTRON\"))\n",
    "print(\"{:^55}\".format(\"TOP-5 Predicted Values\\n\"))\n",
    "print(best_results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
