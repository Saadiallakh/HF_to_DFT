{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The data represents the energy values of supramolecular systems, which were calculated using two different quantum chemical approximations. The \"HF\" (Hartree-Fock) set was calculated using a fast and inaccurate approximation, while \"DFT\" (Density Functional Theory) was calculated using a resource-intensive but accurate approximation.\n",
    "\n",
    "Feature  | Feature Type | Description\n",
    "-------------------|--------------------|--------------------\n",
    "dft_gibbs_free_energy_ev       |Target| Gibbs free energy of the supramolecular system, calculated using the DFT approximation \n",
    "dft_electronic_energy_ev       |Target| Electronic energy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_entropy_ev       |Target| Entropy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_enthalpy_ev       |Target| Enthalpy of the supramolecular system, calculated using the DFT approximation\n",
    "dft_dipole_moment_d       |Target| Dipole moment of the supramolecular system, calculated using the DFT approximation\n",
    "dft_gap_ev      |Target| Energy gap between HOMO and LUMO, calculated using the DFT approximation\n",
    "hf_gibbs_free_energy_ev       |Training| Gibbs free energy of the supramolecular system, calculated using the HF approximation \n",
    "hf_electronic_energy_ev       |Training| Electronic energy of the supramolecular system, calculated using the HF approximation\n",
    "hf_entropy_ev       |Training| Entropy of the supramolecular system, calculated using the HF approximation\n",
    "hf_enthalpy_ev       |Training| Enthalpy of the supramolecular system, calculated using the HF approximation\n",
    "hf_dipole_moment_d       |Training| Dipole moment of the supramolecular system, calculated using the HF approximation\n",
    "hf_gap_ev      |Training| Energy gap between HOMO and LUMO, calculated using the HF approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, KFold \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Load data from a CSV file\n",
    "data = pd.read_csv(\"./NN_ML.csv\",delimiter=\",\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Create an empty list to store the '.xyz' coordinates\n",
    "coordinates_list = []\n",
    "\n",
    "def read_xyz_file(file_path):\n",
    "    \"\"\"\n",
    "    Function to read XYZ format files and extract atomic coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the XYZ file.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Numpy array of atomic coordinates.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "\n",
    "        #-# Skip the first two lines containing metadata\n",
    "        lines = file.readlines()[2:]           \n",
    "        coordinates = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "\n",
    "            #-# Check if the line contains coordinates\n",
    "            if len(parts) == 4:              \n",
    "                coordinates.append([float(parts[1]), float(parts[2]), float(parts[3])])\n",
    "        return np.array(coordinates)\n",
    "\n",
    "#-# For each row in the DataFrame, form the path to the file and extract coordinates\n",
    "for index, row in data.iterrows():\n",
    "    file_name = str(row['name']) + \".xyz\"\n",
    "    file_path = os.path.join(\"./xyz\", file_name)\n",
    "    coordinates = read_xyz_file(file_path)\n",
    "    coordinates_list.append(coordinates)\n",
    "\n",
    "#-# Add the coordinates to the DataFrame as a new column\n",
    "data['coordinates'] = coordinates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% confidence intervals for target (DFT) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for feature in dft_features:\n",
    "    mean = data[feature].mean()\n",
    "    std_dev = data[feature].std()\n",
    "    n = len(data)\n",
    "    std_error = std_dev / np.sqrt(n)\n",
    "    z_score = stats.norm.ppf(0.975) \n",
    "    margin_of_error = z_score * std_error\n",
    "    ci_lower = mean - margin_of_error\n",
    "    ci_upper = mean + margin_of_error\n",
    "    ci_width = ci_upper - ci_lower\n",
    "    \n",
    "    metrics[feature] = {\n",
    "        '95% CI Lower': ci_lower,\n",
    "        '95% CI Upper': ci_upper,\n",
    "        '95% CI Width': ci_width\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sactter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "               'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev']\n",
    "\n",
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "features_of_interest = hf_features + dft_features\n",
    "\n",
    "for feature in features_of_interest:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data.index, data[feature], alpha=0.6)\n",
    "    plt.title(f'{feature}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(f'{feature} Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix between training (HF) and target (DFT) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "               'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev']\n",
    "\n",
    "dft_features = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "                'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "correlation_matrix = data[hf_features + dft_features].corr().loc[hf_features, dft_features]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix between HF Features and DFT Targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Absolute Correlation of each training (HF) feature with any (DFT) target\n",
    "### Maximum Absolute Correlation of each training (HF) feature with any (DFT) target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_correlations = correlation_matrix.abs().mean(axis=1)\n",
    "ranked_features_avg = average_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by average absolute correlation:\\n\",ranked_features_avg)\n",
    "\n",
    "max_correlations = correlation_matrix.abs().max(axis=1)\n",
    "ranked_features_max = max_correlations.sort_values(ascending=False)\n",
    "print(\"\\nHF features ranked by maximum absolute correlation:\\n\",ranked_features_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(features, title):\n",
    "    \"\"\"\n",
    "    Function to plot histograms.\n",
    "\n",
    "    Parameters:\n",
    "    - features (list): List of feature names for which histograms are to be plotted.\n",
    "    - title (str): Title for the entire plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    This function generates subplot histograms for each specified feature, arranging them in a grid.\n",
    "    It dynamically adjusts the layout based on the number of features for optimal display.\n",
    "    \"\"\"\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.hist(data[feature], bins=30, edgecolor='k', alpha=0.7)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms for training (HF) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(hf_features, 'Distribution Histograms for HF Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histograms for target (DFT) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(dft_features, 'Distribution Histograms for DFT Targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(features, title):\n",
    "    \"\"\"\n",
    "    Function to create box-and-whiskers diagrams.\n",
    "\n",
    "    Parameters:\n",
    "    - features (list): List of feature names for which to create boxplots.\n",
    "    - title (str): Title for the entire plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    This function generates subplots of boxplots for each specified feature, arranging them in a grid.\n",
    "    It dynamically adjusts the layout based on the number of features for optimal display.\n",
    "    \"\"\"\n",
    "    n_cols = 3\n",
    "    n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        ax.boxplot(data[feature].dropna(), vert=True, patch_artist=True)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_ylabel('Value')\n",
    "\n",
    "    for i in range(len(features), n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams for training (HF) set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(hf_features, 'Box-and-Whisker Diagrams for HF Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whiskers diagrams for target (DFT) set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(dft_features, 'Box-and-Whisker Diagrams for DFT Targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values for training (HF) and target (DFT) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.DataFrame(index=hf_features, columns=dft_features)\n",
    "\n",
    "for hf in hf_features:\n",
    "    for dft in dft_features:\n",
    "        _, p_value = pearsonr(data[hf], data[dft])\n",
    "        p_values.loc[hf, dft] = p_value\n",
    "\n",
    "print(\"\\nP-values for correlations between HF features and DFT targets:\\n\", p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('./NN_ML.csv')\n",
    "dataframe = dataframe.drop(columns=['mass_au', 'name'])\n",
    "dataframe = dataframe.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "def detect_outliers_iqr(data):\n",
    "    \"\"\"\n",
    "    Function to detect outliers in data using the Interquartile Range (IQR).\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.Series or pandas.DataFrame): One-dimensional or two-dimensional dataset for outlier analysis.\n",
    "\n",
    "    Returns:\n",
    "    - outliers (pandas.Series or pandas.DataFrame): Boolean array of the same shape as the input data,\n",
    "      indicating the presence (True) or absence (False) of outliers in the data.\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
    "    return outliers\n",
    "\n",
    "outliers = dataframe.apply(detect_outliers_iqr)\n",
    "print(\"\\nOutliers detected in each feature:\\n\", outliers.sum())\n",
    "\n",
    "sns.pairplot(dataframe[hf_features + dft_features])\n",
    "plt.suptitle('Pairplot of HF and DFT Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature and target sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['hf_gibbs_free_energy_ev', 'hf_electronic_energy_ev', 'hf_entropy_ev',\n",
    "            'hf_enthalpy_ev', 'hf_dipole_moment_d', 'hf_gap_ev', 'coordinates']\n",
    "\n",
    "targets = ['dft_gibbs_free_energy_ev', 'dft_electronic_energy_ev', 'dft_entropy_ev',\n",
    "           'dft_enthalpy_ev', 'dft_dipole_moment_d', 'dft_gap_ev']\n",
    "\n",
    "#-# Create DataFrame for features and target variables\n",
    "X = data[features].copy()\n",
    "y = data[targets].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Flatten the coordinates into 1D arrays\n",
    "X['coordinates'] = X['coordinates'].apply(np.ravel)\n",
    "\n",
    "#-# Find the maximum length of the coordinate arrays\n",
    "max_length = X['coordinates'].apply(len).max()\n",
    "\n",
    "#-# Pad coordinates with zeros to the maximum length\n",
    "X['coordinates'] = X['coordinates'].apply(lambda x: np.pad(x, (0, max_length - len(x)), mode='constant'))\n",
    "\n",
    "#-# Create a new DataFrame with coordinates as individual columns\n",
    "X_numeric = pd.concat([X.drop(columns='coordinates'),\n",
    "                       pd.DataFrame(np.vstack(X['coordinates']), \n",
    "                                    columns=[f'coord_{i}' for i in range(max_length)], \n",
    "                                    index=X.index)],\n",
    "                      axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_numeric, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#-# Track indices of the test set\n",
    "test_indices_list = X_test_numeric.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "#-# Save column names for later DataFrame reconstruction\n",
    "all_columns = X_train_numeric.columns\n",
    "\n",
    "#-# Apply standardization to the training data\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric[all_columns])\n",
    "\n",
    "#-# Apply the same transformation to the test data\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric[all_columns])\n",
    "\n",
    "#-# Reconstruct DataFrame with normalized data for the training set\n",
    "X_train_numeric_scaled = pd.DataFrame(X_train_numeric_scaled, columns=all_columns, index=X_train_numeric.index)\n",
    "\n",
    "#-# Reconstruct DataFrame with normalized data for the test set\n",
    "X_test_numeric_scaled = pd.DataFrame(X_test_numeric_scaled, columns=all_columns, index=X_test_numeric.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()\n",
    "\n",
    "#-# Apply standardization to the target variables in the training set\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "#-# Apply the same transformation to the target variables in the test set\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dir = './output'\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(scaler, os.path.join(scaler_dir, 'scaler.pkl'))\n",
    "joblib.dump(scaler_y, os.path.join(scaler_dir, 'scaler_y.pkl')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Architecture of \"Elastic Net Regression\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective function for Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna that defines the task for hyperparameter search.\n",
    "\n",
    "    Parameters:\n",
    "    - trial (optuna.trial.Trial): Optuna trial object used to suggest hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean MSE value across all folds of cross-validation.\n",
    "    \"\"\"\n",
    "    #-# Set the hyperparameter grid\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 1000.0, log=True),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0),\n",
    "        'tol': 1e-4,\n",
    "        'selection': 'cyclic'\n",
    "    }\n",
    "    \n",
    "    #-# Create a model with the current hyperparameter values\n",
    "    model = ElasticNet(**param, random_state=42, max_iter=100000)\n",
    "    \n",
    "    #-# Create a KFold object for 3-fold cross-validation with data shuffling\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "\n",
    "    #-# Loop through the data splits\n",
    "    for train_index, val_index in kf.split(X_train_numeric_scaled):\n",
    "        X_train_cv, X_valid_cv = X_train_numeric_scaled.iloc[train_index], X_train_numeric_scaled.iloc[val_index]\n",
    "        y_train_cv, y_valid_cv = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "        \n",
    "        #-# Train the model\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        #-# Evaluate on the validation set\n",
    "        y_pred_scaled_cv = model.predict(X_valid_cv)\n",
    "        mse = mean_squared_error(y_valid_cv, y_pred_scaled_cv)\n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    return np.mean(mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Print the best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "print(f'Best trial value: {best_trial.value}')\n",
    "print(f'Best hyperparameters: {best_trial.params}')\n",
    "\n",
    "with open('./optimization_results.txt', 'w') as f:\n",
    "    f.write(f'Best trial value: {best_trial.value}\\n')\n",
    "    f.write(f'Best hyperparameters: {best_trial.params}\\n')\n",
    "\n",
    "    f.write('\\nAll trial results:\\n')\n",
    "    for trial in study.trials:\n",
    "        f.write(f'Trial {trial.number}: Value={trial.value}, Params={trial.params}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Train the model using the best hyperparameters\n",
    "best_enr = ElasticNet(**best_params, random_state=42, max_iter=100000)\n",
    "best_enr.fit(X_train_numeric_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./output\"\n",
    "\n",
    "joblib.dump(best_enr, os.path.join(model_dir, 'elastic_net_model.pkl'))\n",
    "print(f\"Trained model with Optuna saved to {os.path.join(model_dir, 'elastic_net_model.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Predict values on the test data\n",
    "y_pred_scaled = best_enr.predict(X_test_numeric_scaled)\n",
    "\n",
    "#-# Inverse transform to get original values\n",
    "y_pred_inverse = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_scaled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# Create a .csv file with three columns: index, actual value, and predicted value\n",
    "data = {\n",
    "    'Index': test_indices_list,\n",
    "    'Actual_Gibbs_Energy': y_test_inverse[:, 0],\n",
    "    'Predicted_Gibbs_Energy': y_pred_inverse[:, 0],\n",
    "    'Actual_Electronic_Energy': y_test_inverse[:, 1],\n",
    "    'Predicted_Electronic_Energy': y_pred_inverse[:, 1],\n",
    "    'Actual_Entropy': y_test_inverse[:, 2],\n",
    "    'Predicted_Entropy': y_pred_inverse[:, 2],\n",
    "    'Actual_Enthalpy': y_test_inverse[:, 3],\n",
    "    'Predicted_Enthalpy': y_pred_inverse[:, 3],\n",
    "    'Actual_Dipole_Moment': y_test_inverse[:, 4],\n",
    "    'Predicted_Dipole_Moment': y_pred_inverse[:, 4],\n",
    "    'Actual_Band_Gap': y_test_inverse[:, 5],\n",
    "    'Predicted_Band_Gap': y_pred_inverse[:, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#-# Add a column with the color of predicted points (red if observation number < 48)\n",
    "df['Color'] = df['Index'].apply(lambda x: 'red' if x < 48 else 'blue')\n",
    "\n",
    "#-# Save the file\n",
    "csv_path = \"./test_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved DataFrame to {csv_path}\")\n",
    "\n",
    "#-# Set title\n",
    "target_names = [\n",
    "    'Gibbs Energy', 'Electronic Energy', 'Entropy', \n",
    "    'Enthalpy', 'Dipole Moment', 'Band Gap'\n",
    "]\n",
    "\n",
    "#-# Calculate metrics\n",
    "metrics = {}\n",
    "for i, target in enumerate(target_names):\n",
    "    mae = mean_absolute_error(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    mse = mean_squared_error(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    \n",
    "    metrics[target] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R-squared': r2,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "#-# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, target in enumerate(target_names):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    #-# Create scatter plot with conditional coloring\n",
    "    sns.scatterplot(\n",
    "        x=df[f'Actual_{target.replace(\" \", \"_\")}'],\n",
    "        y=df[f'Predicted_{target.replace(\" \", \"_\")}'],\n",
    "        hue=df['Color'],\n",
    "        palette={'red': 'red', 'blue': 'blue'},\n",
    "        ax=ax,\n",
    "        alpha=0.5,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    #-# Create diagonal line\n",
    "    ax.plot(\n",
    "        [df[f'Actual_{target.replace(\" \", \"_\")}'].min(), df[f'Actual_{target.replace(\" \", \"_\")}'].max()],\n",
    "        [df[f'Actual_{target.replace(\" \", \"_\")}'].min(), df[f'Actual_{target.replace(\" \", \"_\")}'].max()],\n",
    "        color='black', linestyle='--'\n",
    "    )\n",
    "    \n",
    "    #-# Set titles for axes \n",
    "    ax.set_xlabel('Actual Values', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Values', fontsize=12)\n",
    "    ax.set_title(target, fontsize=14)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    #-# Add metrics to the plots\n",
    "    ax.text(\n",
    "        0.05, 0.95,\n",
    "        f\"MSE: {metrics[target]['MSE']:.4f}\\nRMSE: {metrics[target]['RMSE']:.4f}\\nR-squared: {metrics[target]['R-squared']:.4f}\\nMAE: {metrics[target]['MAE']:.4f}\\nMAPE: {metrics[target]['MAPE']:.2f}%\",\n",
    "        transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "for j in range(len(target_names), len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = \"./test_results.png\"\n",
    "plt.savefig(plot_path)\n",
    "print(f\"Saved plots to {plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
